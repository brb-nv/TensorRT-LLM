/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[1765169572.694259] [umb-b300-020:6732 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169573.322865] [umb-b300-020:6731 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169574.046352] [umb-b300-020:6729 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169574.757138] [umb-b300-020:6730 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169574.822146] [umb-b300-020:6734 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169574.918706] [umb-b300-020:6733 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169575.060255] [umb-b300-020:6733 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169575.060333] [umb-b300-020:6734 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169575.060391] [umb-b300-020:6730 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169575.060471] [umb-b300-020:6729 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169575.061224] [umb-b300-020:6731 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169575.061260] [umb-b300-020:6732 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[12/08/2025-04:52:58] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:52:58] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:52:58] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:52:58] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:52:59] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:52:59] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:52:59] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:52:59] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:52:59] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:52:59] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:52:59] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:53:00] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:53:00] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:53:00] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:53:00] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:53:00] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:53:00] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:53:01] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:53:01] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:53:01] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:53:01] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:53:01] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:53:01] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:53:01] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:53:01] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/08/2025-04:53:01] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/08/2025-04:53:01] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/08/2025-04:53:01] [TRT-LLM] [I] TensorRT LLM inited.
[12/08/2025-04:53:01] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/08/2025-04:53:01] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
env_global_rank: 1, set device_id: 1 before importing mpi4py
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
env_global_rank: 5, set device_id: 5 before importing mpi4py
env_global_rank: 0, set device_id: 0 before importing mpi4py
env_global_rank: 3, set device_id: 3 before importing mpi4py
env_global_rank: 2, set device_id: 2 before importing mpi4py
env_global_rank: 4, set device_id: 4 before importing mpi4py
[2025-12-08 04:53:03] INFO disagg_utils.py:318: global_rank: 0, instance_idx: 0, sub_rank: 0, is_leader: True
[2025-12-08 04:53:03] INFO disagg_utils.py:318: global_rank: 4, instance_idx: 1, sub_rank: 2, is_leader: False
[2025-12-08 04:53:03] INFO disagg_utils.py:318: global_rank: 5, instance_idx: 1, sub_rank: 3, is_leader: False
[12/08/2025-04:53:03] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[2025-12-08 04:53:03] INFO disagg_utils.py:318: global_rank: 1, instance_idx: 0, sub_rank: 1, is_leader: False
[2025-12-08 04:53:03] INFO disagg_utils.py:318: global_rank: 2, instance_idx: 1, sub_rank: 0, is_leader: True
[2025-12-08 04:53:03] INFO disagg_utils.py:318: global_rank: 3, instance_idx: 1, sub_rank: 1, is_leader: False
[12/08/2025-04:53:03] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:03] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 0, sub-comm MPI rank: 0
[12/08/2025-04:53:03] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:03] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 5, sub-comm MPI rank: 3
[12/08/2025-04:53:03] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 4, sub-comm MPI rank: 2
[12/08/2025-04:53:03] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:03] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:03] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 1, sub-comm MPI rank: 1
[12/08/2025-04:53:03] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 2, sub-comm MPI rank: 0
[12/08/2025-04:53:03] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:03] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 3, sub-comm MPI rank: 1
[12/08/2025-04:53:03] [TRT-LLM] [W] Overriding kv_cache_config 
[12/08/2025-04:53:03] [TRT-LLM] [W] Overriding kv_cache_config 
[12/08/2025-04:53:03] [TRT-LLM] [I] rank 0 step1: preparing to launch command: ['python3', '/home/bbuddharaju/.local/bin/trtllm-serve', 'disaggregated_mpi_worker', '-c', '/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tests/integration/defs/disaggregated/test_configs/disagg_config_ctxtp2_gentp1cp2_deepseek_v3_lite_bf16_tllm_gen.yaml', '--log_level', 'info']
[12/08/2025-04:53:03] [TRT-LLM] [I] rank 0 step1: preparing to launch command: ['python3', '/home/bbuddharaju/.local/bin/trtllm-serve', 'disaggregated_mpi_worker', '-c', '/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tests/integration/defs/disaggregated/test_configs/disagg_config_ctxtp2_gentp1cp2_deepseek_v3_lite_bf16_tllm_gen.yaml', '--log_level', 'info']
[12/08/2025-04:53:03] [TRT-LLM] [I] Parent process (PID 6729) launched child process (PID 8287).
[12/08/2025-04:53:03] [TRT-LLM] [I] Parent process (PID 6731) launched child process (PID 8288).
[12/08/2025-04:53:03] [TRT-LLM] [I] rank 0 step2: start the mpi session server
[12/08/2025-04:53:03] [TRT-LLM] [I] rank 0 step2: start the mpi session server
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[1765169590.327078] [umb-b300-020:8287 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169590.471932] [umb-b300-020:8287 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169591.887562] [umb-b300-020:8288 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765169592.049590] [umb-b300-020:8288 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[12/08/2025-04:53:13] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:53:13] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:53:14] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:53:14] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:53:14] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
[12/08/2025-04:53:15] [TRT-LLM] [W] Overriding kv_cache_config 
[12/08/2025-04:53:15] [TRT-LLM] [I] rank 0 for index 0 launch the disagg server
[12/08/2025-04:53:15] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:15] [TRT-LLM] [I] Using LLM with PyTorch backend
[12/08/2025-04:53:15] [TRT-LLM] [I] neither checkpoint_format nor checkpoint_loader were provided, checkpoint_format will be set to HF.
[12/08/2025-04:53:15] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:15] [TRT-LLM] [I] start MpiSession with 2 workers
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
You are using a model of type deepseek_v3 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
[12/08/2025-04:53:15] [TRT-LLM] [W] Failed to load hf generation config from DeepSeek-V3-Lite/bf16, encounter error: DeepSeek-V3-Lite/bf16 does not appear to have a file named generation_config.json. Checkout 'https://huggingface.co/DeepSeek-V3-Lite/bf16/tree/main' for available files.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:53:15] [TRT-LLM] [W] Orchestrator is creating IPC executor
[33;20mrank 0 using MpiCommSession to bind to external MPI processes
[0m[12/08/2025-04:53:15] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_queue
[12/08/2025-04:53:15] [TRT-LLM] [I] Generating a new HMAC key for server worker_init_status_queue
[12/08/2025-04:53:15] [TRT-LLM] [I] Generating a new HMAC key for server proxy_result_queue
[12/08/2025-04:53:15] [TRT-LLM] [I] Generating a new HMAC key for server proxy_stats_queue
[12/08/2025-04:53:15] [TRT-LLM] [I] Generating a new HMAC key for server proxy_kv_cache_events_queue
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[12/08/2025-04:53:16] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:16] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:53:16] [TRT-LLM] [RANK 0] [I] Worker process 6729 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] for optimal NUMA-aware scheduling.
[12/08/2025-04:53:16] [TRT-LLM] [RANK 1] [I] Worker process 6730 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] for optimal NUMA-aware scheduling.
[MPIDist::create_cp_comm] rank: 0, cp_rank: 0, cp_group: [0]
[MPIDist::create_tp_comm] rank: 0, tp_rank: 0, tp_group: [0, 1]
[MPIDist::create_cp_comm] rank: 1, cp_rank: 0, cp_group: [1]
[MPIDist::create_tp_comm] rank: 1, tp_rank: 1, tp_group: [0, 1]
[MPIDist::create_pp_comm] rank: 0, pp_rank: 0, pp_group: [0]
[MPIDist::create_pp_comm] rank: 1, pp_rank: 0, pp_group: [1]
[12/08/2025-04:53:16] [TRT-LLM] [RANK 0] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[12/08/2025-04:53:16] [TRT-LLM] [RANK 1] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:53:16] [TRT-LLM] [RANK 0] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:53:16] [TRT-LLM] [RANK 0] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:53:16] [TRT-LLM] [RANK 1] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:53:16] [TRT-LLM] [RANK 1] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[12/08/2025-04:53:16] [TRT-LLM] [RANK 0] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:53:16] [TRT-LLM] [RANK 1] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:53:16] [TRT-LLM] [RANK 0] [I] Use 25.65 GB for model weights.
[12/08/2025-04:53:16] [TRT-LLM] [RANK 0] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:53:16] [TRT-LLM] [RANK 0] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors to memory...
[12/08/2025-04:53:16] [TRT-LLM] [RANK 0] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors to memory...
[12/08/2025-04:53:17] [TRT-LLM] [RANK 1] [I] Use 25.65 GB for model weights.
[12/08/2025-04:53:17] [TRT-LLM] [RANK 1] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:53:17] [TRT-LLM] [RANK 1] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors to memory...
[12/08/2025-04:53:17] [TRT-LLM] [RANK 1] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors to memory...
[12/08/2025-04:53:17] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:53:17] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:53:19] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:53:19] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
[12/08/2025-04:53:25] [TRT-LLM] [W] Overriding kv_cache_config 
[12/08/2025-04:53:25] [TRT-LLM] [I] rank 0 for index 1 launch the disagg server
[12/08/2025-04:53:25] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:25] [TRT-LLM] [I] Using LLM with PyTorch backend
[12/08/2025-04:53:25] [TRT-LLM] [I] neither checkpoint_format nor checkpoint_loader were provided, checkpoint_format will be set to HF.
[12/08/2025-04:53:25] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:25] [TRT-LLM] [I] start MpiSession with 4 workers
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
You are using a model of type deepseek_v3 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
[12/08/2025-04:53:26] [TRT-LLM] [W] Failed to load hf generation config from DeepSeek-V3-Lite/bf16, encounter error: DeepSeek-V3-Lite/bf16 does not appear to have a file named generation_config.json. Checkout 'https://huggingface.co/DeepSeek-V3-Lite/bf16/tree/main' for available files.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:53:26] [TRT-LLM] [W] Orchestrator is creating IPC executor
[33;20mrank 0 using MpiCommSession to bind to external MPI processes
[0m[12/08/2025-04:53:26] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_queue
[12/08/2025-04:53:26] [TRT-LLM] [I] Generating a new HMAC key for server worker_init_status_queue
[12/08/2025-04:53:26] [TRT-LLM] [I] Generating a new HMAC key for server proxy_result_queue
[12/08/2025-04:53:26] [TRT-LLM] [I] Generating a new HMAC key for server proxy_stats_queue
[12/08/2025-04:53:26] [TRT-LLM] [I] Generating a new HMAC key for server proxy_kv_cache_events_queue
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[12/08/2025-04:53:28] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:53:28] [TRT-LLM] [RANK 3] [I] Worker process 6732 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] for optimal NUMA-aware scheduling.
[12/08/2025-04:53:28] [TRT-LLM] [RANK 2] [I] Worker process 6731 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] for optimal NUMA-aware scheduling.
[12/08/2025-04:53:28] [TRT-LLM] [RANK 4] [I] Worker process 6733 CPU affinity set to [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255] for optimal NUMA-aware scheduling.
[12/08/2025-04:53:28] [TRT-LLM] [RANK 5] [I] Worker process 6734 CPU affinity set to [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255] for optimal NUMA-aware scheduling.
[MPIDist::create_cp_comm] rank: 1, cp_rank: 0, cp_group: [1]
[MPIDist::create_tp_comm] rank: 1, tp_rank: 1, tp_group: [0, 1, 2, 3]
[MPIDist::create_cp_comm] rank: 0, cp_rank: 0, cp_group: [0]
[MPIDist::create_tp_comm] rank: 0, tp_rank: 0, tp_group: [0, 1, 2, 3]
[MPIDist::create_cp_comm] rank: 2, cp_rank: 0, cp_group: [2]
[MPIDist::create_tp_comm] rank: 2, tp_rank: 2, tp_group: [0, 1, 2, 3]
[MPIDist::create_cp_comm] rank: 3, cp_rank: 0, cp_group: [3]
[MPIDist::create_tp_comm] rank: 3, tp_rank: 3, tp_group: [0, 1, 2, 3]
[MPIDist::create_pp_comm] rank: 1, pp_rank: 0, pp_group: [1]
[MPIDist::create_pp_comm] rank: 0, pp_rank: 0, pp_group: [0]
[MPIDist::create_pp_comm] rank: 3, pp_rank: 0, pp_group: [3]
[12/08/2025-04:53:28] [TRT-LLM] [RANK 3] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[12/08/2025-04:53:28] [TRT-LLM] [RANK 2] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[MPIDist::create_pp_comm] rank: 2, pp_rank: 0, pp_group: [2]
[12/08/2025-04:53:28] [TRT-LLM] [RANK 5] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[12/08/2025-04:53:28] [TRT-LLM] [RANK 4] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:53:29] [TRT-LLM] [RANK 5] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:53:29] [TRT-LLM] [RANK 5] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[12/08/2025-04:53:29] [TRT-LLM] [RANK 4] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:53:29] [TRT-LLM] [RANK 4] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[12/08/2025-04:53:29] [TRT-LLM] [RANK 2] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:53:29] [TRT-LLM] [RANK 2] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:53:29] [TRT-LLM] [RANK 3] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:53:29] [TRT-LLM] [RANK 3] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[12/08/2025-04:53:30] [TRT-LLM] [RANK 4] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:53:30] [TRT-LLM] [RANK 5] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:53:30] [TRT-LLM] [RANK 2] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:53:30] [TRT-LLM] [RANK 3] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:53:30] [TRT-LLM] [RANK 4] [I] Use 13.18 GB for model weights.
[12/08/2025-04:53:30] [TRT-LLM] [RANK 5] [I] Use 13.18 GB for model weights.
[12/08/2025-04:53:30] [TRT-LLM] [RANK 3] [I] Use 13.18 GB for model weights.
[12/08/2025-04:53:30] [TRT-LLM] [RANK 2] [I] Use 13.18 GB for model weights.
[12/08/2025-04:53:30] [TRT-LLM] [RANK 4] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:53:30] [TRT-LLM] [RANK 4] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors to memory...
[12/08/2025-04:53:30] [TRT-LLM] [RANK 3] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:53:30] [TRT-LLM] [RANK 3] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors to memory...
[12/08/2025-04:53:30] [TRT-LLM] [RANK 2] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:53:30] [TRT-LLM] [RANK 5] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:53:30] [TRT-LLM] [RANK 2] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors to memory...
[12/08/2025-04:53:30] [TRT-LLM] [RANK 2] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors to memory...
[12/08/2025-04:53:30] [TRT-LLM] [RANK 5] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors to memory...
[12/08/2025-04:54:23] [TRT-LLM] [RANK 2] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors.
[12/08/2025-04:58:23] [TRT-LLM] [RANK 0] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors.
[12/08/2025-05:01:00] [TRT-LLM] [RANK 1] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors.
[12/08/2025-05:01:02] [TRT-LLM] [RANK 0] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors.
[12/08/2025-05:01:08] [TRT-LLM] [RANK 2] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors.
[12/08/2025-05:01:13] [TRT-LLM] [RANK 5] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors.
[12/08/2025-05:01:18] [TRT-LLM] [RANK 3] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors.
[12/08/2025-05:01:19] [TRT-LLM] [RANK 4] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors.
[12/08/2025-05:01:31] [TRT-LLM] [RANK 1] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors.
Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s]Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s]Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s]Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s]Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s][12/08/2025-05:01:31] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s][12/08/2025-05:01:31] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/08/2025-05:01:31] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
Loading safetensors weights in parallel:  22%|â–ˆâ–ˆâ–       | 2/9 [00:00<00:00, 13.41it/s]Loading safetensors weights in parallel:  22%|â–ˆâ–ˆâ–       | 2/9 [00:00<00:00, 12.89it/s]Loading safetensors weights in parallel:  22%|â–ˆâ–ˆâ–       | 2/9 [00:00<00:00, 12.04it/s]Loading safetensors weights in parallel:  22%|â–ˆâ–ˆâ–       | 2/9 [00:00<00:00, 10.07it/s]Loading safetensors weights in parallel:  22%|â–ˆâ–ˆâ–       | 2/9 [00:00<00:00,  9.20it/s]Loading safetensors weights in parallel:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 15.62it/s]Loading safetensors weights in parallel:  22%|â–ˆâ–ˆâ–       | 2/9 [00:00<00:00,  7.46it/s]Loading safetensors weights in parallel: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 30.18it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s][Linear::load_weights] self.weight_name: None
Loading safetensors weights in parallel:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 13.24it/s]Loading safetensors weights in parallel: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 28.45it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s][Linear::load_weights] self.weight_name: None
Loading safetensors weights in parallel:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 13.40it/s]Loading safetensors weights in parallel:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 12.53it/s]Loading safetensors weights in parallel: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 26.42it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s]Loading safetensors weights in parallel: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 26.25it/s]
[Linear::load_weights] self.weight_name: None
Loading safetensors weights in parallel: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 26.14it/s]
Loading safetensors weights in parallel:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:00<00:00, 11.92it/s]Loading weights:   0%|          | 0/813 [00:00<?, ?it/s][Linear::load_weights] self.weight_name: None
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s][Linear::load_weights] self.weight_name: None
Loading safetensors weights in parallel: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 25.44it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s][Linear::load_weights] self.weight_name: None
Loading weights:   0%|          | 3/813 [00:00<00:29, 27.68it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   0%|          | 3/813 [00:00<00:32, 24.93it/s]Loading weights:   0%|          | 3/813 [00:00<00:32, 25.16it/s]Loading weights:   0%|          | 3/813 [00:00<00:44, 18.24it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   0%|          | 3/813 [00:00<00:37, 21.73it/s]Loading weights:   0%|          | 3/813 [00:00<00:40, 19.85it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   5%|â–Œ         | 43/813 [00:00<00:04, 154.58it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   5%|â–Œ         | 43/813 [00:00<00:06, 126.67it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   5%|â–Œ         | 43/813 [00:00<00:06, 125.58it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   5%|â–Œ         | 43/813 [00:00<00:06, 121.42it/s]Loading weights:   5%|â–Œ         | 43/813 [00:00<00:06, 124.60it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   5%|â–Œ         | 43/813 [00:00<00:07, 103.77it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|â–Š         | 70/813 [00:00<00:04, 167.10it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|â–Š         | 70/813 [00:00<00:04, 152.56it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|â–Š         | 70/813 [00:00<00:05, 130.94it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|â–Š         | 70/813 [00:00<00:05, 131.24it/s]Loading weights:   9%|â–Š         | 70/813 [00:00<00:05, 133.17it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  12%|â–ˆâ–        | 97/813 [00:00<00:04, 171.49it/s]Loading weights:  12%|â–ˆâ–        | 97/813 [00:00<00:04, 171.99it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|â–Š         | 70/813 [00:00<00:07, 105.46it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  15%|â–ˆâ–Œ        | 124/813 [00:00<00:03, 182.81it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  12%|â–ˆâ–        | 97/813 [00:00<00:05, 131.30it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  15%|â–ˆâ–Œ        | 124/813 [00:00<00:03, 177.27it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  12%|â–ˆâ–        | 97/813 [00:00<00:05, 135.03it/s]Loading weights:  12%|â–ˆâ–        | 97/813 [00:00<00:05, 133.71it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|â–ˆâ–Š        | 151/813 [00:00<00:03, 191.35it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|â–ˆâ–Š        | 151/813 [00:00<00:03, 180.10it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  15%|â–ˆâ–Œ        | 124/813 [00:00<00:05, 132.40it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  12%|â–ˆâ–        | 97/813 [00:00<00:06, 106.01it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  15%|â–ˆâ–Œ        | 124/813 [00:00<00:05, 137.34it/s]Loading weights:  15%|â–ˆâ–Œ        | 124/813 [00:00<00:05, 136.54it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  22%|â–ˆâ–ˆâ–       | 178/813 [00:01<00:03, 187.29it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  22%|â–ˆâ–ˆâ–       | 178/813 [00:01<00:03, 183.16it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|â–ˆâ–Š        | 151/813 [00:01<00:04, 133.86it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 205/813 [00:01<00:03, 192.19it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|â–ˆâ–Š        | 151/813 [00:01<00:04, 139.67it/s]Loading weights:  19%|â–ˆâ–Š        | 151/813 [00:01<00:04, 139.11it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  15%|â–ˆâ–Œ        | 124/813 [00:01<00:06, 106.72it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 205/813 [00:01<00:03, 173.28it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  29%|â–ˆâ–ˆâ–Š       | 232/813 [00:01<00:02, 198.89it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  22%|â–ˆâ–ˆâ–       | 178/813 [00:01<00:04, 135.70it/s][Linear::load_weights] self.weight_name: None
Loading weights:  22%|â–ˆâ–ˆâ–       | 178/813 [00:01<00:04, 142.26it/s]Loading weights:  22%|â–ˆâ–ˆâ–       | 178/813 [00:01<00:04, 141.84it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  29%|â–ˆâ–ˆâ–Š       | 232/813 [00:01<00:03, 168.10it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 259/813 [00:01<00:02, 204.03it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|â–ˆâ–Š        | 151/813 [00:01<00:06, 107.05it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 205/813 [00:01<00:04, 149.47it/s]Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 205/813 [00:01<00:04, 149.20it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 286/813 [00:01<00:02, 204.87it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 259/813 [00:01<00:03, 164.74it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 205/813 [00:01<00:04, 127.29it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  29%|â–ˆâ–ˆâ–Š       | 232/813 [00:01<00:03, 155.52it/s]Loading weights:  29%|â–ˆâ–ˆâ–Š       | 232/813 [00:01<00:03, 155.26it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 313/813 [00:01<00:02, 190.93it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  22%|â–ˆâ–ˆâ–       | 178/813 [00:01<00:05, 107.45it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 286/813 [00:01<00:03, 159.50it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 259/813 [00:01<00:03, 161.07it/s]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 259/813 [00:01<00:03, 161.19it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  29%|â–ˆâ–ˆâ–Š       | 232/813 [00:01<00:04, 122.04it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 340/813 [00:01<00:02, 182.03it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 313/813 [00:01<00:03, 166.41it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 286/813 [00:01<00:03, 162.88it/s]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 286/813 [00:01<00:03, 162.96it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 205/813 [00:02<00:06, 100.33it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 367/813 [00:02<00:02, 175.46it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 340/813 [00:02<00:02, 171.95it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 259/813 [00:02<00:04, 118.98it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 313/813 [00:02<00:03, 155.02it/s]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 313/813 [00:02<00:03, 154.99it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 367/813 [00:02<00:02, 175.75it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 394/813 [00:02<00:02, 169.42it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 286/813 [00:02<00:04, 115.30it/s][Linear::load_weights] self.weight_name: None
Loading weights:  29%|â–ˆâ–ˆâ–Š       | 232/813 [00:02<00:06, 96.55it/s] [Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 394/813 [00:02<00:02, 177.81it/s][Linear::load_weights] self.weight_name: None
Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 421/813 [00:02<00:02, 180.11it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 340/813 [00:02<00:03, 151.31it/s]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 340/813 [00:02<00:03, 151.32it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 448/813 [00:02<00:01, 189.59it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 421/813 [00:02<00:02, 168.15it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 313/813 [00:02<00:04, 120.05it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 367/813 [00:02<00:03, 147.71it/s]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 367/813 [00:02<00:03, 147.58it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 475/813 [00:02<00:01, 198.05it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 259/813 [00:02<00:05, 94.46it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 448/813 [00:02<00:02, 164.65it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 502/813 [00:02<00:01, 202.93it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 340/813 [00:02<00:03, 124.63it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 394/813 [00:02<00:02, 144.44it/s]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 394/813 [00:02<00:02, 144.43it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 529/813 [00:02<00:01, 205.09it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 475/813 [00:02<00:02, 162.24it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 367/813 [00:02<00:03, 128.38it/s][Linear::load_weights] self.weight_name: None
Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 421/813 [00:02<00:02, 150.44it/s]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 421/813 [00:02<00:02, 150.41it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 286/813 [00:02<00:05, 91.31it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 556/813 [00:02<00:01, 208.77it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 502/813 [00:03<00:01, 157.72it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 448/813 [00:03<00:02, 155.38it/s]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 448/813 [00:03<00:02, 155.39it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/813 [00:03<00:01, 212.35it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 394/813 [00:03<00:03, 129.96it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 313/813 [00:03<00:05, 95.82it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 610/813 [00:03<00:00, 215.24it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 529/813 [00:03<00:01, 153.83it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 475/813 [00:03<00:02, 160.64it/s]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 475/813 [00:03<00:02, 160.63it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 421/813 [00:03<00:03, 122.05it/s][Linear::load_weights] self.weight_name: None
Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 637/813 [00:03<00:00, 196.68it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 502/813 [00:03<00:01, 161.69it/s]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 502/813 [00:03<00:01, 161.69it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 556/813 [00:03<00:01, 154.28it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 340/813 [00:03<00:04, 99.49it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 664/813 [00:03<00:00, 185.84it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 529/813 [00:03<00:01, 162.87it/s]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 529/813 [00:03<00:01, 162.83it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/813 [00:03<00:01, 154.95it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 448/813 [00:03<00:03, 118.50it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 367/813 [00:03<00:04, 102.27it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 691/813 [00:03<00:00, 178.77it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 556/813 [00:03<00:01, 163.40it/s]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 556/813 [00:03<00:01, 163.39it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 610/813 [00:03<00:01, 155.15it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 475/813 [00:03<00:02, 116.45it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 718/813 [00:03<00:00, 174.15it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/813 [00:03<00:01, 164.80it/s]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/813 [00:03<00:01, 164.78it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 637/813 [00:03<00:01, 162.33it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 394/813 [00:03<00:04, 104.07it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 745/813 [00:04<00:00, 171.05it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 610/813 [00:04<00:01, 168.10it/s]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 610/813 [00:04<00:01, 168.09it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 664/813 [00:04<00:00, 169.02it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 502/813 [00:04<00:02, 113.15it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 691/813 [00:04<00:00, 173.77it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 772/813 [00:04<00:00, 169.42it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 637/813 [00:04<00:01, 157.58it/s][Linear::load_weights] self.weight_name: None
Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 637/813 [00:04<00:01, 156.96it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 421/813 [00:04<00:04, 97.95it/s] [Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 718/813 [00:04<00:00, 178.19it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 799/813 [00:04<00:00, 167.84it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 529/813 [00:04<00:02, 110.69it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 813/813 [00:04<00:00, 183.63it/s]
Model init total -- 486.69s
Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 664/813 [00:04<00:00, 154.25it/s][Linear::load_weights] self.weight_name: None
Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 664/813 [00:04<00:00, 153.82it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 745/813 [00:04<00:00, 180.47it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 448/813 [00:04<00:03, 94.95it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 691/813 [00:04<00:00, 151.48it/s]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 691/813 [00:04<00:00, 151.74it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 556/813 [00:04<00:02, 110.55it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 772/813 [00:04<00:00, 182.02it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 799/813 [00:04<00:00, 182.20it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 718/813 [00:04<00:00, 148.70it/s]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 718/813 [00:04<00:00, 148.90it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 813/813 [00:04<00:00, 169.73it/s]
[Linear::load_weights] self.weight_name: None
Model init total -- 486.80s
[Linear::load_weights] self.weight_name: None
[12/08/2025-05:01:36] [TRT-LLM] [RANK 2] [I] max_seq_len is not specified, using inferred value 4096
Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 475/813 [00:04<00:03, 93.66it/s][Linear::load_weights] self.weight_name: None
Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/813 [00:04<00:02, 111.00it/s][Linear::load_weights] self.weight_name: None
[12/08/2025-05:01:36] [TRT-LLM] [RANK 2] [I] Using Sampler: TorchSampler
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 745/813 [00:04<00:00, 147.72it/s][Linear::load_weights] self.weight_name: None
Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 745/813 [00:04<00:00, 147.01it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 610/813 [00:05<00:01, 110.65it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 502/813 [00:05<00:03, 91.45it/s]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 772/813 [00:05<00:00, 148.07it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 772/813 [00:05<00:00, 147.31it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 637/813 [00:05<00:01, 116.00it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 799/813 [00:05<00:00, 146.02it/s]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 799/813 [00:05<00:00, 146.31it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 813/813 [00:05<00:00, 150.90it/s]
Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 813/813 [00:05<00:00, 151.25it/s]
[12/08/2025-05:01:37] [TRT-LLM] [RANK 3] [I] max_seq_len is not specified, using inferred value 4096
Model init total -- 487.90s
Model init total -- 487.79s
[12/08/2025-05:01:37] [TRT-LLM] [RANK 3] [I] Using Sampler: TorchSampler
Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 529/813 [00:05<00:03, 89.50it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 664/813 [00:05<00:01, 121.81it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 691/813 [00:05<00:00, 125.51it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 556/813 [00:05<00:02, 89.04it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 718/813 [00:05<00:00, 129.47it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[12/08/2025-05:01:37] [TRT-LLM] [RANK 5] [I] max_seq_len is not specified, using inferred value 4096
[12/08/2025-05:01:37] [TRT-LLM] [RANK 5] [I] Using Sampler: TorchSampler
[12/08/2025-05:01:37] [TRT-LLM] [RANK 4] [I] max_seq_len is not specified, using inferred value 4096
[12/08/2025-05:01:37] [TRT-LLM] [RANK 4] [I] Using Sampler: TorchSampler
[12/08/2025-05:01:37] [TRT-LLM] [RANK 4] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/08/2025-05:01:37] [TRT-LLM] [RANK 5] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/08/2025-05:01:37] [TRT-LLM] [RANK 3] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/08/2025-05:01:37] [TRT-LLM] [RANK 2] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60930, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.76 GiB for max tokens in paged KV cache (1949760).
[12/08/2025-05:01:37] [TRT-LLM] [RANK 2] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-05:01:37] [TRT-LLM] [RANK 2] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60930, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60930, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60930, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.76 GiB for max tokens in paged KV cache (1949760).
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.76 GiB for max tokens in paged KV cache (1949760).
[12/08/2025-05:01:37] [TRT-LLM] [RANK 4] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-05:01:37] [TRT-LLM] [RANK 5] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-05:01:37] [TRT-LLM] [RANK 4] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[12/08/2025-05:01:37] [TRT-LLM] [RANK 5] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.76 GiB for max tokens in paged KV cache (1949760).
[12/08/2025-05:01:37] [TRT-LLM] [RANK 3] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-05:01:37] [TRT-LLM] [RANK 3] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 583/813 [00:06<00:02, 89.28it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 745/813 [00:06<00:00, 130.56it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:44871
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 44871
[TensorRT-LLM][INFO][3] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][2] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][3] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:45557
[TensorRT-LLM][INFO][2] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:42261
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:33061
[TensorRT-LLM][INFO][3] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 45557
[TensorRT-LLM][INFO][2] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 42261
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 33061
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[12/08/2025-05:01:37] [TRT-LLM] [RANK 4] [I] Running autotuner warmup...
[12/08/2025-05:01:37] [TRT-LLM] [RANK 5] [I] Running autotuner warmup...
[12/08/2025-05:01:37] [TRT-LLM] [RANK 2] [I] Running autotuner warmup...
[12/08/2025-05:01:37] [TRT-LLM] [RANK 5] [I] [Autotuner] Autotuning process starts ...
[12/08/2025-05:01:37] [TRT-LLM] [RANK 4] [I] [Autotuner] Autotuning process starts ...
[12/08/2025-05:01:37] [TRT-LLM] [RANK 3] [I] Running autotuner warmup...
[12/08/2025-05:01:37] [TRT-LLM] [RANK 2] [I] [Autotuner] Autotuning process starts ...
[12/08/2025-05:01:37] [TRT-LLM] [RANK 3] [I] [Autotuner] Autotuning process starts ...
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2, block_ids: [256]
Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 772/813 [00:06<00:00, 132.21it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 610/813 [00:06<00:02, 89.38it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 799/813 [00:06<00:00, 132.57it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 813/813 [00:06<00:00, 123.80it/s]
Model init total -- 501.49s
Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 637/813 [00:06<00:01, 94.20it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 664/813 [00:06<00:01, 98.01it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[12/08/2025-05:01:38] [TRT-LLM] [RANK 0] [I] max_seq_len is not specified, using inferred value 4096
[12/08/2025-05:01:38] [TRT-LLM] [RANK 0] [I] Using Sampler: TorchSampler
[12/08/2025-05:01:38] [TRT-LLM] [RANK 0] [I] [kv cache manager] Primary/secondary blocks for window sizes set to {4096: (257, 0)} for estimation dry run
[12/08/2025-05:01:38] [TRT-LLM] [RANK 0] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=257, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.26 GiB for max tokens in paged KV cache (8224).
[12/08/2025-05:01:38] [TRT-LLM] [RANK 0] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-05:01:38] [TRT-LLM] [RANK 0] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 691/813 [00:07<00:01, 100.96it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 718/813 [00:07<00:00, 104.25it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 745/813 [00:07<00:00, 106.27it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 772/813 [00:07<00:00, 108.44it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 799/813 [00:08<00:00, 109.51it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 813/813 [00:08<00:00, 99.77it/s] 
Model init total -- 503.05s
[12/08/2025-05:01:40] [TRT-LLM] [RANK 1] [I] max_seq_len is not specified, using inferred value 4096
[12/08/2025-05:01:40] [TRT-LLM] [RANK 1] [I] Using Sampler: TorchSampler
[12/08/2025-05:01:40] [TRT-LLM] [RANK 1] [I] [kv cache manager] Primary/secondary blocks for window sizes set to {4096: (257, 0)} for estimation dry run
[12/08/2025-05:01:40] [TRT-LLM] [RANK 1] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=257, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.26 GiB for max tokens in paged KV cache (8224).
[12/08/2025-05:01:40] [TRT-LLM] [RANK 1] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-05:01:40] [TRT-LLM] [RANK 1] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:39451
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 39451
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:46841
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 46841
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[12/08/2025-05:01:40] [TRT-LLM] [RANK 1] [I] Running autotuner warmup...
[12/08/2025-05:01:40] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process starts ...
[12/08/2025-05:01:40] [TRT-LLM] [RANK 0] [I] Running autotuner warmup...
[12/08/2025-05:01:40] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process starts ...
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2, block_ids: [256]
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 109405184 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 109405184 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 109405184 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 109405184 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][INFO] Detecting local TP group for rank 3
[TensorRT-LLM][INFO] Detecting local TP group for rank 2
[TensorRT-LLM][INFO] Detecting local TP group for rank 1
[TensorRT-LLM][INFO] Detecting local TP group for rank 0
[TensorRT-LLM][INFO] TP group is intra-node for rank 1
[TensorRT-LLM][INFO] TP group is intra-node for rank 3
[TensorRT-LLM][INFO] TP group is intra-node for rank 0
[TensorRT-LLM][INFO] TP group is intra-node for rank 2
[TensorRT-LLM][INFO] Detecting local TP group for rank 1
[TensorRT-LLM][INFO] Detecting local TP group for rank 0
[TensorRT-LLM][INFO] TP group is intra-node for rank 1
[TensorRT-LLM][INFO] TP group is intra-node for rank 0
[12/08/2025-05:01:48] [TRT-LLM] [RANK 4] [I] [Autotuner] Autotuning process ends
[12/08/2025-05:01:48] [TRT-LLM] [RANK 4] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-05:01:48] [TRT-LLM] [RANK 2] [I] [Autotuner] Autotuning process ends
[12/08/2025-05:01:48] [TRT-LLM] [RANK 2] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-05:01:48] [TRT-LLM] [RANK 5] [I] [Autotuner] Autotuning process ends
[12/08/2025-05:01:48] [TRT-LLM] [RANK 5] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-05:01:48] [TRT-LLM] [RANK 3] [I] [Autotuner] Autotuning process ends
[12/08/2025-05:01:48] [TRT-LLM] [RANK 3] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-05:01:48] [TRT-LLM] [RANK 2] [I] global_steady_clock_offset at each rank: [0.0, 4.0000013541430235e-06, 1.00000761449337e-06, 2.00001522898674e-06]
[12/08/2025-05:01:48] [TRT-LLM] [RANK 2] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/08/2025-05:01:48] [TRT-LLM] [RANK 4] [I] Setting global_steady_clock_offset: 1.00000761449337e-06 seconds for rank 2
[12/08/2025-05:01:48] [TRT-LLM] [RANK 5] [I] Setting global_steady_clock_offset: 2.00001522898674e-06 seconds for rank 3
[12/08/2025-05:01:48] [TRT-LLM] [RANK 3] [I] Setting global_steady_clock_offset: 4.0000013541430235e-06 seconds for rank 1
[12/08/2025-05:01:48] [TRT-LLM] [RANK 2] [I] Setting PyTorch memory fraction to 0.7443831990177424 (199.26312255859375 GiB)
[12/08/2025-05:01:48] [TRT-LLM] [RANK 5] [I] Setting PyTorch memory fraction to 0.7448501589784339 (199.38812255859375 GiB)
[12/08/2025-05:01:48] [TRT-LLM] [RANK 4] [I] Setting PyTorch memory fraction to 0.7442664590275696 (199.23187255859375 GiB)
[12/08/2025-05:01:48] [TRT-LLM] [RANK 3] [I] Setting PyTorch memory fraction to 0.7442664590275696 (199.23187255859375 GiB)
[12/08/2025-05:01:48] [TRT-LLM] [RANK 2] [I] LLM Args:
model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=4 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-05:01:48] [TRT-LLM] [RANK 5] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=4 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-05:01:48] [TRT-LLM] [RANK 2] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=4 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-05:01:48] [TRT-LLM] [RANK 4] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=4 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-05:01:48] [TRT-LLM] [RANK 3] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=4 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-05:01:48] [TRT-LLM] [I] get signal from executor worker
[12/08/2025-05:01:48] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process ends
[12/08/2025-05:01:48] [TRT-LLM] [RANK 1] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-05:01:48] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process ends
[12/08/2025-05:01:48] [TRT-LLM] [RANK 0] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-05:01:48] [TRT-LLM] [RANK 0] [I] global_steady_clock_offset at each rank: [0.0, 0.0]
[12/08/2025-05:01:48] [TRT-LLM] [RANK 1] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 1
[12/08/2025-05:01:48] [TRT-LLM] [RANK 0] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/08/2025-05:01:48] [TRT-LLM] [RANK 1] [I] Memory used after loading model weights (inside torch) in memory usage profiling: 26.30 GiB
[12/08/2025-05:01:48] [TRT-LLM] [RANK 0] [I] Memory used after loading model weights (inside torch) in memory usage profiling: 26.30 GiB
[12/08/2025-05:01:48] [TRT-LLM] [RANK 1] [I] Memory used after loading model weights (outside torch) in memory usage profiling: 4.41 GiB
[12/08/2025-05:01:48] [TRT-LLM] [RANK 0] [I] Memory used after loading model weights (outside torch) in memory usage profiling: 5.77 GiB
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14445, 17155, 38291,  ..., 68250, 93408, 91993], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14445, 17155, 38291,  ..., 68250, 93408, 91993], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2049, block_ids: [255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2050, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2049, block_ids: [255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2050, block_ids: [256]
[32mINFO[0m:     Started server process [[36m8288[0m]
[32mINFO[0m:     Waiting for application startup.
[32mINFO[0m:     Application startup complete.
[32mINFO[0m:     Uvicorn running on [1mhttp://localhost:8002[0m (Press CTRL+C to quit)
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2048
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2049
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2050
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2048
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2049
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2050
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] Memory dynamically allocated during inference (inside torch) in memory usage profiling: 0.41 GiB
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] Memory used outside torch (e.g., NCCL and CUDA graphs) in memory usage profiling: 4.53 GiB
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] Memory dynamically allocated during inference (inside torch) in memory usage profiling: 0.41 GiB
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] Memory used outside torch (e.g., NCCL and CUDA graphs) in memory usage profiling: 5.89 GiB
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] Peak memory during memory usage profiling (torch + non-torch): 32.60 GiB, available KV cache memory when calculating max tokens: 58.84 GiB, fraction is set 0.25, kv size is 34560. device total memory 267.69 GiB, , tmp kv_mem 0.26 GiB
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] Estimated max memory in KV cache : 58.84 GiB
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [W] Both free_gpu_memory_fraction and max_tokens are set (to 0.25 and 1828062 with free memory 235.81573486328125GiB of total memory 267.68890380859375GiB, respectively). The smaller value will be used.
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] Peak memory during memory usage profiling (torch + non-torch): 31.24 GiB, available KV cache memory when calculating max tokens: 59.18 GiB, fraction is set 0.25, kv size is 34560. device total memory 267.69 GiB, , tmp kv_mem 0.26 GiB
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] Estimated max memory in KV cache : 59.18 GiB
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [W] Both free_gpu_memory_fraction and max_tokens are set (to 0.25 and 1838622 with free memory 237.17523193359375GiB of total memory 267.68890380859375GiB, respectively). The smaller value will be used.
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=57126, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 58.84 GiB for max tokens in paged KV cache (1828032).
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=57126, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 58.84 GiB for max tokens in paged KV cache (1828032).
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:39409
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:38497
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 38497
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 39409
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] Running autotuner warmup...
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] Running autotuner warmup...
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process starts ...
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process starts ...
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2, block_ids: [256]
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process ends
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process ends
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] global_steady_clock_offset at each rank: [0.0, -2.00001522898674e-06]
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] Setting global_steady_clock_offset: -2.00001522898674e-06 seconds for rank 1
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] Setting PyTorch memory fraction to 0.766439760911028 (205.16741943359375 GiB)
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] Setting PyTorch memory fraction to 0.7613611153229217 (203.80792236328125 GiB)
[12/08/2025-05:01:49] [TRT-LLM] [RANK 1] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=1838622, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=63542786048, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] LLM Args:
model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=1828062, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=63177848832, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-05:01:49] [TRT-LLM] [RANK 0] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=1828062, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=63177848832, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-05:01:49] [TRT-LLM] [I] get signal from executor worker
[32mINFO[0m:     Started server process [[36m8287[0m]
[32mINFO[0m:     Waiting for application startup.
[32mINFO[0m:     Application startup complete.
[32mINFO[0m:     Uvicorn running on [1mhttp://localhost:8001[0m (Press CTRL+C to quit)
[32mINFO[0m:     127.0.0.1:56470 - "[1mGET /health HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     127.0.0.1:47808 - "[1mGET /health HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     127.0.0.1:56482 - "[1mGET /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     127.0.0.1:47824 - "[1mGET /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[12/08/2025-05:01:51] [TRT-LLM] [I] The steady clock offset between local and disagg server: -0.0003675000189105049 second
[12/08/2025-05:01:51] [TRT-LLM] [I] The steady clock offset between local and disagg server: -0.0007744999893475324 second
[32mINFO[0m:     127.0.0.1:47836 - "[1mPOST /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     127.0.0.1:56494 - "[1mPOST /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([28803, 27194,   344,   270,  1606,  2112,  9059,   295,  6677,  5169,
         7677,   513, 30603, 26251,   538,  2883,  4577,    14, 21779, 26498,
        32329,    14,   305, 80124,    16,   983, 10401,   304, 35317, 10639,
           14, 15852, 41366,    14,   305, 15075,  9670,   396, 52498, 29810,
           14, 25676,    14,   305,  1482,    16, 12795, 12327,  5217,  4271,
          344,   223], device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([28803, 27194,   344,   270,  1606,  2112,  9059,   295,  6677,  5169,
         7677,   513, 30603, 26251,   538,  2883,  4577,    14, 21779, 26498,
        32329,    14,   305, 80124,    16,   983, 10401,   304, 35317, 10639,
           14, 15852, 41366,    14,   305, 15075,  9670,   396, 52498, 29810,
           14, 25676,    14,   305,  1482,    16, 12795, 12327,  5217,  4271,
          344,   223], device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]],
       device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  52, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]],
       device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  52, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[32mINFO[0m:     127.0.0.1:56470 - "[1mPOST /v1/completions HTTP/1.1[0m" [32m200 OK[0m
[ExecutorRequestQueue::_merge_helix_requests][rank 1][cp_rank 0]: input_ids_this_rank: [28803, 27194, 344, 270, 1606, 2112, 9059, 295, 6677, 5169, 7677, 513, 30603, 26251, 538, 2883, 4577, 14, 21779, 26498, 32329, 14, 305, 80124, 16, 983, 10401, 304, 35317, 10639, 14, 15852, 41366, 14, 305, 15075, 9670, 396, 52498, 29810, 14, 25676, 14, 305, 1482, 16, 12795, 12327, 5217, 4271, 344, 223]
[ExecutorRequestQueue::_merge_helix_requests][rank 1][cp_rank 0]: position_ids_this_rank: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
[TensorRT-LLM][INFO] Set logger level to INFO
[ExecutorRequestQueue::_merge_helix_requests][rank 0][cp_rank 0]: input_ids_this_rank: [28803, 27194, 344, 270, 1606, 2112, 9059, 295, 6677, 5169, 7677, 513, 30603, 26251, 538, 2883, 4577, 14, 21779, 26498, 32329, 14, 305, 80124, 16, 983, 10401, 304, 35317, 10639, 14, 15852, 41366, 14, 305, 15075, 9670, 396, 52498, 29810, 14, 25676, 14, 305, 1482, 16, 12795, 12327, 5217, 4271, 344, 223]
[ExecutorRequestQueue::_merge_helix_requests][rank 0][cp_rank 0]: position_ids_this_rank: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
[TensorRT-LLM][INFO] Set logger level to INFO
[ExecutorRequestQueue::_merge_helix_requests][rank 2][cp_rank 0]: input_ids_this_rank: [28803, 27194, 344, 270, 1606, 2112, 9059, 295, 6677, 5169, 7677, 513, 30603, 26251, 538, 2883, 4577, 14, 21779, 26498, 32329, 14, 305, 80124, 16, 983, 10401, 304, 35317, 10639, 14, 15852, 41366, 14, 305, 15075, 9670, 396, 52498, 29810, 14, 25676, 14, 305, 1482, 16, 12795, 12327, 5217, 4271, 344, 223]
[ExecutorRequestQueue::_merge_helix_requests][rank 2][cp_rank 0]: position_ids_this_rank: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
[ExecutorRequestQueue::_merge_helix_requests][rank 3][cp_rank 0]: input_ids_this_rank: [28803, 27194, 344, 270, 1606, 2112, 9059, 295, 6677, 5169, 7677, 513, 30603, 26251, 538, 2883, 4577, 14, 21779, 26498, 32329, 14, 305, 80124, 16, 983, 10401, 304, 35317, 10639, 14, 15852, 41366, 14, 305, 15075, 9670, 396, 52498, 29810, 14, 25676, 14, 305, 1482, 16, 12795, 12327, 5217, 4271, 344, 223]
[ExecutorRequestQueue::_merge_helix_requests][rank 3][cp_rank 0]: position_ids_this_rank: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 0][cp_rank 0]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([56840], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[52]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [52]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  53, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 1][cp_rank 0]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([56840], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[52]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [52]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  53, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 2][cp_rank 0]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([56840], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[52]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [52]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  53, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 3][cp_rank 0]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([56840], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[52]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [52]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  53, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:3', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank1_cp0_tp1_before_input_layernorm.pt
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:3', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank1_cp0_tp1_before_attn.pt
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:2', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank0_cp0_tp0_before_input_layernorm.pt
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:2', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank0_cp0_tp0_before_attn.pt
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 109405184 bytes to 361103360 bytes
[MLA::forward][rank 1][cp_rank 0][tp_rank 1]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[-3.6716e-05, -3.3264e-03,  8.2397e-03,  ...,  1.8387e-03,
          6.0654e-04,  3.8910e-04]], device='cuda:3', dtype=torch.bfloat16) 
 weight.shape: torch.Size([2560, 1024]) weight.tp_rank: 1 weight.tp_size: 4 
 Parameter containing:
tensor([[-0.0115, -0.0047,  0.0172,  ..., -0.0148, -0.0096, -0.0148],
        [ 0.0115,  0.0229,  0.0129,  ...,  0.0053, -0.0192,  0.0128],
        [ 0.0010, -0.0143, -0.0032,  ..., -0.0020, -0.0034,  0.0048],
        ...,
        [ 0.0065,  0.0177,  0.0022,  ..., -0.0220, -0.0240, -0.0027],
        [-0.0177,  0.0212,  0.0106,  ..., -0.0030,  0.0065, -0.0016],
        [ 0.0094,  0.0153, -0.0059,  ..., -0.0110,  0.0150,  0.0200]],
       device='cuda:3', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank1_cp0_tp1_before_o_proj.pt
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 109405184 bytes to 361103360 bytes
[MLA::forward][rank 0][cp_rank 0][tp_rank 0]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[-0.0023,  0.0041,  0.0032,  ..., -0.0003,  0.0005, -0.0003]],
       device='cuda:2', dtype=torch.bfloat16) 
 weight.shape: torch.Size([2560, 1024]) weight.tp_rank: 0 weight.tp_size: 4 
 Parameter containing:
tensor([[-0.0195, -0.0114,  0.0053,  ...,  0.0031,  0.0087, -0.0047],
        [-0.0090,  0.0244, -0.0041,  ..., -0.0034,  0.0087,  0.0062],
        [-0.0195,  0.0081,  0.0018,  ...,  0.0069, -0.0019, -0.0225],
        ...,
        [ 0.0072,  0.0008, -0.0087,  ..., -0.0074,  0.0130, -0.0112],
        [-0.0096, -0.0193, -0.0043,  ...,  0.0046, -0.0205,  0.0130],
        [ 0.0048, -0.0077, -0.0096,  ..., -0.0021,  0.0046,  0.0130]],
       device='cuda:2', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank1_cp0_tp1_before_o_proj_weight.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank0_cp0_tp0_before_o_proj.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank1_cp0_tp1_before_o_proj_bias.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank0_cp0_tp0_before_o_proj_weight.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank0_cp0_tp0_before_o_proj_bias.pt
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank2_cp0_tp2_before_input_layernorm.pt
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:5', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank2_cp0_tp2_before_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank3_cp0_tp3_before_input_layernorm.pt
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:5', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank3_cp0_tp3_before_attn.pt
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 109405184 bytes to 361103360 bytes
[MLA::forward][rank 2][cp_rank 0][tp_rank 2]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[ 4.4861e-03, -2.3499e-03, -4.8161e-05,  ..., -3.4142e-04,
          3.0975e-03, -2.2888e-03]], device='cuda:4', dtype=torch.bfloat16) 
 weight.shape: torch.Size([2560, 1024]) weight.tp_rank: 2 weight.tp_size: 4 
 Parameter containing:
tensor([[-0.0175,  0.0079,  0.0192,  ..., -0.0305, -0.0070,  0.0140],
        [-0.0114,  0.0017, -0.0087,  ..., -0.0070, -0.0023,  0.0376],
        [-0.0140, -0.0052,  0.0175,  ..., -0.0211,  0.0082,  0.0023],
        ...,
        [ 0.0077, -0.0034,  0.0308,  ...,  0.0037, -0.0055,  0.0073],
        [ 0.0171,  0.0032,  0.0103,  ..., -0.0134,  0.0009,  0.0195],
        [-0.0094, -0.0256,  0.0085,  ..., -0.0023, -0.0037,  0.0079]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank2_cp0_tp2_before_o_proj.pt
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 109405184 bytes to 361103360 bytes
[MLA::forward][rank 3][cp_rank 0][tp_rank 3]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[-0.0018, -0.0044, -0.0079,  ..., -0.0009, -0.0009,  0.0006]],
       device='cuda:5', dtype=torch.bfloat16) 
 weight.shape: torch.Size([2560, 1024]) weight.tp_rank: 3 weight.tp_size: 4 
 Parameter containing:
tensor([[-0.0082, -0.0330, -0.0181,  ..., -0.0035, -0.0055,  0.0109],
        [ 0.0002,  0.0181,  0.0027,  ...,  0.0085,  0.0008, -0.0009],
        [ 0.0165,  0.0027,  0.0062,  ..., -0.0035,  0.0101,  0.0233],
        ...,
        [ 0.0081,  0.0223,  0.0028,  ...,  0.0140, -0.0157, -0.0052],
        [-0.0112,  0.0071,  0.0152,  ..., -0.0026,  0.0114,  0.0175],
        [ 0.0264,  0.0243, -0.0203,  ..., -0.0070,  0.0044, -0.0192]],
       device='cuda:5', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank3_cp0_tp3_before_o_proj.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank2_cp0_tp2_before_o_proj_weight.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank3_cp0_tp3_before_o_proj_weight.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank2_cp0_tp2_before_o_proj_bias.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank3_cp0_tp3_before_o_proj_bias.pt
[MLA::forward][rank 1][cp_rank 0][tp_rank 1]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[ 0.0119, -0.0552, -0.0212,  ...,  0.0205, -0.0889,  0.0104]],
       device='cuda:3', dtype=torch.bfloat16)
[MLA::forward][rank 0][cp_rank 0][tp_rank 0]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[ 0.0119, -0.0552, -0.0212,  ...,  0.0205, -0.0889,  0.0104]],
       device='cuda:2', dtype=torch.bfloat16)
[MLA::forward][rank 2][cp_rank 0][tp_rank 2]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[ 0.0119, -0.0552, -0.0212,  ...,  0.0205, -0.0889,  0.0104]],
       device='cuda:4', dtype=torch.bfloat16)
[MLA::forward][rank 3][cp_rank 0][tp_rank 3]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[ 0.0119, -0.0552, -0.0212,  ...,  0.0205, -0.0889,  0.0104]],
       device='cuda:5', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank1_cp0_tp1_after_o_proj.pt
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[ 0.0119, -0.0552, -0.0212,  ...,  0.0205, -0.0889,  0.0104]],
       device='cuda:3', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank0_cp0_tp0_after_o_proj.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank2_cp0_tp2_after_o_proj.pt
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[ 0.0119, -0.0552, -0.0212,  ...,  0.0205, -0.0889,  0.0104]],
       device='cuda:2', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank3_cp0_tp3_after_o_proj.pt
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[ 0.0119, -0.0552, -0.0212,  ...,  0.0205, -0.0889,  0.0104]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank1_cp0_tp1_after_attn.pt
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[ 0.0119, -0.0552, -0.0212,  ...,  0.0205, -0.0889,  0.0104]],
       device='cuda:5', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank0_cp0_tp0_after_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank2_cp0_tp2_after_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank3_cp0_tp3_after_attn.pt
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3203, -0.2930,  0.2090,  ..., -0.0703, -0.3809,  0.3516]],
       device='cuda:3', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3203, -0.2930,  0.2090,  ..., -0.0703, -0.3809,  0.3516]],
       device='cuda:2', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3203, -0.2930,  0.2090,  ..., -0.0703, -0.3809,  0.3516]],
       device='cuda:4', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3203, -0.2930,  0.2090,  ..., -0.0703, -0.3809,  0.3516]],
       device='cuda:5', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank1_cp0_tp1_after_mlp.pt
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1396, -0.1318,  0.0742,  ..., -0.0298, -0.1680,  0.1455]],
       device='cuda:3', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank0_cp0_tp0_after_mlp.pt
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1396, -0.1318,  0.0742,  ..., -0.0298, -0.1680,  0.1455]],
       device='cuda:2', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank2_cp0_tp2_after_mlp.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank3_cp0_tp3_after_mlp.pt
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1396, -0.1318,  0.0742,  ..., -0.0298, -0.1680,  0.1455]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank1_cp0_tp1_residual.pt
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1396, -0.1318,  0.0742,  ..., -0.0298, -0.1680,  0.1455]],
       device='cuda:5', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank0_cp0_tp0_residual.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank2_cp0_tp2_residual.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/pureTP4_weightsave/rank3_cp0_tp3_residual.pt
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([8158], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[53]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [53]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  54, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([8158], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[53]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [53]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  54, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([8158], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[53]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [53]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  54, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([8158], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[53]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [53]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  54, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4895], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[54]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [54]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4895], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[54]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [54]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  55, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  55, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([4895], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[54]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [54]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  55, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([4895], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[54]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [54]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  55, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([34118], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[55]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [55]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([34118], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[55]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [55]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  56, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  56, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([34118], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[55]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [55]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  56, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([34118], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[55]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [55]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  56, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1536], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1536], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[56]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [56]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[56]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [56]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  57, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  57, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([1536], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[56]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [56]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  57, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([1536], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[56]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [56]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  57, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([26272], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([26272], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[57]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [57]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[57]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [57]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  58, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  58, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([26272], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[57]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [57]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  58, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([26272], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[57]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [57]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  58, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([28094], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([28094], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[58]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [58]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[58]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [58]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  59, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  59, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([28094], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[58]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [58]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  59, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([28094], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[58]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [58]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  59, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([271], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[59]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [59]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([271], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[59]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [59]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  60, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  60, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([271], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[59]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [59]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  60, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([271], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[59]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [59]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  60, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1536], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[60]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [60]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1536], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[60]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [60]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  61, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  61, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([1536], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[60]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [60]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  61, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([1536], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[60]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [60]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  61, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([223], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[61]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [61]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([223], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[61]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [61]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  62, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  62, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([223], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[61]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [61]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  62, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([223], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[61]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [61]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  62, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([643], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[62]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [62]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([643], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[62]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [62]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  63, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  63, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([643], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[62]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [62]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  63, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([643], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[62]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [62]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  63, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([21], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[63]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [63]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([21], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[63]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [63]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  64, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  64, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([21], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[63]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [63]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  64, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([21], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[63]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [63]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  64, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([54500], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[64]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [64]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([54500], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[64]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [64]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  65, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([54500], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  65, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[64]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [64]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  65, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([54500], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[64]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [64]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  65, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([16408], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[65]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [65]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([16408], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[65]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [65]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  66, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  66, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([16408], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[65]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [65]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  66, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([16408], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[65]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [65]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  66, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([66337], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[66]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [66]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([66337], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[66]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [66]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  67, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  67, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([66337], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[66]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [66]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  67, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([66337], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[66]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [66]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  67, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6485], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6485], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[67]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [67]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[67]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [67]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  68, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  68, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([6485], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[67]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [67]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  68, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([6485], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[67]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [67]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  68, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([11218], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[68]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [68]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([11218], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[68]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [68]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  69, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  69, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([11218], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[68]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [68]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  69, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([11218], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[68]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [68]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  69, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([17665], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[69]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [69]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([17665], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[69]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [69]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  70, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  70, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([17665], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[69]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [69]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  70, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([17665], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[69]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [69]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  70, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([271], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[70]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [70]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([271], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[70]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [70]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  71, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  71, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([271], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[70]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [70]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  71, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([271], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[70]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [70]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  71, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([671], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([671], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[71]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [71]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[71]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [71]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  72, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  72, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([671], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[71]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [71]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  72, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([671], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[71]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [71]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  72, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4680], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4680], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[72]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [72]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[72]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [72]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  73, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  73, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([4680], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[72]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [72]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  73, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([4680], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[72]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [72]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  73, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[73]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [73]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[73]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [73]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  74, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  74, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[73]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [73]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  74, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[73]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [73]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  74, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5217], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5217], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[74]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [74]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[74]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [74]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  75, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  75, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([5217], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[74]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [74]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  75, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([5217], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[74]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [74]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  75, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([27194], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([27194], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[75]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [75]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[75]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [75]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  76, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  76, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([27194], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[75]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [75]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  76, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([27194], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[75]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [75]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  76, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([477], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([477], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[76]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [76]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[76]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [76]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  77, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  77, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([477], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[76]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [76]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  77, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([477], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[76]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [76]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  77, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[77]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [77]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[77]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [77]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  78, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  78, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[77]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [77]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  78, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[77]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [77]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  78, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([7537], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([7537], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[78]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [78]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[78]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [78]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  79, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  79, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([7537], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[78]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [78]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  79, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([7537], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[78]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [78]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  79, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([305], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[79]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [79]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([305], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[79]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [79]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  80, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([305], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  80, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[79]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [79]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  80, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([305], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[79]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [79]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  80, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([2799], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[80]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [80]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([2799], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[80]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [80]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  81, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  81, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([2799], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[80]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [80]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  81, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([2799], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[80]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [80]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  81, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4123], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[81]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [81]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4123], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[81]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [81]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  82, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  82, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([4123], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[81]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [81]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  82, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([4123], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[81]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [81]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  82, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([7677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[82]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [82]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([7677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[82]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [82]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  83, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  83, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([7677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[82]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [82]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  83, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([7677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[82]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [82]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  83, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([343], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[83]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [83]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([343], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[83]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [83]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  84, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([343], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  84, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[83]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [83]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  84, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([343], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[83]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [83]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  84, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([34973], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[84]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [84]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([34973], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[84]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [84]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  85, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  85, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([34973], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[84]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [84]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  85, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([34973], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[84]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [84]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  85, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([367], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[85]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [85]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([367], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[85]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [85]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  86, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  86, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([367], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[85]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [85]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  86, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([367], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[85]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [85]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  86, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([469], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[86]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [86]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([469], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[86]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [86]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  87, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([469], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  87, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[86]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [86]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  87, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([469], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[86]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [86]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  87, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([39602], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[87]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [87]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([39602], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[87]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [87]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  88, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([39602], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  88, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[87]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [87]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  88, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([39602], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[87]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [87]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  88, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([11], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[88]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [88]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([11], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[88]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [88]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  89, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([11], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  89, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[88]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [88]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  89, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([11], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[88]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [88]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  89, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([513], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([513], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[89]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [89]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[89]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [89]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  90, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  90, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([513], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[89]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [89]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  90, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([513], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[89]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [89]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  90, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([2883], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[90]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [90]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([2883], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[90]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [90]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  91, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  91, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([2883], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([2883], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[90]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [90]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[90]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [90]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  91, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  91, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([15610], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[91]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [91]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([15610], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[91]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [91]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  92, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  92, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([15610], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[91]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [91]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  92, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([15610], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[91]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [91]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  92, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[92]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [92]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[92]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [92]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  93, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  93, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[92]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [92]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  93, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[92]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [92]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  93, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([30603], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[93]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [93]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([30603], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[93]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [93]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  94, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  94, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([30603], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[93]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [93]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  94, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([30603], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[93]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [93]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  94, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([26251], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[94]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [94]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([26251], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[94]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [94]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  95, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  95, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([26251], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[94]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [94]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  95, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([26251], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[94]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [94]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  95, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([16], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[95]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [95]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([16], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[95]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [95]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  96, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([16], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  96, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[95]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [95]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  96, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([16], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[95]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [95]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  96, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([2454], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[96]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [96]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([2454], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[96]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [96]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  97, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([2454], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  97, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[96]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [96]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  97, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([2454], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[96]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [96]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  97, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[97]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [97]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[97]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [97]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  98, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  98, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[97]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [97]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  98, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[97]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [97]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  98, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([260], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([260], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[98]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [98]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[98]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [98]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  99, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  99, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([260], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[98]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [98]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([  99, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([260], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[98]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [98]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([  99, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([9356], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[99]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [99]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([9356], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[99]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [99]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 100, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([9356], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 100, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[99]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [99]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 100, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([9356], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[99]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [99]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 100, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([25870], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[100]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [100]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([25870], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[100]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [100]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 101, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([25870], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 101, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[100]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [100]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 101, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([25870], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[100]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [100]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 101, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([396], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[101]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [101]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([396], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[101]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [101]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 102, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 102, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([396], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[101]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [101]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 102, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([396], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[101]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [101]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 102, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([7769], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[102]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [102]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([7769], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[102]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [102]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 103, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 103, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([7769], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[102]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [102]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 103, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([7769], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[102]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [102]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 103, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([2757], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[103]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [103]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([2757], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[103]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [103]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 104, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 104, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([2757], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[103]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [103]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 104, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([2757], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[103]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [103]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 104, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[104]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [104]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[104]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [104]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 105, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 105, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[104]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [104]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 105, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[104]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [104]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 105, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([22688], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[105]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [105]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([22688], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[105]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [105]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 106, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 106, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([22688], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[105]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [105]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 106, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([22688], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[105]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [105]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 106, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([305], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[106]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [106]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([305], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[106]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [106]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 107, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 107, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([305], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[106]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [106]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 107, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([305], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[106]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [106]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 107, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([396], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[107]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [107]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([396], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[107]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [107]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 108, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 108, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([396], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[107]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [107]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 108, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([396], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[107]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [107]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 108, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([2883], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[108]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [108]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([2883], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[108]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [108]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 109, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([2883], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 109, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[108]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [108]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 109, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([2883], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[108]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [108]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 109, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4768], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[109]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [109]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4768], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[109]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [109]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 110, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([4768], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 110, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[109]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [109]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 110, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([4768], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[109]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [109]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 110, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([477], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[110]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [110]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([477], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[110]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [110]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 111, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([477], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 111, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[110]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [110]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 111, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([477], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[110]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [110]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 111, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[111]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [111]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[111]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [111]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 112, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 112, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[111]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [111]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 112, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[111]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [111]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 112, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6074], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[112]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [112]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6074], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[112]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [112]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 113, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 113, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([6074], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[112]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [112]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 113, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([6074], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[112]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [112]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 113, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([12789], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([12789], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[113]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [113]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[113]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [113]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 114, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 114, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([12789], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[113]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [113]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 114, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([12789], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[113]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [113]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 114, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([16], device='cuda:3', dtype=torch.int32)
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[114]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [114]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([16], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[114]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [114]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 115, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 115, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([16], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[114]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [114]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 115, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([16], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[114]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [114]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 115, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([764], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([764], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[115]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [115]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[115]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [115]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 116, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 116, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([764], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[115]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [115]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([764], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 116, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[115]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [115]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 116, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([19], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[116]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [116]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([19], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[116]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [116]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 117, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 117, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([19], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([19], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[116]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [116]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[116]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [116]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 117, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 117, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([63], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[117]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [117]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([63], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[117]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [117]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 118, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 118, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([63], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([63], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[117]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [117]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[117]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [117]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 118, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 118, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([764], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([764], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[118]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [118]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[118]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [118]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 119, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 119, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([764], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([764], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[118]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [118]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[118]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [118]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 119, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 119, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([20], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[119]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [119]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([20], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[119]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [119]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 120, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 120, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([20], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([20], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[119]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [119]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[119]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [119]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 120, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 120, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([63], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[120]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [120]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([63], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[120]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [120]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 121, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 121, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([63], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[120]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [120]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([63], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[120]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [120]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 121, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 121, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([764], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([764], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[121]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [121]
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[121]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [121]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 122, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 122, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([764], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[121]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [121]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 122, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([764], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[121]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [121]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 122, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([21], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([21], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[122]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [122]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[122]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [122]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 123, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 123, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([21], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[122]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [122]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 123, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([21], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[122]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [122]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 123, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([63], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([63], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[123]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [123]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[123]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [123]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 124, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 124, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([63], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[123]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [123]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 124, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([63], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[123]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [123]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 124, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([455], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[124]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [124]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([455], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[124]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [124]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 125, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 125, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([455], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[124]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [124]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 125, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([455], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[124]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [124]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 125, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1840], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[125]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [125]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1840], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[125]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [125]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 126, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 126, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([1840], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[125]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [125]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 126, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([1840], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[125]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [125]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 126, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([12789], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([12789], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[126]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [126]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[126]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [126]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 127, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 127, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([12789], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[126]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [126]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 127, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([12789], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[126]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [126]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 127, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[127]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[127]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [127]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 128, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 128, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[127]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [127]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 128, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[127]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [127]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 128, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5167], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[128]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [128]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5167], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[128]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [128]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 129, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 129, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([5167], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[128]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [128]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 129, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([5167], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[128]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [128]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 129, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([30603], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([30603], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[129]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [129]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[129]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [129]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 130, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 130, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([30603], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[129]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [129]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 130, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([30603], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[129]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [129]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 130, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([26251], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([26251], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[130]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [130]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[130]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [130]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 131, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 131, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([26251], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[130]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [130]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 131, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([26251], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[130]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [130]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 131, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([295], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([295], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[131]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [131]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[131]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [131]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 132, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 132, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([295], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[131]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [131]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 132, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([295], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[131]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [131]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 132, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[132]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [132]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[132]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [132]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 133, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 133, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[132]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [132]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 133, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[132]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [132]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 133, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[133]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [133]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[133]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [133]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 134, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 134, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[133]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [133]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 134, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[133]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [133]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 134, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[134]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [134]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[134]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [134]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 135, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 135, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[134]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [134]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 135, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[134]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [134]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 135, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([2883], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[135]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [135]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([2883], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[135]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [135]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 136, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 136, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([2883], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[135]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [135]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 136, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([2883], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[135]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [135]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 136, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4588], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4588], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[136]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [136]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[136]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [136]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 137, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 137, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([4588], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[136]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [136]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 137, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([4588], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[136]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [136]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 137, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([671], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([671], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[137]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [137]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[137]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [137]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 138, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 138, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([671], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[137]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [137]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 138, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([671], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[137]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [137]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 138, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4680], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4680], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[138]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [138]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[138]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [138]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 139, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 139, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([4680], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[138]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [138]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 139, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([4680], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[138]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [138]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 139, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[139]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [139]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[139]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [139]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 140, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 140, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[139]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [139]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 140, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[139]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [139]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 140, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5217], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5217], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[140]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [140]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[140]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [140]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 141, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 141, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([5217], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[140]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [140]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 141, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([5217], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[140]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [140]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 141, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([27194], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([27194], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[141]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [141]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[141]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [141]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 142, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 142, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([27194], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[141]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [141]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 142, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([27194], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[141]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [141]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 142, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([477], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[142]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [142]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([477], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[142]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [142]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 143, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 143, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([477], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[142]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [142]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 143, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([477], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[142]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [142]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 143, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[143]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [143]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[143]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [143]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 144, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 144, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[143]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [143]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 144, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[143]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [143]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 144, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([7537], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[144]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [144]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([7537], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[144]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [144]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 145, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 145, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([7537], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[144]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [144]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 145, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([7537], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[144]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [144]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 145, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([305], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([305], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[145]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [145]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[145]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [145]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 146, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 146, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([305], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[145]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [145]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 146, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([305], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[145]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [145]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 146, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([2799], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([2799], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[146]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [146]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[146]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [146]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 147, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 147, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([2799], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[146]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [146]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 147, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([2799], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[146]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [146]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 147, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4123], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4123], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[147]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [147]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[147]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [147]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 148, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 148, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([4123], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[147]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [147]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 148, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([4123], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[147]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [147]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 148, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([7677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([7677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[148]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [148]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[148]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [148]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 149, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 149, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([7677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[148]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [148]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 149, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([7677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[148]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [148]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 149, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([343], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([343], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[149]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [149]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[149]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [149]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 150, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 150, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([343], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[149]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [149]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 150, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([343], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[149]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [149]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 150, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([34973], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([34973], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[150]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [150]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[150]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [150]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([ 151, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([ 151, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] input_ids: tensor([34973], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] position_ids: tensor([[150]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [150]
[ResourceManager::prepare_resources][rank 3][cp_rank 0] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] kv_lens_cuda: tensor([ 151, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] input_ids: tensor([34973], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] position_ids: tensor([[150]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [150]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] kv_lens_cuda: tensor([ 151, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 0] request_id: 2048, block_ids: [257, 258, 259, 260, 261]
[32mINFO[0m:     127.0.0.1:50326 - "[1mPOST /v1/completions HTTP/1.1[0m" [32m200 OK[0m
