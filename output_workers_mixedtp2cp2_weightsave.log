/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[1765168559.172272] [umb-b300-020:1911 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168559.172314] [umb-b300-020:1906 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168559.781496] [umb-b300-020:1908 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168559.782859] [umb-b300-020:1909 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168560.216833] [umb-b300-020:1907 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168560.292581] [umb-b300-020:1910 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168560.455674] [umb-b300-020:1910 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168560.455683] [umb-b300-020:1909 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168560.455795] [umb-b300-020:1907 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168560.455863] [umb-b300-020:1906 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168560.455916] [umb-b300-020:1908 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168560.455948] [umb-b300-020:1911 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[12/08/2025-04:36:06] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:36:06] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:36:06] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:36:06] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:36:07] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:36:07] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:36:07] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:36:07] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:36:07] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:36:07] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:36:07] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:36:07] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:36:07] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:36:07] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:36:07] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:36:07] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:36:07] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:36:08] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:36:08] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:36:08] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:36:08] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:36:08] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:36:08] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:36:08] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:36:08] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/08/2025-04:36:08] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/08/2025-04:36:08] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/08/2025-04:36:08] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/08/2025-04:36:08] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/08/2025-04:36:08] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
env_global_rank: 0, set device_id: 0 before importing mpi4py
env_global_rank: 3, set device_id: 3 before importing mpi4py
env_global_rank: 2, set device_id: 2 before importing mpi4py
env_global_rank: 4, set device_id: 4 before importing mpi4py
env_global_rank: 1, set device_id: 1 before importing mpi4py
env_global_rank: 5, set device_id: 5 before importing mpi4py
[2025-12-08 04:36:12] INFO disagg_utils.py:318: global_rank: 3, instance_idx: 1, sub_rank: 1, is_leader: False
[2025-12-08 04:36:12] INFO disagg_utils.py:318: global_rank: 2, instance_idx: 1, sub_rank: 0, is_leader: True
[2025-12-08 04:36:12] INFO disagg_utils.py:318: global_rank: 4, instance_idx: 1, sub_rank: 2, is_leader: False
[2025-12-08 04:36:12] INFO disagg_utils.py:318: global_rank: 1, instance_idx: 0, sub_rank: 1, is_leader: False
[2025-12-08 04:36:12] INFO disagg_utils.py:318: global_rank: 0, instance_idx: 0, sub_rank: 0, is_leader: True
[2025-12-08 04:36:12] INFO disagg_utils.py:318: global_rank: 5, instance_idx: 1, sub_rank: 3, is_leader: False
[12/08/2025-04:36:12] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:12] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 3, sub-comm MPI rank: 1
[12/08/2025-04:36:12] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:12] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 2, sub-comm MPI rank: 0
[12/08/2025-04:36:12] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:12] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 4, sub-comm MPI rank: 2
[12/08/2025-04:36:12] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:12] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 1, sub-comm MPI rank: 1
[12/08/2025-04:36:12] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:12] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 0, sub-comm MPI rank: 0
[12/08/2025-04:36:12] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:12] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 5, sub-comm MPI rank: 3
[12/08/2025-04:36:12] [TRT-LLM] [W] Overriding kv_cache_config 
[12/08/2025-04:36:12] [TRT-LLM] [W] Overriding kv_cache_config 
[12/08/2025-04:36:12] [TRT-LLM] [I] rank 0 step1: preparing to launch command: ['python3', '/home/bbuddharaju/.local/bin/trtllm-serve', 'disaggregated_mpi_worker', '-c', '/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tests/integration/defs/disaggregated/test_configs/disagg_config_ctxtp2_gentp1cp2_deepseek_v3_lite_bf16_tllm_gen.yaml', '--log_level', 'info']
[12/08/2025-04:36:12] [TRT-LLM] [I] rank 0 step1: preparing to launch command: ['python3', '/home/bbuddharaju/.local/bin/trtllm-serve', 'disaggregated_mpi_worker', '-c', '/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tests/integration/defs/disaggregated/test_configs/disagg_config_ctxtp2_gentp1cp2_deepseek_v3_lite_bf16_tllm_gen.yaml', '--log_level', 'info']
[12/08/2025-04:36:12] [TRT-LLM] [I] Parent process (PID 1906) launched child process (PID 3465).
[12/08/2025-04:36:12] [TRT-LLM] [I] Parent process (PID 1908) launched child process (PID 3464).
[12/08/2025-04:36:12] [TRT-LLM] [I] rank 0 step2: start the mpi session server
[12/08/2025-04:36:12] [TRT-LLM] [I] rank 0 step2: start the mpi session server
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[1765168579.274387] [umb-b300-020:3465 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168579.438371] [umb-b300-020:3465 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168580.053707] [umb-b300-020:3464 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765168580.217591] [umb-b300-020:3464 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[12/08/2025-04:36:26] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:36:26] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/08/2025-04:36:26] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:36:26] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:36:26] [TRT-LLM] [I] cutlass dsl is available
[12/08/2025-04:36:26] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/08/2025-04:36:27] [TRT-LLM] [I] Starting TensorRT LLM init.
[12/08/2025-04:36:27] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[12/08/2025-04:36:27] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/08/2025-04:36:27] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
[12/08/2025-04:36:28] [TRT-LLM] [W] Overriding kv_cache_config 
[12/08/2025-04:36:28] [TRT-LLM] [I] rank 0 for index 0 launch the disagg server
[12/08/2025-04:36:28] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:28] [TRT-LLM] [I] Using LLM with PyTorch backend
[12/08/2025-04:36:28] [TRT-LLM] [I] neither checkpoint_format nor checkpoint_loader were provided, checkpoint_format will be set to HF.
[12/08/2025-04:36:28] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:28] [TRT-LLM] [I] start MpiSession with 2 workers
[12/08/2025-04:36:28] [TRT-LLM] [W] Overriding kv_cache_config 
[12/08/2025-04:36:28] [TRT-LLM] [I] rank 0 for index 1 launch the disagg server
[12/08/2025-04:36:28] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:28] [TRT-LLM] [I] Using LLM with PyTorch backend
[12/08/2025-04:36:28] [TRT-LLM] [I] neither checkpoint_format nor checkpoint_loader were provided, checkpoint_format will be set to HF.
[12/08/2025-04:36:28] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:28] [TRT-LLM] [I] start MpiSession with 4 workers
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
You are using a model of type deepseek_v3 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type deepseek_v3 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
[12/08/2025-04:36:29] [TRT-LLM] [W] Failed to load hf generation config from DeepSeek-V3-Lite/bf16, encounter error: DeepSeek-V3-Lite/bf16 does not appear to have a file named generation_config.json. Checkout 'https://huggingface.co/DeepSeek-V3-Lite/bf16/tree/main' for available files.
[12/08/2025-04:36:29] [TRT-LLM] [W] Failed to load hf generation config from DeepSeek-V3-Lite/bf16, encounter error: DeepSeek-V3-Lite/bf16 does not appear to have a file named generation_config.json. Checkout 'https://huggingface.co/DeepSeek-V3-Lite/bf16/tree/main' for available files.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:36:29] [TRT-LLM] [W] Orchestrator is creating IPC executor
[33;20mrank 0 using MpiCommSession to bind to external MPI processes
[0m[12/08/2025-04:36:29] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_queue
[12/08/2025-04:36:29] [TRT-LLM] [I] Generating a new HMAC key for server worker_init_status_queue
[12/08/2025-04:36:29] [TRT-LLM] [I] Generating a new HMAC key for server proxy_result_queue
[12/08/2025-04:36:29] [TRT-LLM] [I] Generating a new HMAC key for server proxy_stats_queue
[12/08/2025-04:36:29] [TRT-LLM] [I] Generating a new HMAC key for server proxy_kv_cache_events_queue
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:36:29] [TRT-LLM] [W] Orchestrator is creating IPC executor
[33;20mrank 0 using MpiCommSession to bind to external MPI processes
[0m[12/08/2025-04:36:29] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_queue
[12/08/2025-04:36:29] [TRT-LLM] [I] Generating a new HMAC key for server worker_init_status_queue
[12/08/2025-04:36:29] [TRT-LLM] [I] Generating a new HMAC key for server proxy_result_queue
[12/08/2025-04:36:29] [TRT-LLM] [I] Generating a new HMAC key for server proxy_stats_queue
[12/08/2025-04:36:29] [TRT-LLM] [I] Generating a new HMAC key for server proxy_kv_cache_events_queue
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[12/08/2025-04:36:30] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[12/08/2025-04:36:30] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/08/2025-04:36:30] [TRT-LLM] [RANK 0] [I] Worker process 1906 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] for optimal NUMA-aware scheduling.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 1] [I] Worker process 1907 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] for optimal NUMA-aware scheduling.
[MPIDist::create_cp_comm] rank: 0, cp_rank: 0, cp_group: [0]
[MPIDist::create_tp_comm] rank: 0, tp_rank: 0, tp_group: [0, 1]
[MPIDist::create_cp_comm] rank: 1, cp_rank: 0, cp_group: [1]
[MPIDist::create_tp_comm] rank: 1, tp_rank: 1, tp_group: [0, 1]
[MPIDist::create_pp_comm] rank: 0, pp_rank: 0, pp_group: [0]
[MPIDist::create_pp_comm] rank: 1, pp_rank: 0, pp_group: [1]
[12/08/2025-04:36:30] [TRT-LLM] [RANK 0] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[12/08/2025-04:36:30] [TRT-LLM] [RANK 1] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:36:30] [TRT-LLM] [RANK 0] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:36:30] [TRT-LLM] [RANK 0] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:36:30] [TRT-LLM] [RANK 1] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:36:30] [TRT-LLM] [RANK 1] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 4] [I] Worker process 1910 CPU affinity set to [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255] for optimal NUMA-aware scheduling.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 2] [I] Worker process 1908 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] for optimal NUMA-aware scheduling.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 3] [I] Worker process 1909 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] for optimal NUMA-aware scheduling.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 5] [I] Worker process 1911 CPU affinity set to [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255] for optimal NUMA-aware scheduling.
[MPIDist::create_cp_comm] rank: 2, cp_rank: 1, cp_group: [0, 2]
[MPIDist::create_cp_comm] rank: 0, cp_rank: 0, cp_group: [0, 2]
[12/08/2025-04:36:30] [TRT-LLM] [RANK 2] [I] [MPIDist::__init__] Repurposing CP ranks to TP for Helix.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 4] [I] [MPIDist::__init__] Repurposing CP ranks to TP for Helix.
[MPIDist::create_tp_comm] rank: 0, tp_rank: 0, tp_group: [0, 1, 2, 3]
[MPIDist::create_tp_comm] rank: 2, tp_rank: 2, tp_group: [0, 1, 2, 3]
[MPIDist::create_cp_comm] rank: 3, cp_rank: 1, cp_group: [1, 3]
[MPIDist::create_cp_comm] rank: 1, cp_rank: 0, cp_group: [1, 3]
[12/08/2025-04:36:30] [TRT-LLM] [RANK 5] [I] [MPIDist::__init__] Repurposing CP ranks to TP for Helix.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 3] [I] [MPIDist::__init__] Repurposing CP ranks to TP for Helix.
[MPIDist::create_tp_comm] rank: 3, tp_rank: 3, tp_group: [0, 1, 2, 3]
[MPIDist::create_tp_comm] rank: 1, tp_rank: 1, tp_group: [0, 1, 2, 3]
[MPIDist::create_pp_comm] rank: 3, pp_rank: 0, pp_group: [3]
[MPIDist::create_pp_comm] rank: 1, pp_rank: 0, pp_group: [1]
[MPIDist::create_pp_comm] rank: 0, pp_rank: 0, pp_group: [0]
[MPIDist::create_pp_comm] rank: 2, pp_rank: 0, pp_group: [2]
[12/08/2025-04:36:30] [TRT-LLM] [RANK 5] [I] [MPIDist::__init__] Restoring original mapping undoing Helix manipulation.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 2] [I] [MPIDist::__init__] Restoring original mapping undoing Helix manipulation.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 3] [I] [MPIDist::__init__] Restoring original mapping undoing Helix manipulation.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 5] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[12/08/2025-04:36:30] [TRT-LLM] [RANK 2] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[12/08/2025-04:36:30] [TRT-LLM] [RANK 4] [I] [MPIDist::__init__] Restoring original mapping undoing Helix manipulation.
[12/08/2025-04:36:30] [TRT-LLM] [RANK 3] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[12/08/2025-04:36:30] [TRT-LLM] [RANK 4] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:36:30] [TRT-LLM] [RANK 5] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:36:30] [TRT-LLM] [RANK 5] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[DeepseekV3ForCausalLM::__init__] Repurposing KVP ranks to TP while keeping other details the same.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[DeepseekV3ForCausalLM::__init__] Repurposing KVP ranks to TP while keeping other details the same.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[DeepseekV3ForCausalLM::__init__] Repurposing KVP ranks to TP while keeping other details the same.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[DeepseekV3ForCausalLM::__init__] Repurposing KVP ranks to TP while keeping other details the same.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 0] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:36:31] [TRT-LLM] [RANK 1] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:36:31] [TRT-LLM] [RANK 0] [I] Use 25.65 GB for model weights.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 0] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 0] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors to memory...
[12/08/2025-04:36:31] [TRT-LLM] [RANK 0] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors to memory...
[12/08/2025-04:36:31] [TRT-LLM] [RANK 1] [I] Use 25.65 GB for model weights.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 1] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 1] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors to memory...
[12/08/2025-04:36:31] [TRT-LLM] [RANK 1] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors to memory...
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[DeepseekV3ForCausalLM::__init__] Restoring original mapping.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [I] Use 13.49 GB for model weights.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors to memory...
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 2] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors to memory...
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[DeepseekV3ForCausalLM::__init__] Restoring original mapping.
[DeepseekV3ForCausalLM::__init__] Restoring original mapping.
[DeepseekV3ForCausalLM::__init__] Restoring original mapping.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [I] Use 13.49 GB for model weights.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [I] Use 13.49 GB for model weights.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [I] Use 13.49 GB for model weights.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 5] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors to memory...
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 4] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors to memory...
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [I] Prefetching 53.30GB checkpoint files.
[12/08/2025-04:36:31] [TRT-LLM] [RANK 3] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors to memory...
[12/08/2025-04:37:22] [TRT-LLM] [RANK 2] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors.
[12/08/2025-04:41:47] [TRT-LLM] [RANK 0] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors.
[12/08/2025-04:44:52] [TRT-LLM] [RANK 1] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors.
[12/08/2025-04:44:52] [TRT-LLM] [RANK 0] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors.
[12/08/2025-04:44:53] [TRT-LLM] [RANK 3] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors.
[12/08/2025-04:44:53] [TRT-LLM] [RANK 5] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors.
[12/08/2025-04:44:53] [TRT-LLM] [RANK 2] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors.
[12/08/2025-04:44:53] [TRT-LLM] [RANK 4] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors.
[12/08/2025-04:45:05] [TRT-LLM] [RANK 1] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors.
Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s]Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s]Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s]Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s][12/08/2025-04:45:05] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s][12/08/2025-04:45:05] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s][12/08/2025-04:45:05] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/08/2025-04:45:05] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00, 15.96it/s]Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00, 13.06it/s]Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00, 11.99it/s]Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00, 10.34it/s]Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00, 10.25it/s]Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00,  9.95it/s]Loading safetensors weights in parallel:  44%|████▍     | 4/9 [00:00<00:00, 15.98it/s]Loading safetensors weights in parallel:  56%|█████▌    | 5/9 [00:00<00:00, 19.41it/s]Loading safetensors weights in parallel:  56%|█████▌    | 5/9 [00:00<00:00, 19.57it/s]Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 32.54it/s]
Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 32.51it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s]Loading weights:   0%|          | 0/813 [00:00<?, ?it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 31.02it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s][Linear::load_weights] self.weight_name: None
Loading safetensors weights in parallel:  44%|████▍     | 4/9 [00:00<00:00, 14.32it/s]Loading safetensors weights in parallel:  44%|████▍     | 4/9 [00:00<00:00, 13.90it/s]Loading safetensors weights in parallel:  44%|████▍     | 4/9 [00:00<00:00, 13.11it/s]Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 27.90it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s][Linear::load_weights] self.weight_name: None
Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 27.77it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s][Linear::load_weights] self.weight_name: None
Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 27.45it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s][Linear::load_weights] self.weight_name: None
Loading weights:   0%|          | 3/813 [00:00<00:28, 28.82it/s][Linear::load_weights] self.weight_name: None
Loading weights:   1%|          | 8/813 [00:00<00:10, 79.61it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   0%|          | 3/813 [00:00<00:40, 19.79it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   0%|          | 3/813 [00:00<00:35, 22.56it/s]Loading weights:   0%|          | 3/813 [00:00<00:33, 23.86it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   0%|          | 3/813 [00:00<00:37, 21.41it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   5%|▌         | 43/813 [00:00<00:04, 174.62it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   5%|▌         | 43/813 [00:00<00:05, 132.40it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   5%|▌         | 43/813 [00:00<00:05, 146.94it/s][Linear::load_weights] self.weight_name: None
Loading weights:   5%|▌         | 43/813 [00:00<00:06, 125.92it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|▊         | 70/813 [00:00<00:03, 193.19it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   5%|▌         | 43/813 [00:00<00:05, 130.17it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   5%|▌         | 43/813 [00:00<00:07, 106.43it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|▊         | 70/813 [00:00<00:04, 162.56it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|▊         | 70/813 [00:00<00:05, 135.57it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|▊         | 70/813 [00:00<00:05, 137.79it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  12%|█▏        | 97/813 [00:00<00:03, 179.09it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|▊         | 70/813 [00:00<00:05, 140.96it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  12%|█▏        | 97/813 [00:00<00:04, 151.34it/s]Loading weights:  12%|█▏        | 97/813 [00:00<00:04, 155.44it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:   9%|▊         | 70/813 [00:00<00:06, 107.79it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  15%|█▌        | 124/813 [00:00<00:03, 172.48it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  12%|█▏        | 97/813 [00:00<00:04, 153.69it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  12%|█▏        | 97/813 [00:00<00:05, 121.47it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  15%|█▌        | 124/813 [00:00<00:04, 161.68it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  15%|█▌        | 124/813 [00:00<00:04, 163.00it/s]Loading weights:  15%|█▌        | 124/813 [00:00<00:04, 152.99it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|█▊        | 151/813 [00:00<00:03, 168.80it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  12%|█▏        | 97/813 [00:00<00:06, 115.66it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|█▊        | 151/813 [00:01<00:03, 168.73it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|█▊        | 151/813 [00:00<00:03, 169.59it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  15%|█▌        | 124/813 [00:01<00:05, 116.79it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  22%|██▏       | 178/813 [00:01<00:03, 167.01it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|█▊        | 151/813 [00:01<00:04, 152.40it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  15%|█▌        | 124/813 [00:01<00:05, 122.73it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  22%|██▏       | 178/813 [00:01<00:03, 173.65it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  22%|██▏       | 178/813 [00:01<00:03, 171.32it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|██▌       | 205/813 [00:01<00:03, 165.46it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  22%|██▏       | 178/813 [00:01<00:04, 152.20it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|█▊        | 151/813 [00:01<00:05, 114.75it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|██▌       | 205/813 [00:01<00:03, 176.56it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  19%|█▊        | 151/813 [00:01<00:05, 128.32it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|██▌       | 205/813 [00:01<00:03, 174.43it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  29%|██▊       | 232/813 [00:01<00:03, 164.57it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|██▌       | 205/813 [00:01<00:04, 151.23it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  29%|██▊       | 232/813 [00:01<00:03, 178.66it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  29%|██▊       | 232/813 [00:01<00:03, 176.34it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  22%|██▏       | 178/813 [00:01<00:04, 131.94it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  22%|██▏       | 178/813 [00:01<00:05, 113.50it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|███▏      | 259/813 [00:01<00:03, 163.63it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|███▏      | 259/813 [00:01<00:03, 180.36it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  29%|██▊       | 232/813 [00:01<00:03, 150.87it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|███▏      | 259/813 [00:01<00:03, 178.33it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|██▌       | 205/813 [00:01<00:04, 133.76it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|███▌      | 286/813 [00:01<00:03, 163.33it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|███▌      | 286/813 [00:01<00:02, 180.09it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  25%|██▌       | 205/813 [00:01<00:05, 112.25it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|███▏      | 259/813 [00:01<00:03, 150.99it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|███▌      | 286/813 [00:01<00:02, 178.40it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|███▊      | 313/813 [00:01<00:02, 179.77it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|███▊      | 313/813 [00:01<00:03, 163.27it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  29%|██▊       | 232/813 [00:01<00:04, 135.25it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|███▊      | 313/813 [00:01<00:02, 178.71it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|███▌      | 286/813 [00:01<00:03, 148.59it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  29%|██▊       | 232/813 [00:02<00:05, 111.62it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  42%|████▏     | 340/813 [00:02<00:02, 179.92it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  42%|████▏     | 340/813 [00:02<00:02, 163.76it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  42%|████▏     | 340/813 [00:02<00:02, 179.77it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|███▏      | 259/813 [00:02<00:04, 136.63it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|███▊      | 313/813 [00:02<00:03, 147.13it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  45%|████▌     | 367/813 [00:02<00:02, 180.20it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  45%|████▌     | 367/813 [00:02<00:02, 164.09it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  45%|████▌     | 367/813 [00:02<00:02, 180.61it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  32%|███▏      | 259/813 [00:02<00:04, 111.48it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|███▌      | 286/813 [00:02<00:03, 135.32it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  48%|████▊     | 394/813 [00:02<00:02, 180.44it/s][Linear::load_weights] self.weight_name: None
Loading weights:  42%|████▏     | 340/813 [00:02<00:03, 147.97it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  48%|████▊     | 394/813 [00:02<00:02, 181.43it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  48%|████▊     | 394/813 [00:02<00:02, 164.58it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|███▊      | 313/813 [00:02<00:03, 134.39it/s]Loading weights:  52%|█████▏    | 421/813 [00:02<00:02, 177.56it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  35%|███▌      | 286/813 [00:02<00:04, 110.26it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  45%|████▌     | 367/813 [00:02<00:02, 149.21it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  52%|█████▏    | 421/813 [00:02<00:02, 169.16it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  52%|█████▏    | 421/813 [00:02<00:02, 170.82it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|█████▌    | 448/813 [00:02<00:01, 188.85it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|█████▌    | 448/813 [00:02<00:02, 164.02it/s][Linear::load_weights] self.weight_name: None
Loading weights:  48%|████▊     | 394/813 [00:02<00:02, 149.33it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  42%|████▏     | 340/813 [00:02<00:03, 135.79it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|█████▌    | 448/813 [00:02<00:02, 165.03it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  58%|█████▊    | 475/813 [00:02<00:01, 195.22it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  38%|███▊      | 313/813 [00:02<00:04, 109.73it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  52%|█████▏    | 421/813 [00:02<00:02, 156.08it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|██████▏   | 502/813 [00:02<00:01, 200.64it/s]Loading weights:  58%|█████▊    | 475/813 [00:02<00:02, 157.32it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  45%|████▌     | 367/813 [00:02<00:03, 137.31it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  58%|█████▊    | 475/813 [00:02<00:02, 157.93it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|█████▌    | 448/813 [00:02<00:02, 163.28it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  42%|████▏     | 340/813 [00:03<00:04, 110.01it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  65%|██████▌   | 529/813 [00:02<00:01, 203.80it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|██████▏   | 502/813 [00:03<00:02, 155.24it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  48%|████▊     | 394/813 [00:03<00:03, 138.32it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|██████▏   | 502/813 [00:03<00:02, 155.33it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|██████▊   | 556/813 [00:03<00:01, 207.29it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  58%|█████▊    | 475/813 [00:03<00:02, 167.51it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  65%|██████▌   | 529/813 [00:03<00:01, 153.16it/s][Linear::load_weights] self.weight_name: None
Loading weights:  45%|████▌     | 367/813 [00:03<00:04, 110.66it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  72%|███████▏  | 583/813 [00:03<00:01, 208.05it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  65%|██████▌   | 529/813 [00:03<00:01, 153.17it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|██████▏   | 502/813 [00:03<00:01, 171.93it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  52%|█████▏    | 421/813 [00:03<00:03, 127.27it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|███████▌  | 610/813 [00:03<00:00, 210.80it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|██████▊   | 556/813 [00:03<00:01, 152.70it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  65%|██████▌   | 529/813 [00:03<00:01, 174.88it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|██████▊   | 556/813 [00:03<00:01, 152.57it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  48%|████▊     | 394/813 [00:03<00:03, 111.24it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  78%|███████▊  | 637/813 [00:03<00:00, 204.83it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|█████▌    | 448/813 [00:03<00:02, 121.86it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|██████▊   | 556/813 [00:03<00:01, 177.70it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  72%|███████▏  | 583/813 [00:03<00:01, 149.94it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  72%|███████▏  | 583/813 [00:03<00:01, 149.24it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  52%|█████▏    | 421/813 [00:03<00:03, 118.75it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  82%|████████▏ | 664/813 [00:03<00:00, 191.00it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  72%|███████▏  | 583/813 [00:03<00:01, 178.17it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|███████▌  | 610/813 [00:03<00:01, 150.07it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|███████▌  | 610/813 [00:03<00:01, 149.65it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  58%|█████▊    | 475/813 [00:03<00:02, 115.85it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  85%|████████▍ | 691/813 [00:03<00:00, 182.18it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  55%|█████▌    | 448/813 [00:03<00:02, 124.89it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|███████▌  | 610/813 [00:03<00:01, 179.40it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  78%|███████▊  | 637/813 [00:03<00:01, 152.59it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  78%|███████▊  | 637/813 [00:03<00:01, 152.45it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  88%|████████▊ | 718/813 [00:04<00:00, 176.60it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  78%|███████▊  | 637/813 [00:04<00:01, 175.01it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  58%|█████▊    | 475/813 [00:04<00:02, 128.73it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|██████▏   | 502/813 [00:04<00:02, 113.50it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  82%|████████▏ | 664/813 [00:04<00:00, 160.94it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  82%|████████▏ | 664/813 [00:04<00:00, 160.33it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  92%|█████████▏| 745/813 [00:04<00:00, 172.22it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  82%|████████▏ | 664/813 [00:04<00:00, 166.76it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  62%|██████▏   | 502/813 [00:04<00:02, 133.42it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  85%|████████▍ | 691/813 [00:04<00:00, 167.39it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  85%|████████▍ | 691/813 [00:04<00:00, 166.56it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  65%|██████▌   | 529/813 [00:04<00:02, 111.89it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  95%|█████████▍| 772/813 [00:04<00:00, 169.62it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  88%|████████▊ | 718/813 [00:04<00:00, 172.05it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  85%|████████▍ | 691/813 [00:04<00:00, 161.63it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  65%|██████▌   | 529/813 [00:04<00:02, 136.28it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  88%|████████▊ | 718/813 [00:04<00:00, 171.49it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  98%|█████████▊| 799/813 [00:04<00:00, 167.42it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  92%|█████████▏| 745/813 [00:04<00:00, 174.93it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|██████████| 813/813 [00:04<00:00, 178.89it/s]
Model init total -- 519.10s
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|██████▊   | 556/813 [00:04<00:02, 111.42it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  88%|████████▊ | 718/813 [00:04<00:00, 158.87it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  92%|█████████▏| 745/813 [00:04<00:00, 174.17it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  68%|██████▊   | 556/813 [00:04<00:01, 138.69it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  95%|█████████▍| 772/813 [00:04<00:00, 176.35it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  95%|█████████▍| 772/813 [00:04<00:00, 176.88it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  92%|█████████▏| 745/813 [00:04<00:00, 155.48it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  72%|███████▏  | 583/813 [00:04<00:01, 138.61it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  72%|███████▏  | 583/813 [00:04<00:02, 109.24it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  98%|█████████▊| 799/813 [00:04<00:00, 176.95it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|██████████| 813/813 [00:04<00:00, 166.22it/s]
Loading weights:  98%|█████████▊| 799/813 [00:04<00:00, 178.78it/s][Linear::load_weights] self.weight_name: None
Model init total -- 519.28s
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|██████████| 813/813 [00:04<00:00, 166.59it/s]
Model init total -- 519.41s
Loading weights:  95%|█████████▍| 772/813 [00:04<00:00, 154.04it/s][12/08/2025-04:45:10] [TRT-LLM] [RANK 5] [I] max_seq_len is not specified, using inferred value 4096
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[12/08/2025-04:45:10] [TRT-LLM] [RANK 5] [I] Using Sampler: TorchSampler
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|███████▌  | 610/813 [00:05<00:01, 140.43it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  75%|███████▌  | 610/813 [00:05<00:01, 109.08it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  98%|█████████▊| 799/813 [00:05<00:00, 153.76it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|██████████| 813/813 [00:05<00:00, 158.63it/s]
Model init total -- 519.61s
Loading weights:  78%|███████▊  | 637/813 [00:05<00:01, 136.42it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  78%|███████▊  | 637/813 [00:05<00:01, 111.09it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[12/08/2025-04:45:10] [TRT-LLM] [RANK 2] [I] max_seq_len is not specified, using inferred value 4096
[12/08/2025-04:45:10] [TRT-LLM] [RANK 2] [I] Using Sampler: TorchSampler
[12/08/2025-04:45:10] [TRT-LLM] [RANK 3] [I] max_seq_len is not specified, using inferred value 4096
[12/08/2025-04:45:10] [TRT-LLM] [RANK 3] [I] Using Sampler: TorchSampler
Loading weights:  82%|████████▏ | 664/813 [00:05<00:01, 127.84it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  82%|████████▏ | 664/813 [00:05<00:01, 118.45it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  85%|████████▍ | 691/813 [00:05<00:00, 122.80it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  85%|████████▍ | 691/813 [00:05<00:00, 124.27it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[12/08/2025-04:45:11] [TRT-LLM] [RANK 4] [I] max_seq_len is not specified, using inferred value 4096
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[12/08/2025-04:45:11] [TRT-LLM] [RANK 4] [I] Using Sampler: TorchSampler
[12/08/2025-04:45:11] [TRT-LLM] [RANK 5] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/08/2025-04:45:11] [TRT-LLM] [RANK 2] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/08/2025-04:45:11] [TRT-LLM] [RANK 3] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/08/2025-04:45:11] [TRT-LLM] [RANK 4] [I] Adjusted attention window size to 4096 in blocks_per_window
[Linear::load_weights] self.weight_name: None
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60967, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60967, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60967, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.79 GiB for max tokens in paged KV cache (1950944).
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.79 GiB for max tokens in paged KV cache (1950944).
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.79 GiB for max tokens in paged KV cache (1950944).
[12/08/2025-04:45:11] [TRT-LLM] [RANK 5] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-04:45:11] [TRT-LLM] [RANK 2] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-04:45:11] [TRT-LLM] [RANK 3] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60967, secondary blocks=0, max sequence length=4096
[12/08/2025-04:45:11] [TRT-LLM] [RANK 5] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[12/08/2025-04:45:11] [TRT-LLM] [RANK 3] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[12/08/2025-04:45:11] [TRT-LLM] [RANK 2] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.79 GiB for max tokens in paged KV cache (1950944).
[12/08/2025-04:45:11] [TRT-LLM] [RANK 4] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-04:45:11] [TRT-LLM] [RANK 4] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
Loading weights:  88%|████████▊ | 718/813 [00:05<00:00, 127.89it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  88%|████████▊ | 718/813 [00:05<00:00, 118.85it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[TensorRT-LLM][INFO][2] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][3] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][2] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:38605
[TensorRT-LLM][INFO][2] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 38605
[TensorRT-LLM][INFO][3] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:34771
[TensorRT-LLM][INFO][3] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 34771
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:43541
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:37705
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 37705
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 43541
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[12/08/2025-04:45:11] [TRT-LLM] [RANK 5] [I] Running autotuner warmup...
[12/08/2025-04:45:11] [TRT-LLM] [RANK 2] [I] Running autotuner warmup...
[12/08/2025-04:45:11] [TRT-LLM] [RANK 4] [I] Running autotuner warmup...
[12/08/2025-04:45:11] [TRT-LLM] [RANK 3] [I] Running autotuner warmup...
[12/08/2025-04:45:11] [TRT-LLM] [RANK 5] [I] [Autotuner] Autotuning process starts ...
[12/08/2025-04:45:11] [TRT-LLM] [RANK 2] [I] [Autotuner] Autotuning process starts ...
[12/08/2025-04:45:11] [TRT-LLM] [RANK 3] [I] [Autotuner] Autotuning process starts ...
[12/08/2025-04:45:11] [TRT-LLM] [RANK 4] [I] [Autotuner] Autotuning process starts ...
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: []
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: []
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: []
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: []
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2, block_ids: [256]
Loading weights:  92%|█████████▏| 745/813 [00:06<00:00, 129.62it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  92%|█████████▏| 745/813 [00:06<00:00, 115.55it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  95%|█████████▍| 772/813 [00:06<00:00, 131.40it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  95%|█████████▍| 772/813 [00:06<00:00, 114.01it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights:  98%|█████████▊| 799/813 [00:06<00:00, 133.23it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|██████████| 813/813 [00:06<00:00, 124.77it/s]
Model init total -- 521.24s
Loading weights:  98%|█████████▊| 799/813 [00:06<00:00, 113.15it/s][Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
[Linear::load_weights] self.weight_name: None
Loading weights: 100%|██████████| 813/813 [00:06<00:00, 120.32it/s]
Model init total -- 521.49s
[12/08/2025-04:45:12] [TRT-LLM] [RANK 1] [I] max_seq_len is not specified, using inferred value 4096
[12/08/2025-04:45:12] [TRT-LLM] [RANK 1] [I] Using Sampler: TorchSampler
[12/08/2025-04:45:12] [TRT-LLM] [RANK 1] [I] [kv cache manager] Primary/secondary blocks for window sizes set to {4096: (257, 0)} for estimation dry run
[12/08/2025-04:45:12] [TRT-LLM] [RANK 1] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=257, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.26 GiB for max tokens in paged KV cache (8224).
[12/08/2025-04:45:12] [TRT-LLM] [RANK 1] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-04:45:12] [TRT-LLM] [RANK 1] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[12/08/2025-04:45:12] [TRT-LLM] [RANK 0] [I] max_seq_len is not specified, using inferred value 4096
[12/08/2025-04:45:12] [TRT-LLM] [RANK 0] [I] Using Sampler: TorchSampler
[12/08/2025-04:45:12] [TRT-LLM] [RANK 0] [I] [kv cache manager] Primary/secondary blocks for window sizes set to {4096: (257, 0)} for estimation dry run
[12/08/2025-04:45:12] [TRT-LLM] [RANK 0] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=257, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.26 GiB for max tokens in paged KV cache (8224).
[12/08/2025-04:45:12] [TRT-LLM] [RANK 0] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-04:45:12] [TRT-LLM] [RANK 0] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:43981
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:33099
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 33099
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 43981
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[12/08/2025-04:45:12] [TRT-LLM] [RANK 1] [I] Running autotuner warmup...
[12/08/2025-04:45:12] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process starts ...
[12/08/2025-04:45:12] [TRT-LLM] [RANK 0] [I] Running autotuner warmup...
[12/08/2025-04:45:12] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process starts ...
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2, block_ids: [256]
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][INFO] Detecting local TP group for rank 0
[TensorRT-LLM][INFO] Detecting local TP group for rank 1
[TensorRT-LLM][INFO] TP group is intra-node for rank 0
[TensorRT-LLM][INFO] TP group is intra-node for rank 1
[TensorRT-LLM][INFO] Detecting local TP group for rank 3
[TensorRT-LLM][INFO] Detecting local TP group for rank 2
[TensorRT-LLM][INFO] Detecting local TP group for rank 0
[TensorRT-LLM][INFO] Detecting local TP group for rank 1
[TensorRT-LLM][INFO] TP group is intra-node for rank 3
[TensorRT-LLM][INFO] TP group is intra-node for rank 1
[TensorRT-LLM][INFO] TP group is intra-node for rank 2
[TensorRT-LLM][INFO] TP group is intra-node for rank 0
[12/08/2025-04:45:30] [TRT-LLM] [RANK 2] [I] [Autotuner] Autotuning process ends
[12/08/2025-04:45:30] [TRT-LLM] [RANK 5] [I] [Autotuner] Autotuning process ends
[12/08/2025-04:45:30] [TRT-LLM] [RANK 2] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-04:45:30] [TRT-LLM] [RANK 3] [I] [Autotuner] Autotuning process ends
[12/08/2025-04:45:30] [TRT-LLM] [RANK 5] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-04:45:30] [TRT-LLM] [RANK 3] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-04:45:30] [TRT-LLM] [RANK 4] [I] [Autotuner] Autotuning process ends
[12/08/2025-04:45:30] [TRT-LLM] [RANK 4] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-04:45:30] [TRT-LLM] [RANK 2] [I] global_steady_clock_offset at each rank: [0.0, 1.00000761449337e-06, -3.999972250312567e-06, -1.9999861251562834e-06]
[12/08/2025-04:45:30] [TRT-LLM] [RANK 3] [I] Setting global_steady_clock_offset: 1.00000761449337e-06 seconds for rank 1
[12/08/2025-04:45:30] [TRT-LLM] [RANK 4] [I] Setting global_steady_clock_offset: -3.999972250312567e-06 seconds for rank 2
[12/08/2025-04:45:30] [TRT-LLM] [RANK 5] [I] Setting global_steady_clock_offset: -1.9999861251562834e-06 seconds for rank 3
[12/08/2025-04:45:30] [TRT-LLM] [RANK 2] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/08/2025-04:45:30] [TRT-LLM] [RANK 3] [I] Setting PyTorch memory fraction to 0.7459883738826193 (199.69281005859375 GiB)
[12/08/2025-04:45:30] [TRT-LLM] [RANK 5] [I] Setting PyTorch memory fraction to 0.7465720738334837 (199.84906005859375 GiB)
[12/08/2025-04:45:30] [TRT-LLM] [RANK 4] [I] Setting PyTorch memory fraction to 0.7459883738826193 (199.69281005859375 GiB)
[12/08/2025-04:45:30] [TRT-LLM] [RANK 2] [I] Setting PyTorch memory fraction to 0.7461051138727922 (199.72406005859375 GiB)
[12/08/2025-04:45:30] [TRT-LLM] [RANK 3] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=2 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-04:45:30] [TRT-LLM] [RANK 5] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=2 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-04:45:30] [TRT-LLM] [RANK 2] [I] LLM Args:
model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=2 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-04:45:30] [TRT-LLM] [RANK 4] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=2 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-04:45:30] [TRT-LLM] [RANK 2] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=2 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-04:45:30] [TRT-LLM] [I] get signal from executor worker
[12/08/2025-04:45:30] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process ends
[12/08/2025-04:45:30] [TRT-LLM] [RANK 0] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-04:45:30] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process ends
[12/08/2025-04:45:30] [TRT-LLM] [RANK 1] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-04:45:30] [TRT-LLM] [RANK 0] [I] global_steady_clock_offset at each rank: [0.0, -5.0000089686363935e-06]
[12/08/2025-04:45:30] [TRT-LLM] [RANK 0] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/08/2025-04:45:30] [TRT-LLM] [RANK 1] [I] Setting global_steady_clock_offset: -5.0000089686363935e-06 seconds for rank 1
[12/08/2025-04:45:30] [TRT-LLM] [RANK 0] [I] Memory used after loading model weights (inside torch) in memory usage profiling: 26.30 GiB
[12/08/2025-04:45:30] [TRT-LLM] [RANK 0] [I] Memory used after loading model weights (outside torch) in memory usage profiling: 5.77 GiB
[12/08/2025-04:45:30] [TRT-LLM] [RANK 1] [I] Memory used after loading model weights (inside torch) in memory usage profiling: 26.30 GiB
[12/08/2025-04:45:30] [TRT-LLM] [RANK 1] [I] Memory used after loading model weights (outside torch) in memory usage profiling: 4.41 GiB
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([126340,  42198,  61849,  ...,  39932,  50589, 119390], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([126340,  42198,  61849,  ...,  39932,  50589, 119390], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2049, block_ids: [255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2050, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2049, block_ids: [255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2050, block_ids: [256]
[32mINFO[0m:     Started server process [[36m3464[0m]
[32mINFO[0m:     Waiting for application startup.
[32mINFO[0m:     Application startup complete.
[32mINFO[0m:     Uvicorn running on [1mhttp://localhost:8002[0m (Press CTRL+C to quit)
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2048
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2049
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2050
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2048
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2049
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2050
[12/08/2025-04:45:31] [TRT-LLM] [RANK 0] [I] Memory dynamically allocated during inference (inside torch) in memory usage profiling: 0.41 GiB
[12/08/2025-04:45:31] [TRT-LLM] [RANK 0] [I] Memory used outside torch (e.g., NCCL and CUDA graphs) in memory usage profiling: 5.89 GiB
[12/08/2025-04:45:31] [TRT-LLM] [RANK 1] [I] Memory dynamically allocated during inference (inside torch) in memory usage profiling: 0.41 GiB
[12/08/2025-04:45:31] [TRT-LLM] [RANK 1] [I] Memory used outside torch (e.g., NCCL and CUDA graphs) in memory usage profiling: 4.53 GiB
[12/08/2025-04:45:31] [TRT-LLM] [RANK 0] [I] Peak memory during memory usage profiling (torch + non-torch): 32.60 GiB, available KV cache memory when calculating max tokens: 58.84 GiB, fraction is set 0.25, kv size is 34560. device total memory 267.69 GiB, , tmp kv_mem 0.26 GiB
[12/08/2025-04:45:31] [TRT-LLM] [RANK 0] [I] Estimated max memory in KV cache : 58.84 GiB
[12/08/2025-04:45:31] [TRT-LLM] [RANK 1] [I] Peak memory during memory usage profiling (torch + non-torch): 31.24 GiB, available KV cache memory when calculating max tokens: 59.18 GiB, fraction is set 0.25, kv size is 34560. device total memory 267.69 GiB, , tmp kv_mem 0.26 GiB
[12/08/2025-04:45:31] [TRT-LLM] [RANK 0] [W] Both free_gpu_memory_fraction and max_tokens are set (to 0.25 and 1828062 with free memory 235.81573486328125GiB of total memory 267.68890380859375GiB, respectively). The smaller value will be used.
[12/08/2025-04:45:31] [TRT-LLM] [RANK 1] [I] Estimated max memory in KV cache : 59.18 GiB
[12/08/2025-04:45:31] [TRT-LLM] [RANK 1] [W] Both free_gpu_memory_fraction and max_tokens are set (to 0.25 and 1838622 with free memory 237.17523193359375GiB of total memory 267.68890380859375GiB, respectively). The smaller value will be used.
[12/08/2025-04:45:31] [TRT-LLM] [RANK 0] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/08/2025-04:45:31] [TRT-LLM] [RANK 1] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=57126, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 58.84 GiB for max tokens in paged KV cache (1828032).
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=57126, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 58.84 GiB for max tokens in paged KV cache (1828032).
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[12/08/2025-04:45:32] [TRT-LLM] [RANK 1] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/08/2025-04:45:32] [TRT-LLM] [RANK 1] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:44473
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 44473
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager localIp: 10.176.181.12
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.176.181.12:36595
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager ip: 10.176.181.12, port: 36595
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] Running autotuner warmup...
[12/08/2025-04:45:32] [TRT-LLM] [RANK 1] [I] Running autotuner warmup...
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process starts ...
[12/08/2025-04:45:32] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process starts ...
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2, block_ids: [256]
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process ends
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-04:45:32] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process ends
[12/08/2025-04:45:32] [TRT-LLM] [RANK 1] [I] [Autotuner] Cache size after warmup is 28
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] global_steady_clock_offset at each rank: [0.0, -2.9999937396496534e-06]
[12/08/2025-04:45:32] [TRT-LLM] [RANK 1] [I] Setting global_steady_clock_offset: -2.9999937396496534e-06 seconds for rank 1
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] Setting PyTorch memory fraction to 0.7613611153229217 (203.80792236328125 GiB)
[12/08/2025-04:45:32] [TRT-LLM] [RANK 1] [I] Setting PyTorch memory fraction to 0.766439760911028 (205.16741943359375 GiB)
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] LLM Args:
model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=1828062, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=63177848832, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-04:45:32] [TRT-LLM] [RANK 0] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=1828062, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=63177848832, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-04:45:32] [TRT-LLM] [RANK 1] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=1838622, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=63542786048, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/08/2025-04:45:32] [TRT-LLM] [I] get signal from executor worker
[32mINFO[0m:     127.0.0.1:36408 - "[1mGET /health HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     Started server process [[36m3465[0m]
[32mINFO[0m:     Waiting for application startup.
[32mINFO[0m:     Application startup complete.
[32mINFO[0m:     Uvicorn running on [1mhttp://localhost:8001[0m (Press CTRL+C to quit)
[32mINFO[0m:     127.0.0.1:50888 - "[1mGET /health HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     127.0.0.1:36408 - "[1mGET /health HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     127.0.0.1:36424 - "[1mGET /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[12/08/2025-04:45:35] [TRT-LLM] [I] The steady clock offset between local and disagg server: -0.00012700000661425292 second
[32mINFO[0m:     127.0.0.1:36428 - "[1mPOST /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     127.0.0.1:50900 - "[1mGET /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[12/08/2025-04:45:35] [TRT-LLM] [I] The steady clock offset between local and disagg server: 0.0007475000020349398 second
[32mINFO[0m:     127.0.0.1:50912 - "[1mPOST /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([28803, 27194,   344,   270,  1606,  2112,  9059,   295,  6677,  5169,
         7677,   513, 30603, 26251,   538,  2883,  4577,    14, 21779, 26498,
        32329,    14,   305, 80124,    16,   983, 10401,   304, 35317, 10639,
           14, 15852, 41366,    14,   305, 15075,  9670,   396, 52498, 29810,
           14, 25676,    14,   305,  1482,    16, 12795, 12327,  5217,  4271,
          344,   223], device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([28803, 27194,   344,   270,  1606,  2112,  9059,   295,  6677,  5169,
         7677,   513, 30603, 26251,   538,  2883,  4577,    14, 21779, 26498,
        32329,    14,   305, 80124,    16,   983, 10401,   304, 35317, 10639,
           14, 15852, 41366,    14,   305, 15075,  9670,   396, 52498, 29810,
           14, 25676,    14,   305,  1482,    16, 12795, 12327,  5217,  4271,
          344,   223], device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]],
       device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]],
       device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  52, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  52, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[32mINFO[0m:     127.0.0.1:50888 - "[1mPOST /v1/completions HTTP/1.1[0m" [32m200 OK[0m
[ExecutorRequestQueue::_merge_helix_requests][rank 0][cp_rank 0]: input_ids_this_rank: [28803, 27194, 344, 270, 1606, 2112, 9059, 295, 6677, 5169, 7677, 513, 30603, 26251, 538, 2883, 4577, 14, 21779, 26498, 32329, 14, 305, 80124, 16, 983, 10401, 304, 35317, 10639, 14, 15852]
[ExecutorRequestQueue::_merge_helix_requests][rank 0][cp_rank 0]: position_ids_this_rank: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[ExecutorRequestQueue::_merge_helix_requests][rank 1][cp_rank 0]: input_ids_this_rank: [28803, 27194, 344, 270, 1606, 2112, 9059, 295, 6677, 5169, 7677, 513, 30603, 26251, 538, 2883, 4577, 14, 21779, 26498, 32329, 14, 305, 80124, 16, 983, 10401, 304, 35317, 10639, 14, 15852]
[ExecutorRequestQueue::_merge_helix_requests][rank 1][cp_rank 0]: position_ids_this_rank: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[ExecutorRequestQueue::_merge_helix_requests][rank 3][cp_rank 1]: input_ids_this_rank: [41366, 14, 305, 15075, 9670, 396, 52498, 29810, 14, 25676, 14, 305, 1482, 16, 12795, 12327, 5217, 4271, 344, 223]
[ExecutorRequestQueue::_merge_helix_requests][rank 3][cp_rank 1]: position_ids_this_rank: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
[ExecutorRequestQueue::_merge_helix_requests][rank 2][cp_rank 1]: input_ids_this_rank: [41366, 14, 305, 15075, 9670, 396, 52498, 29810, 14, 25676, 14, 305, 1482, 16, 12795, 12327, 5217, 4271, 344, 223]
[ExecutorRequestQueue::_merge_helix_requests][rank 2][cp_rank 1]: position_ids_this_rank: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 1][cp_rank 0]: TRANSMISSION COMPLETE for request ID: 2048
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 0][cp_rank 0]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([56840], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[52]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([56840], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[52]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 2][cp_rank 1]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([56840], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[52]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [20]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  21, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 3][cp_rank 1]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([56840], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[52]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [20]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  21, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:3', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:2', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:5', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank1_cp0_tp1_before_input_layernorm.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank3_cp0_tp3_before_input_layernorm.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank0_cp0_tp0_before_input_layernorm.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank2_cp0_tp2_before_input_layernorm.pt
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:3', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:5', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:2', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank1_cp0_tp1_before_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank3_cp0_tp3_before_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank0_cp0_tp0_before_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank2_cp0_tp2_before_attn.pt
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 184902656 bytes to 380108800 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 184902656 bytes to 380108800 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 184902656 bytes to 380108800 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 184902656 bytes to 380108800 bytes
[MLA::forward][rank 3][cp_rank 1][tp_rank 1]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[-0.0018, -0.0044, -0.0079,  ..., -0.0009, -0.0009,  0.0006]],
       device='cuda:5', dtype=torch.bfloat16) 
 weight.shape: torch.Size([2560, 1024]) weight.tp_rank: 3 weight.tp_size: 4 
 Parameter containing:
tensor([[-0.0082, -0.0330, -0.0181,  ..., -0.0035, -0.0055,  0.0109],
        [ 0.0002,  0.0181,  0.0027,  ...,  0.0085,  0.0008, -0.0009],
        [ 0.0165,  0.0027,  0.0062,  ..., -0.0035,  0.0101,  0.0233],
        ...,
        [ 0.0081,  0.0223,  0.0028,  ...,  0.0140, -0.0157, -0.0052],
        [-0.0112,  0.0071,  0.0152,  ..., -0.0026,  0.0114,  0.0175],
        [ 0.0264,  0.0243, -0.0203,  ..., -0.0070,  0.0044, -0.0192]],
       device='cuda:5', dtype=torch.bfloat16)
[MLA::forward][rank 1][cp_rank 0][tp_rank 1]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[ 4.4861e-03, -2.3651e-03, -5.0068e-05,  ..., -3.4332e-04,
          3.0823e-03, -2.2736e-03]], device='cuda:3', dtype=torch.bfloat16) 
 weight.shape: torch.Size([2560, 1024]) weight.tp_rank: 1 weight.tp_size: 4 
 Parameter containing:
tensor([[-0.0115, -0.0047,  0.0172,  ..., -0.0148, -0.0096, -0.0148],
        [ 0.0115,  0.0229,  0.0129,  ...,  0.0053, -0.0192,  0.0128],
        [ 0.0010, -0.0143, -0.0032,  ..., -0.0020, -0.0034,  0.0048],
        ...,
        [ 0.0065,  0.0177,  0.0022,  ..., -0.0220, -0.0240, -0.0027],
        [-0.0177,  0.0212,  0.0106,  ..., -0.0030,  0.0065, -0.0016],
        [ 0.0094,  0.0153, -0.0059,  ..., -0.0110,  0.0150,  0.0200]],
       device='cuda:3', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank3_cp1_tp1_before_o_proj.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank1_cp0_tp1_before_o_proj.pt
[MLA::forward][rank 0][cp_rank 0][tp_rank 0]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[-0.0023,  0.0042,  0.0032,  ..., -0.0003,  0.0005, -0.0003]],
       device='cuda:2', dtype=torch.bfloat16) 
 weight.shape: torch.Size([2560, 1024]) weight.tp_rank: 0 weight.tp_size: 4 
 Parameter containing:
tensor([[-0.0195, -0.0114,  0.0053,  ...,  0.0031,  0.0087, -0.0047],
        [-0.0090,  0.0244, -0.0041,  ..., -0.0034,  0.0087,  0.0062],
        [-0.0195,  0.0081,  0.0018,  ...,  0.0069, -0.0019, -0.0225],
        ...,
        [ 0.0072,  0.0008, -0.0087,  ..., -0.0074,  0.0130, -0.0112],
        [-0.0096, -0.0193, -0.0043,  ...,  0.0046, -0.0205,  0.0130],
        [ 0.0048, -0.0077, -0.0096,  ..., -0.0021,  0.0046,  0.0130]],
       device='cuda:2', dtype=torch.bfloat16)
[MLA::forward][rank 2][cp_rank 1][tp_rank 0]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[-3.5524e-05, -3.3417e-03,  8.1787e-03,  ...,  1.8311e-03,
          6.0654e-04,  3.9291e-04]], device='cuda:4', dtype=torch.bfloat16) 
 weight.shape: torch.Size([2560, 1024]) weight.tp_rank: 2 weight.tp_size: 4 
 Parameter containing:
tensor([[-0.0175,  0.0079,  0.0192,  ..., -0.0305, -0.0070,  0.0140],
        [-0.0114,  0.0017, -0.0087,  ..., -0.0070, -0.0023,  0.0376],
        [-0.0140, -0.0052,  0.0175,  ..., -0.0211,  0.0082,  0.0023],
        ...,
        [ 0.0077, -0.0034,  0.0308,  ...,  0.0037, -0.0055,  0.0073],
        [ 0.0171,  0.0032,  0.0103,  ..., -0.0134,  0.0009,  0.0195],
        [-0.0094, -0.0256,  0.0085,  ..., -0.0023, -0.0037,  0.0079]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank3_cp1_tp1_before_o_proj_weight.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank1_cp0_tp1_before_o_proj_weight.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank0_cp0_tp0_before_o_proj.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank3_cp1_tp1_before_o_proj_bias.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank2_cp1_tp0_before_o_proj.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank1_cp0_tp1_before_o_proj_bias.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank0_cp0_tp0_before_o_proj_weight.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank2_cp1_tp0_before_o_proj_weight.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank0_cp0_tp0_before_o_proj_bias.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank2_cp1_tp0_before_o_proj_bias.pt
[MLA::forward][rank 3][cp_rank 1][tp_rank 1]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:5', dtype=torch.bfloat16)
[MLA::forward][rank 1][cp_rank 0][tp_rank 1]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:3', dtype=torch.bfloat16)
[MLA::forward][rank 0][cp_rank 0][tp_rank 0]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:2', dtype=torch.bfloat16)
[MLA::forward][rank 2][cp_rank 1][tp_rank 0]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank3_cp1_tp1_after_o_proj.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank1_cp0_tp1_after_o_proj.pt
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:5', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank0_cp0_tp0_after_o_proj.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank2_cp1_tp0_after_o_proj.pt
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:3', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:2', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank3_cp0_tp3_after_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank1_cp0_tp1_after_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank0_cp0_tp0_after_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank2_cp0_tp2_after_attn.pt
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3477, -0.2656,  0.0327,  ..., -0.1367, -0.4453,  0.3652]],
       device='cuda:5', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3477, -0.2656,  0.0327,  ..., -0.1367, -0.4453,  0.3652]],
       device='cuda:2', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3477, -0.2656,  0.0327,  ..., -0.1367, -0.4453,  0.3652]],
       device='cuda:3', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3477, -0.2656,  0.0327,  ..., -0.1367, -0.4453,  0.3652]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank3_cp0_tp3_after_mlp.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank0_cp0_tp0_after_mlp.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank1_cp0_tp1_after_mlp.pt
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1465, -0.1152,  0.0112,  ..., -0.0562, -0.1895,  0.1465]],
       device='cuda:5', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank2_cp0_tp2_after_mlp.pt
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1465, -0.1152,  0.0112,  ..., -0.0562, -0.1895,  0.1465]],
       device='cuda:2', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1465, -0.1152,  0.0112,  ..., -0.0562, -0.1895,  0.1465]],
       device='cuda:3', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1465, -0.1152,  0.0112,  ..., -0.0562, -0.1895,  0.1465]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank3_cp0_tp3_residual.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank0_cp0_tp0_residual.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank1_cp0_tp1_residual.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2_fix/rank2_cp0_tp2_residual.pt
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([8158], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[53]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([8158], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([8158], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[53]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[53]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [21]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([8158], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[53]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [21]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  22, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  22, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([850], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([850], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[54]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[54]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [22]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  23, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([850], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([850], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[54]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [22]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[54]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  23, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([271], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[55]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [23]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([271], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[55]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  24, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([271], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([271], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[55]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [23]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[55]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  24, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([28803], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([28803], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[56]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [24]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[56]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  25, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([28803], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([28803], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[56]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [24]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[56]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  25, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([27194], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([27194], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[57]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [25]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[57]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  26, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([27194], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([27194], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[57]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [25]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[57]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  26, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[58]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [26]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[58]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  27, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[58]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [26]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[58]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  27, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[59]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [27]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[59]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  28, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[59]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [27]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[59]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  28, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4138], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4138], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[60]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [28]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[60]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  29, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4138], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4138], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[60]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [28]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[60]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  29, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([295], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([295], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[61]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [29]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[61]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  30, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([295], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([295], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[61]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [29]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[61]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  30, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[62]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [30]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[62]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  31, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[62]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [30]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[62]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  31, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[63]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [31]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[63]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[63]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [31]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[63]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[64]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[64]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  33, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[64]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[64]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  33, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5004], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5004], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[65]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [33]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[65]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  34, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5004], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5004], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[65]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [33]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[65]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  34, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[66]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [34]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[66]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  35, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[66]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [34]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[66]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  35, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[67]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [35]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[67]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  36, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[67]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [35]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[67]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  36, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[68]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [36]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[68]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  37, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[68]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [36]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[68]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  37, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[69]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [37]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[69]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  38, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[69]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [37]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[69]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  38, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[70]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [38]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[70]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  39, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[70]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [38]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[70]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  39, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[71]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [39]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[71]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  40, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[71]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [39]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[71]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  40, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[72]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [40]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[72]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  41, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[72]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [40]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[72]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  41, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[73]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [41]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[73]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  42, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[73]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [41]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[73]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  42, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[74]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [42]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[74]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  43, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[74]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [42]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[74]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  43, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[75]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [43]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[75]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  44, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[75]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [43]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[75]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  44, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[76]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [44]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[76]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[76]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[76]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [44]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  45, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  45, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[77]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [45]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[77]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[77]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [45]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[77]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  46, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  46, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[78]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [46]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[78]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  47, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[78]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [46]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[78]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  47, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[79]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [47]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[79]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  48, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[79]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [47]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[79]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  48, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[80]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [48]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[80]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  49, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[80]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [48]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[80]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  49, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[81]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [49]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[81]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  50, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[81]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [49]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[81]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  50, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[82]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [50]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[82]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  51, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[82]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [50]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[82]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  51, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[83]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [51]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[83]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  52, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[83]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [51]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[83]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  52, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[84]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [52]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[84]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  53, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[84]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [52]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[84]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  53, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[85]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [53]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[85]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  54, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[85]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [53]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[85]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  54, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[86]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [54]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[86]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  55, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[86]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [54]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[86]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  55, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[87]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [55]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[87]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  56, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[87]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [55]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[87]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  56, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[88]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [56]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[88]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  57, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[88]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [56]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[88]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  57, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[89]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [57]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[89]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  58, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[89]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [57]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[89]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  58, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[90]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [58]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[90]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  59, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[90]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [58]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[90]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  59, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[91]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [59]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[91]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  60, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[91]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [59]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[91]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  60, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[92]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [60]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[92]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  61, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[92]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [60]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[92]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  61, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[93]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [61]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[93]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  62, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[93]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [61]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[93]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  62, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[94]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [62]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[94]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  63, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[94]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [62]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[94]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  63, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[95]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [63]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[95]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  64, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[95]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [63]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[95]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  64, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[96]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [64]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[96]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  65, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[96]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [64]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[96]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  65, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[97]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [65]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[97]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  66, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[97]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [65]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[97]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  66, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[98]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [66]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[98]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  67, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[98]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [66]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[98]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  67, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[99]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [67]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[99]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  68, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[99]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [67]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[99]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  68, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[100]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [68]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[100]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  69, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[100]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [68]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[100]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  69, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[101]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [69]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[101]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  70, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[101]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [69]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[101]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  70, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[102]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [70]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[102]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  71, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[102]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [70]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[102]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  71, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[103]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [71]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[103]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  72, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[103]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [71]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[103]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  72, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[104]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [72]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[104]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  73, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[104]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [72]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[104]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  73, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[105]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [73]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[105]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  74, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[105]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [73]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[105]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  74, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[106]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[106]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [74]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  75, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[106]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [74]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[106]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  75, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[107]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [75]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[107]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  76, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[107]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [75]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[107]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  76, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[108]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [76]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[108]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  77, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[108]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [76]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[108]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  77, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[109]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [77]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[109]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[109]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [77]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  78, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[109]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  78, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[110]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [78]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[110]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  79, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[110]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [78]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[110]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  79, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[111]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [79]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[111]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  80, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[111]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [79]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[111]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  80, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[112]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [80]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[112]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  81, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[112]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [80]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[112]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  81, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[113]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [81]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[113]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  82, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[113]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [81]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[113]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  82, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[114]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [82]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[114]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  83, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[114]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [82]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[114]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  83, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[115]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [83]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[115]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  84, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[115]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [83]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[115]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  84, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[116]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [84]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[116]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  85, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[116]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [84]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[116]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  85, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[117]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [85]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[117]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  86, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[117]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [85]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[117]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  86, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[118]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [86]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[118]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  87, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[118]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [86]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[118]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  87, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[119]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [87]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[119]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  88, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[119]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [87]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[119]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  88, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[120]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [88]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[120]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  89, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[120]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [88]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[120]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  89, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[121]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [89]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[121]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  90, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[121]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [89]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[121]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  90, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[122]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [90]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[122]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  91, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[122]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [90]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[122]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  91, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[123]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [91]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[123]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  92, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[123]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [91]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[123]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  92, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[124]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [92]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[124]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  93, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[124]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [92]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[124]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  93, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[125]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [93]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[125]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  94, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[125]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [93]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[125]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  94, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[126]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [94]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[126]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  95, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[126]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [94]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[126]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  95, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[127]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [95]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[127]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  96, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[127]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [95]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[127]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  96, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[128]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [96]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[128]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  97, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[128]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [96]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[128]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  97, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[129]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [97]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[129]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  98, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[129]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [97]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[129]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  98, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[130]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [98]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[130]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  99, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[130]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [98]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[130]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  99, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[131]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [99]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[131]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 100, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[131]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [99]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[131]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 100, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[132]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [100]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[132]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 101, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[132]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [100]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[132]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 101, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[133]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [101]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[133]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 102, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[133]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [101]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[133]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 102, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[134]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [102]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[134]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 103, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[134]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [102]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[134]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 103, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[135]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [103]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[135]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 104, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[135]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [103]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[135]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 104, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[136]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [104]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[136]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 105, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[136]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [104]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[136]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 105, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[137]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [105]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[137]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 106, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[137]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [105]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[137]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 106, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[138]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [106]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[138]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 107, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[138]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [106]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[138]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 107, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[139]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [107]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[139]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 108, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[139]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [107]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[139]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 108, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[140]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [108]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[140]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 109, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[140]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [108]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[140]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 109, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[141]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [109]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[141]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[141]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [109]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[141]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 110, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 110, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[142]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [110]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[142]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[142]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[142]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [110]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 111, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 111, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[143]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [111]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[143]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [111]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[143]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[143]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 112, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 112, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[144]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [112]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[144]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[144]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [112]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[144]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 113, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 113, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[145]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [113]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[145]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[145]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [113]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[145]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 114, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 114, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[146]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [114]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[146]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[146]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[146]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [114]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 115, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 115, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[147]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [115]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[147]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [115]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[147]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[147]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 116, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 116, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([14], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([14], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([14], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([14], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[148]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [116]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[148]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [116]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[148]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[148]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 117, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 117, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[149]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [117]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[149]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[149]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [117]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[149]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 118, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 118, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([13801], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([13801], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([13801], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([13801], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[150]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [118]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[150]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[150]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [118]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[150]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 119, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 119, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[32mINFO[0m:     127.0.0.1:44140 - "[1mPOST /v1/completions HTTP/1.1[0m" [32m200 OK[0m
