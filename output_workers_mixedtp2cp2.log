/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[1765142609.206149] [umb-b300-dp-186:6543 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142609.712743] [umb-b300-dp-186:6547 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142609.713043] [umb-b300-dp-186:6544 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142609.920799] [umb-b300-dp-186:6546 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142609.927745] [umb-b300-dp-186:6542 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142609.976996] [umb-b300-dp-186:6545 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142610.105680] [umb-b300-dp-186:6545 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142610.105610] [umb-b300-dp-186:6543 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142610.105656] [umb-b300-dp-186:6547 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142610.105732] [umb-b300-dp-186:6546 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142610.105749] [umb-b300-dp-186:6544 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142610.105897] [umb-b300-dp-186:6542 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[12/07/2025-21:23:35] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/07/2025-21:23:35] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/07/2025-21:23:35] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/07/2025-21:23:35] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/07/2025-21:23:35] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/07/2025-21:23:35] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/07/2025-21:23:35] [TRT-LLM] [I] cutlass dsl is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cutlass dsl is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cutlass dsl is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cutlass dsl is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cutlass dsl is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cutlass dsl is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/07/2025-21:23:35] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/07/2025-21:23:36] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/07/2025-21:23:36] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/07/2025-21:23:36] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/07/2025-21:23:36] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/07/2025-21:23:36] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/07/2025-21:23:36] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/07/2025-21:23:36] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/07/2025-21:23:36] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/07/2025-21:23:36] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/07/2025-21:23:36] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/07/2025-21:23:36] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/07/2025-21:23:36] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
env_global_rank: 3, set device_id: 3 before importing mpi4py
env_global_rank: 1, set device_id: 1 before importing mpi4py
env_global_rank: 0, set device_id: 0 before importing mpi4py
env_global_rank: 2, set device_id: 2 before importing mpi4py
env_global_rank: 5, set device_id: 5 before importing mpi4py
env_global_rank: 4, set device_id: 4 before importing mpi4py
[2025-12-07 21:23:39] INFO disagg_utils.py:318: global_rank: 3, instance_idx: 1, sub_rank: 1, is_leader: False
[2025-12-07 21:23:39] INFO disagg_utils.py:318: global_rank: 2, instance_idx: 1, sub_rank: 0, is_leader: True
[2025-12-07 21:23:39] INFO disagg_utils.py:318: global_rank: 5, instance_idx: 1, sub_rank: 3, is_leader: False
[2025-12-07 21:23:39] INFO disagg_utils.py:318: global_rank: 4, instance_idx: 1, sub_rank: 2, is_leader: False
[2025-12-07 21:23:39] INFO disagg_utils.py:318: global_rank: 0, instance_idx: 0, sub_rank: 0, is_leader: True
[2025-12-07 21:23:39] INFO disagg_utils.py:318: global_rank: 1, instance_idx: 0, sub_rank: 1, is_leader: False
[12/07/2025-21:23:39] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:39] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 3, sub-comm MPI rank: 1
[12/07/2025-21:23:39] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:39] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 0, sub-comm MPI rank: 0
[12/07/2025-21:23:39] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:39] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 2, sub-comm MPI rank: 0
[12/07/2025-21:23:39] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:39] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 5, sub-comm MPI rank: 3
[12/07/2025-21:23:39] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:39] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 4, sub-comm MPI rank: 2
[12/07/2025-21:23:39] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:39] [TRT-LLM] [I] mpi_session is provided for LLM instance. Global MPI rank: 1, sub-comm MPI rank: 1
[12/07/2025-21:23:39] [TRT-LLM] [W] Overriding kv_cache_config 
[12/07/2025-21:23:39] [TRT-LLM] [W] Overriding kv_cache_config 
[12/07/2025-21:23:39] [TRT-LLM] [I] rank 0 step1: preparing to launch command: ['python3', '/home/bbuddharaju/.local/bin/trtllm-serve', 'disaggregated_mpi_worker', '-c', '/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tests/integration/defs/disaggregated/test_configs/disagg_config_ctxtp2_gentp1cp2_deepseek_v3_lite_bf16_tllm_gen.yaml', '--log_level', 'info']
[12/07/2025-21:23:39] [TRT-LLM] [I] rank 0 step1: preparing to launch command: ['python3', '/home/bbuddharaju/.local/bin/trtllm-serve', 'disaggregated_mpi_worker', '-c', '/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tests/integration/defs/disaggregated/test_configs/disagg_config_ctxtp2_gentp1cp2_deepseek_v3_lite_bf16_tllm_gen.yaml', '--log_level', 'info']
[12/07/2025-21:23:39] [TRT-LLM] [I] Parent process (PID 6542) launched child process (PID 8099).
[12/07/2025-21:23:39] [TRT-LLM] [I] rank 0 step2: start the mpi session server
[12/07/2025-21:23:39] [TRT-LLM] [I] Parent process (PID 6544) launched child process (PID 8100).
[12/07/2025-21:23:39] [TRT-LLM] [I] rank 0 step2: start the mpi session server
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[1765142628.097880] [umb-b300-dp-186:8099 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142628.100313] [umb-b300-dp-186:8100 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142628.255385] [umb-b300-dp-186:8099 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765142628.256407] [umb-b300-dp-186:8100 :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[12/07/2025-21:23:52] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/07/2025-21:23:52] [TRT-LLM] [I] flashinfer is available: 0.3.1.post1
[12/07/2025-21:23:53] [TRT-LLM] [I] cutlass dsl is available
[12/07/2025-21:23:53] [TRT-LLM] [I] cutlass dsl is available
[12/07/2025-21:23:53] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/07/2025-21:23:53] [TRT-LLM] [I] cuBLASLt FP4 GEMM is available
[12/07/2025-21:23:53] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/07/2025-21:23:53] [TRT-LLM] [I] Starting TensorRT LLM init.
[TensorRT-LLM][INFO] Set logger level to INFO
[12/07/2025-21:23:53] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
[12/07/2025-21:23:53] [TRT-LLM] [I] TensorRT LLM inited.
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc5
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
/home/scratch.bbuddharaju_gpu/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py:104: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
[12/07/2025-21:23:54] [TRT-LLM] [W] Overriding kv_cache_config 
[12/07/2025-21:23:54] [TRT-LLM] [I] rank 0 for index 1 launch the disagg server
[12/07/2025-21:23:54] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:54] [TRT-LLM] [I] Using LLM with PyTorch backend
[12/07/2025-21:23:54] [TRT-LLM] [I] neither checkpoint_format nor checkpoint_loader were provided, checkpoint_format will be set to HF.
[12/07/2025-21:23:54] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:54] [TRT-LLM] [I] start MpiSession with 4 workers
[12/07/2025-21:23:54] [TRT-LLM] [W] Overriding kv_cache_config 
[12/07/2025-21:23:54] [TRT-LLM] [I] rank 0 for index 0 launch the disagg server
[12/07/2025-21:23:54] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:54] [TRT-LLM] [I] Using LLM with PyTorch backend
[12/07/2025-21:23:54] [TRT-LLM] [I] neither checkpoint_format nor checkpoint_loader were provided, checkpoint_format will be set to HF.
[12/07/2025-21:23:54] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:54] [TRT-LLM] [I] start MpiSession with 2 workers
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
You are using a model of type deepseek_v3 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
[12/07/2025-21:23:55] [TRT-LLM] [W] Failed to load hf generation config from DeepSeek-V3-Lite/bf16, encounter error: DeepSeek-V3-Lite/bf16 does not appear to have a file named generation_config.json. Checkout 'https://huggingface.co/DeepSeek-V3-Lite/bf16/tree/main' for available files.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
You are using a model of type deepseek_v3 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
[12/07/2025-21:23:55] [TRT-LLM] [W] Failed to load hf generation config from DeepSeek-V3-Lite/bf16, encounter error: DeepSeek-V3-Lite/bf16 does not appear to have a file named generation_config.json. Checkout 'https://huggingface.co/DeepSeek-V3-Lite/bf16/tree/main' for available files.
`torch_dtype` is deprecated! Use `dtype` instead!
[33;20mrank 0 using MpiCommSession to bind to external MPI processes
[0m[12/07/2025-21:23:55] [TRT-LLM] [W] Orchestrator is creating IPC executor
[12/07/2025-21:23:55] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_queue
[12/07/2025-21:23:55] [TRT-LLM] [I] Generating a new HMAC key for server worker_init_status_queue
[12/07/2025-21:23:55] [TRT-LLM] [I] Generating a new HMAC key for server proxy_result_queue
[12/07/2025-21:23:55] [TRT-LLM] [I] Generating a new HMAC key for server proxy_stats_queue
[12/07/2025-21:23:55] [TRT-LLM] [I] Generating a new HMAC key for server proxy_kv_cache_events_queue
`torch_dtype` is deprecated! Use `dtype` instead!
[33;20mrank 0 using MpiCommSession to bind to external MPI processes
[0m[12/07/2025-21:23:55] [TRT-LLM] [W] Orchestrator is creating IPC executor
[12/07/2025-21:23:55] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_queue
[12/07/2025-21:23:55] [TRT-LLM] [I] Generating a new HMAC key for server worker_init_status_queue
[12/07/2025-21:23:55] [TRT-LLM] [I] Generating a new HMAC key for server proxy_result_queue
[12/07/2025-21:23:55] [TRT-LLM] [I] Generating a new HMAC key for server proxy_stats_queue
[12/07/2025-21:23:55] [TRT-LLM] [I] Generating a new HMAC key for server proxy_kv_cache_events_queue
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Refreshed the MPI local session
[12/07/2025-21:23:56] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[12/07/2025-21:23:56] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info
[12/07/2025-21:23:56] [TRT-LLM] [RANK 0] [I] Worker process 6542 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167] for optimal NUMA-aware scheduling.
[12/07/2025-21:23:56] [TRT-LLM] [RANK 1] [I] Worker process 6543 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167] for optimal NUMA-aware scheduling.
[MPIDist::create_cp_comm] rank: 0, cp_rank: 0, cp_group: [0]
[MPIDist::create_tp_comm] rank: 0, tp_rank: 0, tp_group: [0, 1]
[MPIDist::create_cp_comm] rank: 1, cp_rank: 0, cp_group: [1]
[MPIDist::create_tp_comm] rank: 1, tp_rank: 1, tp_group: [0, 1]
[MPIDist::create_pp_comm] rank: 1, pp_rank: 0, pp_group: [1]
[MPIDist::create_pp_comm] rank: 0, pp_rank: 0, pp_group: [0]
[12/07/2025-21:23:56] [TRT-LLM] [RANK 0] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[12/07/2025-21:23:56] [TRT-LLM] [RANK 1] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
`torch_dtype` is deprecated! Use `dtype` instead!
[12/07/2025-21:23:57] [TRT-LLM] [RANK 0] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/07/2025-21:23:57] [TRT-LLM] [RANK 0] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/07/2025-21:23:57] [TRT-LLM] [RANK 1] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/07/2025-21:23:57] [TRT-LLM] [RANK 1] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [I] Worker process 6545 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167] for optimal NUMA-aware scheduling.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [I] Worker process 6547 CPU affinity set to [56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223] for optimal NUMA-aware scheduling.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] Worker process 6544 CPU affinity set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167] for optimal NUMA-aware scheduling.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [I] Worker process 6546 CPU affinity set to [56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223] for optimal NUMA-aware scheduling.
[MPIDist::create_cp_comm] rank: 1, cp_rank: 0, cp_group: [1, 3]
[MPIDist::create_cp_comm] rank: 3, cp_rank: 1, cp_group: [1, 3]
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [I] [MPIDist::__init__] Repurposing CP ranks to TP for Helix.
[MPIDist::create_tp_comm] rank: 1, tp_rank: 1, tp_group: [0, 1, 2, 3]
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [I] [MPIDist::__init__] Repurposing CP ranks to TP for Helix.
[MPIDist::create_tp_comm] rank: 3, tp_rank: 3, tp_group: [0, 1, 2, 3]
[MPIDist::create_cp_comm] rank: 0, cp_rank: 0, cp_group: [0, 2]
[MPIDist::create_cp_comm] rank: 2, cp_rank: 1, cp_group: [0, 2]
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] [MPIDist::__init__] Repurposing CP ranks to TP for Helix.
[MPIDist::create_tp_comm] rank: 0, tp_rank: 0, tp_group: [0, 1, 2, 3]
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [I] [MPIDist::__init__] Repurposing CP ranks to TP for Helix.
[MPIDist::create_tp_comm] rank: 2, tp_rank: 2, tp_group: [0, 1, 2, 3]
[MPIDist::create_pp_comm] rank: 0, pp_rank: 0, pp_group: [0]
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] [MPIDist::__init__] Restoring original mapping undoing Helix manipulation.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[MPIDist::create_pp_comm] rank: 3, pp_rank: 0, pp_group: [3]
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [I] [MPIDist::__init__] Restoring original mapping undoing Helix manipulation.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[MPIDist::create_pp_comm] rank: 2, pp_rank: 0, pp_group: [2]
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [I] [MPIDist::__init__] Restoring original mapping undoing Helix manipulation.
[MPIDist::create_pp_comm] rank: 1, pp_rank: 0, pp_group: [1]
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [I] [MPIDist::__init__] Restoring original mapping undoing Helix manipulation.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=False, has_speculative_draft_tokens=False, chunk_size=8192, chunked_prefill_buffer_batch_size=4)
`torch_dtype` is deprecated! Use `dtype` instead!
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[DeepseekV3ForCausalLM::__init__] Repurposing KVP ranks to TP while keeping other details the same.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[DeepseekV3ForCausalLM::__init__] Repurposing KVP ranks to TP while keeping other details the same.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[DeepseekV3ForCausalLM::__init__] Repurposing KVP ranks to TP while keeping other details the same.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[DeepseekV3ForCausalLM::__init__] Repurposing KVP ranks to TP while keeping other details the same.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 0] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/07/2025-21:23:57] [TRT-LLM] [RANK 1] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/07/2025-21:23:57] [TRT-LLM] [RANK 1] [I] Use 25.65 GB for model weights.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 0] [I] Use 25.65 GB for model weights.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 1] [I] Prefetching 53.30GB checkpoint files.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 0] [I] Prefetching 53.30GB checkpoint files.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 1] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors to memory...
[12/07/2025-21:23:57] [TRT-LLM] [RANK 0] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors to memory...
[12/07/2025-21:23:57] [TRT-LLM] [RANK 1] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors to memory...
[12/07/2025-21:23:57] [TRT-LLM] [RANK 0] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors to memory...
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [I] CutlassFusedMoE selects alltoall_method_type <AlltoallMethodType.NotEnabled: 0>
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [W] [MLA::__init__] Overriding mapping with CP detected.
[DeepseekV3ForCausalLM::__init__] Restoring original mapping.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[DeepseekV3ForCausalLM::__init__] Restoring original mapping.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[DeepseekV3ForCausalLM::__init__] Restoring original mapping.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [W] [MLA::__init__] Overriding mapping with CP detected.
[DeepseekV3ForCausalLM::__init__] Restoring original mapping.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [I] Use 13.49 GB for model weights.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] Use 13.49 GB for model weights.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [I] Prefetching 53.30GB checkpoint files.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 5] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors to memory...
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [I] Use 13.49 GB for model weights.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] Prefetching 53.30GB checkpoint files.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors to memory...
[12/07/2025-21:23:57] [TRT-LLM] [RANK 2] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors to memory...
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [I] Use 13.49 GB for model weights.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [I] Prefetching 53.30GB checkpoint files.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 3] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors to memory...
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [I] Prefetching 53.30GB checkpoint files.
[12/07/2025-21:23:57] [TRT-LLM] [RANK 4] [I] Prefetching DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors to memory...
[12/07/2025-21:24:05] [TRT-LLM] [RANK 2] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors.
[12/07/2025-21:25:08] [TRT-LLM] [RANK 0] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors.
[12/07/2025-21:26:51] [TRT-LLM] [RANK 1] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors.
[12/07/2025-21:27:37] [TRT-LLM] [RANK 5] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors.
[12/07/2025-21:27:51] [TRT-LLM] [RANK 0] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors.
[12/07/2025-21:27:58] [TRT-LLM] [RANK 3] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors.
[12/07/2025-21:28:00] [TRT-LLM] [RANK 2] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors.
[12/07/2025-21:28:00] [TRT-LLM] [RANK 4] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors.
[12/07/2025-21:29:33] [TRT-LLM] [RANK 1] [I] Finished prefetching DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors.
Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s][12/07/2025-21:29:33] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s]Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s]Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s][12/07/2025-21:29:33] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s][12/07/2025-21:29:33] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
Loading safetensors weights in parallel:   0%|          | 0/9 [00:00<?, ?it/s][12/07/2025-21:29:33] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00001-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00002-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00003-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00004-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00005-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00006-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00007-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00008-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 2] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 3] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 1] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 5] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 0] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
[12/07/2025-21:29:33] [TRT-LLM] [RANK 4] [I] Start to load safetensor file DeepSeek-V3-Lite/bf16/model-00009-of-000009.safetensors
Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00, 14.55it/s]Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00, 10.96it/s]Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00, 10.02it/s]Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00,  7.95it/s]Loading safetensors weights in parallel:  44%|████▍     | 4/9 [00:00<00:00, 14.59it/s]Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:00,  7.03it/s]Loading safetensors weights in parallel:  44%|████▍     | 4/9 [00:00<00:00, 14.53it/s]Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 30.98it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s]Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 30.33it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s]Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 29.32it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s]Loading safetensors weights in parallel:  33%|███▎      | 3/9 [00:00<00:00,  7.74it/s]Loading weights:   0%|          | 3/813 [00:00<00:30, 26.62it/s]Loading safetensors weights in parallel:  44%|████▍     | 4/9 [00:00<00:00, 10.60it/s]Loading weights:   0%|          | 3/813 [00:00<00:29, 27.78it/s]Loading weights:   1%|          | 8/813 [00:00<00:10, 79.86it/s]Loading safetensors weights in parallel:  22%|██▏       | 2/9 [00:00<00:01,  4.80it/s]Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 20.86it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s]Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 19.95it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s]Loading safetensors weights in parallel: 100%|██████████| 9/9 [00:00<00:00, 18.66it/s]
Loading weights:   0%|          | 0/813 [00:00<?, ?it/s]Loading weights:   2%|▏         | 13/813 [00:00<00:06, 123.74it/s]Loading weights:   0%|          | 3/813 [00:00<00:31, 25.89it/s]Loading weights:   5%|▌         | 43/813 [00:00<00:04, 162.47it/s]Loading weights:   5%|▌         | 43/813 [00:00<00:04, 164.37it/s]Loading weights:   2%|▏         | 13/813 [00:00<00:06, 123.29it/s]Loading weights:   5%|▌         | 43/813 [00:00<00:05, 131.19it/s]Loading weights:   5%|▌         | 43/813 [00:00<00:04, 165.46it/s]Loading weights:   5%|▌         | 43/813 [00:00<00:04, 173.29it/s]Loading weights:   9%|▊         | 70/813 [00:00<00:04, 170.16it/s]Loading weights:   9%|▊         | 70/813 [00:00<00:04, 171.09it/s]Loading weights:   5%|▌         | 43/813 [00:00<00:06, 126.36it/s]Loading weights:   9%|▊         | 70/813 [00:00<00:05, 137.05it/s]Loading weights:   9%|▊         | 70/813 [00:00<00:04, 182.78it/s]Loading weights:   9%|▊         | 70/813 [00:00<00:03, 198.11it/s]Loading weights:  12%|█▏        | 97/813 [00:00<00:04, 171.64it/s]Loading weights:  12%|█▏        | 97/813 [00:00<00:04, 172.18it/s]Loading weights:  12%|█▏        | 97/813 [00:00<00:03, 209.76it/s]Loading weights:  12%|█▏        | 97/813 [00:00<00:03, 190.21it/s]Loading weights:  12%|█▏        | 97/813 [00:00<00:05, 140.00it/s]Loading weights:   9%|▊         | 70/813 [00:00<00:05, 126.89it/s]Loading weights:  15%|█▌        | 124/813 [00:00<00:03, 173.80it/s]Loading weights:  15%|█▌        | 124/813 [00:00<00:03, 174.14it/s]Loading weights:  15%|█▌        | 124/813 [00:00<00:03, 218.83it/s]Loading weights:  15%|█▌        | 124/813 [00:00<00:03, 196.46it/s]Loading weights:  19%|█▊        | 151/813 [00:00<00:02, 223.20it/s]Loading weights:  15%|█▌        | 124/813 [00:00<00:04, 142.98it/s]Loading weights:  19%|█▊        | 151/813 [00:00<00:03, 173.83it/s]Loading weights:  19%|█▊        | 151/813 [00:00<00:03, 174.05it/s]Loading weights:  19%|█▊        | 151/813 [00:00<00:03, 199.70it/s]Loading weights:  12%|█▏        | 97/813 [00:00<00:05, 127.41it/s]Loading weights:  22%|██▏       | 178/813 [00:00<00:02, 227.02it/s]Loading weights:  22%|██▏       | 178/813 [00:01<00:03, 175.84it/s]Loading weights:  22%|██▏       | 178/813 [00:01<00:03, 175.98it/s]Loading weights:  22%|██▏       | 178/813 [00:00<00:03, 203.29it/s]Loading weights:  19%|█▊        | 151/813 [00:01<00:04, 144.55it/s]Loading weights:  25%|██▌       | 205/813 [00:00<00:02, 219.66it/s]Loading weights:  15%|█▌        | 124/813 [00:00<00:05, 128.84it/s]Loading weights:  25%|██▌       | 205/813 [00:01<00:02, 211.34it/s]Loading weights:  25%|██▌       | 205/813 [00:01<00:03, 184.24it/s]Loading weights:  25%|██▌       | 205/813 [00:01<00:03, 184.33it/s]Loading weights:  22%|██▏       | 178/813 [00:01<00:04, 146.18it/s]Loading weights:  29%|██▊       | 232/813 [00:01<00:02, 213.78it/s]Loading weights:  29%|██▊       | 232/813 [00:01<00:02, 215.90it/s]Loading weights:  29%|██▊       | 232/813 [00:01<00:02, 194.33it/s]Loading weights:  29%|██▊       | 232/813 [00:01<00:02, 194.35it/s]Loading weights:  19%|█▊        | 151/813 [00:01<00:05, 129.89it/s]Loading weights:  32%|███▏      | 259/813 [00:01<00:02, 207.96it/s]Loading weights:  32%|███▏      | 259/813 [00:01<00:02, 217.02it/s]Loading weights:  32%|███▏      | 259/813 [00:01<00:02, 201.55it/s]Loading weights:  32%|███▏      | 259/813 [00:01<00:02, 201.14it/s]Loading weights:  25%|██▌       | 205/813 [00:01<00:04, 141.26it/s]Loading weights:  35%|███▌      | 286/813 [00:01<00:02, 218.08it/s]Loading weights:  35%|███▌      | 286/813 [00:01<00:02, 205.23it/s]Loading weights:  35%|███▌      | 286/813 [00:01<00:02, 207.51it/s]Loading weights:  35%|███▌      | 286/813 [00:01<00:02, 207.13it/s]Loading weights:  22%|██▏       | 178/813 [00:01<00:04, 130.93it/s]Loading weights:  38%|███▊      | 313/813 [00:01<00:02, 213.74it/s]Loading weights:  38%|███▊      | 313/813 [00:01<00:02, 214.24it/s]Loading weights:  29%|██▊       | 232/813 [00:01<00:04, 138.25it/s]Loading weights:  38%|███▊      | 313/813 [00:01<00:02, 206.18it/s]Loading weights:  38%|███▊      | 313/813 [00:01<00:02, 205.24it/s]Loading weights:  25%|██▌       | 205/813 [00:01<00:04, 137.35it/s]Loading weights:  42%|████▏     | 340/813 [00:01<00:02, 221.81it/s]Loading weights:  42%|████▏     | 340/813 [00:01<00:02, 213.18it/s]Loading weights:  42%|████▏     | 340/813 [00:01<00:02, 206.66it/s]Loading weights:  42%|████▏     | 340/813 [00:01<00:02, 205.07it/s]Loading weights:  32%|███▏      | 259/813 [00:01<00:04, 136.06it/s]Loading weights:  45%|████▌     | 367/813 [00:01<00:01, 227.79it/s]Loading weights:  29%|██▊       | 232/813 [00:01<00:04, 141.83it/s]Loading weights:  45%|████▌     | 367/813 [00:01<00:02, 211.59it/s]Loading weights:  45%|████▌     | 367/813 [00:01<00:02, 206.43it/s]Loading weights:  45%|████▌     | 367/813 [00:01<00:02, 204.39it/s]Loading weights:  48%|████▊     | 394/813 [00:01<00:01, 229.71it/s]Loading weights:  48%|████▊     | 394/813 [00:01<00:02, 207.98it/s]Loading weights:  48%|████▊     | 394/813 [00:02<00:02, 205.70it/s]Loading weights:  48%|████▊     | 394/813 [00:02<00:02, 204.02it/s]Loading weights:  35%|███▌      | 286/813 [00:02<00:03, 134.45it/s]Loading weights:  32%|███▏      | 259/813 [00:01<00:03, 144.46it/s]Loading weights:  52%|█████▏    | 421/813 [00:01<00:01, 220.33it/s]Loading weights:  52%|█████▏    | 421/813 [00:02<00:01, 212.69it/s]Loading weights:  52%|█████▏    | 421/813 [00:02<00:01, 212.46it/s]Loading weights:  52%|█████▏    | 421/813 [00:02<00:01, 211.02it/s]Loading weights:  55%|█████▌    | 448/813 [00:02<00:01, 213.88it/s]Loading weights:  38%|███▊      | 313/813 [00:02<00:03, 138.34it/s]Loading weights:  35%|███▌      | 286/813 [00:02<00:03, 144.51it/s]Loading weights:  55%|█████▌    | 448/813 [00:02<00:01, 216.35it/s]Loading weights:  55%|█████▌    | 448/813 [00:02<00:01, 218.36it/s]Loading weights:  55%|█████▌    | 448/813 [00:02<00:01, 216.98it/s]Loading weights:  58%|█████▊    | 475/813 [00:02<00:01, 209.86it/s]Loading weights:  58%|█████▊    | 475/813 [00:02<00:01, 219.25it/s]Loading weights:  58%|█████▊    | 475/813 [00:02<00:01, 222.67it/s]Loading weights:  58%|█████▊    | 475/813 [00:02<00:01, 221.48it/s]Loading weights:  42%|████▏     | 340/813 [00:02<00:03, 140.90it/s]Loading weights:  38%|███▊      | 313/813 [00:02<00:03, 140.56it/s]Loading weights:  62%|██████▏   | 502/813 [00:02<00:01, 206.37it/s]Loading weights:  62%|██████▏   | 502/813 [00:02<00:01, 225.26it/s]Loading weights:  62%|██████▏   | 502/813 [00:02<00:01, 220.98it/s]Loading weights:  62%|██████▏   | 502/813 [00:02<00:01, 225.30it/s]Loading weights:  45%|████▌     | 367/813 [00:02<00:03, 142.78it/s]Loading weights:  65%|██████▌   | 529/813 [00:02<00:01, 225.81it/s]Loading weights:  65%|██████▌   | 529/813 [00:02<00:01, 222.16it/s]Loading weights:  65%|██████▌   | 529/813 [00:02<00:01, 228.05it/s]Loading weights:  65%|██████▌   | 529/813 [00:02<00:01, 205.28it/s]Loading weights:  42%|████▏     | 340/813 [00:02<00:03, 138.35it/s]Loading weights:  68%|██████▊   | 556/813 [00:02<00:01, 227.94it/s]Loading weights:  68%|██████▊   | 556/813 [00:02<00:01, 227.41it/s]Loading weights:  68%|██████▊   | 556/813 [00:02<00:01, 222.30it/s]Loading weights:  68%|██████▊   | 556/813 [00:02<00:01, 206.16it/s]Loading weights:  48%|████▊     | 394/813 [00:02<00:02, 143.20it/s]Loading weights:  45%|████▌     | 367/813 [00:02<00:03, 136.81it/s]Loading weights:  72%|███████▏  | 583/813 [00:02<00:01, 228.65it/s]Loading weights:  72%|███████▏  | 583/813 [00:02<00:01, 219.82it/s]Loading weights:  72%|███████▏  | 583/813 [00:02<00:01, 216.46it/s]Loading weights:  72%|███████▏  | 583/813 [00:02<00:01, 206.44it/s]Loading weights:  75%|███████▌  | 610/813 [00:03<00:00, 229.33it/s]Loading weights:  52%|█████▏    | 421/813 [00:03<00:02, 138.43it/s]Loading weights:  75%|███████▌  | 610/813 [00:03<00:00, 214.57it/s]Loading weights:  75%|███████▌  | 610/813 [00:02<00:00, 212.31it/s]Loading weights:  75%|███████▌  | 610/813 [00:02<00:00, 206.91it/s]Loading weights:  48%|████▊     | 394/813 [00:02<00:03, 134.24it/s]Loading weights:  78%|███████▊  | 637/813 [00:03<00:00, 228.99it/s]Loading weights:  78%|███████▊  | 637/813 [00:03<00:00, 212.03it/s]Loading weights:  78%|███████▊  | 637/813 [00:03<00:00, 210.16it/s]Loading weights:  78%|███████▊  | 637/813 [00:03<00:00, 206.36it/s]Loading weights:  55%|█████▌    | 448/813 [00:03<00:02, 135.83it/s]Loading weights:  82%|████████▏ | 664/813 [00:03<00:00, 224.19it/s]Loading weights:  52%|█████▏    | 421/813 [00:03<00:02, 138.19it/s]Loading weights:  82%|████████▏ | 664/813 [00:03<00:00, 215.10it/s]Loading weights:  82%|████████▏ | 664/813 [00:03<00:00, 205.48it/s]Loading weights:  82%|████████▏ | 664/813 [00:03<00:00, 204.55it/s]Loading weights:  85%|████████▍ | 691/813 [00:03<00:00, 219.57it/s]Loading weights:  85%|████████▍ | 691/813 [00:03<00:00, 222.38it/s]Loading weights:  58%|█████▊    | 475/813 [00:03<00:02, 134.66it/s]Loading weights:  55%|█████▌    | 448/813 [00:03<00:02, 142.31it/s]Loading weights:  85%|████████▍ | 691/813 [00:03<00:00, 199.82it/s]Loading weights:  85%|████████▍ | 691/813 [00:03<00:00, 199.20it/s]Loading weights:  88%|████████▊ | 718/813 [00:03<00:00, 215.38it/s]Loading weights:  88%|████████▊ | 718/813 [00:03<00:00, 226.85it/s]Loading weights:  88%|████████▊ | 718/813 [00:03<00:00, 195.42it/s]Loading weights:  88%|████████▊ | 718/813 [00:03<00:00, 195.81it/s]Loading weights:  58%|█████▊    | 475/813 [00:03<00:02, 145.18it/s]Loading weights:  92%|█████████▏| 745/813 [00:03<00:00, 214.52it/s]Loading weights:  92%|█████████▏| 745/813 [00:03<00:00, 231.05it/s]Loading weights:  62%|██████▏   | 502/813 [00:03<00:02, 133.53it/s]Loading weights:  92%|█████████▏| 745/813 [00:03<00:00, 199.42it/s]Loading weights:  92%|█████████▏| 745/813 [00:03<00:00, 197.48it/s]Loading weights:  95%|█████████▍| 772/813 [00:03<00:00, 234.51it/s]Loading weights:  95%|█████████▍| 772/813 [00:03<00:00, 215.17it/s]Loading weights:  62%|██████▏   | 502/813 [00:03<00:02, 146.76it/s]Loading weights:  65%|██████▌   | 529/813 [00:03<00:02, 132.83it/s]Loading weights:  95%|█████████▍| 772/813 [00:03<00:00, 202.69it/s]Loading weights:  98%|█████████▊| 799/813 [00:03<00:00, 235.61it/s]Loading weights:  95%|█████████▍| 772/813 [00:03<00:00, 199.42it/s]Loading weights:  98%|█████████▊| 799/813 [00:03<00:00, 215.34it/s]Loading weights: 100%|██████████| 813/813 [00:03<00:00, 217.12it/s]
Model init total -- 340.42s
Loading weights: 100%|██████████| 813/813 [00:03<00:00, 207.53it/s]
Model init total -- 340.39s
Loading weights:  65%|██████▌   | 529/813 [00:03<00:01, 148.22it/s]Loading weights:  98%|█████████▊| 799/813 [00:03<00:00, 205.52it/s]Loading weights:  98%|█████████▊| 799/813 [00:04<00:00, 201.25it/s]Loading weights: 100%|██████████| 813/813 [00:03<00:00, 208.87it/s]
Model init total -- 340.61s
Loading weights: 100%|██████████| 813/813 [00:04<00:00, 201.16it/s]
Model init total -- 340.45s
Loading weights:  68%|██████▊   | 556/813 [00:04<00:01, 132.83it/s]Loading weights:  68%|██████▊   | 556/813 [00:03<00:01, 149.63it/s]Loading weights:  72%|███████▏  | 583/813 [00:04<00:01, 133.13it/s][12/07/2025-21:29:38] [TRT-LLM] [RANK 3] [I] max_seq_len is not specified, using inferred value 4096
[12/07/2025-21:29:38] [TRT-LLM] [RANK 3] [I] Using Sampler: TorchSampler
[12/07/2025-21:29:38] [TRT-LLM] [RANK 4] [I] max_seq_len is not specified, using inferred value 4096
[12/07/2025-21:29:38] [TRT-LLM] [RANK 4] [I] Using Sampler: TorchSampler
Loading weights:  72%|███████▏  | 583/813 [00:04<00:01, 150.33it/s][12/07/2025-21:29:38] [TRT-LLM] [RANK 2] [I] max_seq_len is not specified, using inferred value 4096
[12/07/2025-21:29:38] [TRT-LLM] [RANK 2] [I] Using Sampler: TorchSampler
[12/07/2025-21:29:38] [TRT-LLM] [RANK 5] [I] max_seq_len is not specified, using inferred value 4096
[12/07/2025-21:29:38] [TRT-LLM] [RANK 5] [I] Using Sampler: TorchSampler
[12/07/2025-21:29:38] [TRT-LLM] [RANK 4] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/07/2025-21:29:38] [TRT-LLM] [RANK 5] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/07/2025-21:29:38] [TRT-LLM] [RANK 2] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/07/2025-21:29:38] [TRT-LLM] [RANK 3] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60967, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.79 GiB for max tokens in paged KV cache (1950944).
[12/07/2025-21:29:38] [TRT-LLM] [RANK 2] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/07/2025-21:29:38] [TRT-LLM] [RANK 2] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60967, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.79 GiB for max tokens in paged KV cache (1950944).
[12/07/2025-21:29:38] [TRT-LLM] [RANK 4] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/07/2025-21:29:38] [TRT-LLM] [RANK 4] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60967, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=60967, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.79 GiB for max tokens in paged KV cache (1950944).
[12/07/2025-21:29:38] [TRT-LLM] [RANK 3] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/07/2025-21:29:38] [TRT-LLM] [RANK 3] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 62.79 GiB for max tokens in paged KV cache (1950944).
[12/07/2025-21:29:38] [TRT-LLM] [RANK 5] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/07/2025-21:29:38] [TRT-LLM] [RANK 5] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
Loading weights:  75%|███████▌  | 610/813 [00:04<00:01, 133.23it/s]Loading weights:  75%|███████▌  | 610/813 [00:04<00:01, 151.01it/s][TensorRT-LLM][INFO][2] UcxConnectionManager::UcxConnectionManager localIp: 10.34.4.83
[TensorRT-LLM][INFO][3] UcxConnectionManager::UcxConnectionManager localIp: 10.34.4.83
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager localIp: 10.34.4.83
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager localIp: 10.34.4.83
[TensorRT-LLM][INFO][2] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.34.4.83:33441
[TensorRT-LLM][INFO][2] UcxConnectionManager::UcxConnectionManager ip: 10.34.4.83, port: 33441
[TensorRT-LLM][INFO][3] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.34.4.83:40093
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.34.4.83:38047
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.34.4.83:44271
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager ip: 10.34.4.83, port: 44271
[TensorRT-LLM][INFO][3] UcxConnectionManager::UcxConnectionManager ip: 10.34.4.83, port: 40093
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager ip: 10.34.4.83, port: 38047
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[12/07/2025-21:29:38] [TRT-LLM] [RANK 4] [I] Running autotuner warmup...
[12/07/2025-21:29:38] [TRT-LLM] [RANK 4] [I] [Autotuner] Autotuning process starts ...
[12/07/2025-21:29:38] [TRT-LLM] [RANK 5] [I] Running autotuner warmup...
[12/07/2025-21:29:38] [TRT-LLM] [RANK 2] [I] Running autotuner warmup...
[12/07/2025-21:29:38] [TRT-LLM] [RANK 3] [I] Running autotuner warmup...
[12/07/2025-21:29:38] [TRT-LLM] [RANK 5] [I] [Autotuner] Autotuning process starts ...
[12/07/2025-21:29:38] [TRT-LLM] [RANK 3] [I] [Autotuner] Autotuning process starts ...
[12/07/2025-21:29:38] [TRT-LLM] [RANK 2] [I] [Autotuner] Autotuning process starts ...
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: []
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: []
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: []
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: []
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2, block_ids: [256]
Loading weights:  78%|███████▊  | 637/813 [00:04<00:01, 133.20it/s]Loading weights:  78%|███████▊  | 637/813 [00:04<00:01, 151.25it/s]Loading weights:  82%|████████▏ | 664/813 [00:04<00:01, 137.92it/s]Loading weights:  82%|████████▏ | 664/813 [00:04<00:01, 145.93it/s]Loading weights:  85%|████████▍ | 691/813 [00:05<00:00, 141.09it/s]Loading weights:  85%|████████▍ | 691/813 [00:04<00:00, 141.92it/s]Loading weights:  88%|████████▊ | 718/813 [00:05<00:00, 142.83it/s]Loading weights:  88%|████████▊ | 718/813 [00:05<00:00, 138.38it/s]Loading weights:  92%|█████████▏| 745/813 [00:05<00:00, 144.74it/s]Loading weights:  92%|█████████▏| 745/813 [00:05<00:00, 137.05it/s]Loading weights:  95%|█████████▍| 772/813 [00:05<00:00, 146.01it/s]Loading weights:  95%|█████████▍| 772/813 [00:05<00:00, 136.30it/s]Loading weights:  98%|█████████▊| 799/813 [00:05<00:00, 146.66it/s]Loading weights: 100%|██████████| 813/813 [00:05<00:00, 139.71it/s]
Model init total -- 342.49s
Loading weights:  98%|█████████▊| 799/813 [00:05<00:00, 135.04it/s]Loading weights: 100%|██████████| 813/813 [00:05<00:00, 140.30it/s]
Model init total -- 342.71s
[12/07/2025-21:29:40] [TRT-LLM] [RANK 1] [I] max_seq_len is not specified, using inferred value 4096
[12/07/2025-21:29:40] [TRT-LLM] [RANK 1] [I] Using Sampler: TorchSampler
[12/07/2025-21:29:40] [TRT-LLM] [RANK 1] [I] [kv cache manager] Primary/secondary blocks for window sizes set to {4096: (257, 0)} for estimation dry run
[12/07/2025-21:29:40] [TRT-LLM] [RANK 1] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=257, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.26 GiB for max tokens in paged KV cache (8224).
[12/07/2025-21:29:40] [TRT-LLM] [RANK 1] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/07/2025-21:29:40] [TRT-LLM] [RANK 1] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[12/07/2025-21:29:40] [TRT-LLM] [RANK 0] [I] max_seq_len is not specified, using inferred value 4096
[12/07/2025-21:29:40] [TRT-LLM] [RANK 0] [I] Using Sampler: TorchSampler
[12/07/2025-21:29:40] [TRT-LLM] [RANK 0] [I] [kv cache manager] Primary/secondary blocks for window sizes set to {4096: (257, 0)} for estimation dry run
[12/07/2025-21:29:40] [TRT-LLM] [RANK 0] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=257, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.26 GiB for max tokens in paged KV cache (8224).
[12/07/2025-21:29:40] [TRT-LLM] [RANK 0] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/07/2025-21:29:40] [TRT-LLM] [RANK 0] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager localIp: 10.34.4.83
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.34.4.83:33469
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager ip: 10.34.4.83, port: 33469
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager localIp: 10.34.4.83
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.34.4.83:44069
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager ip: 10.34.4.83, port: 44069
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[12/07/2025-21:29:40] [TRT-LLM] [RANK 0] [I] Running autotuner warmup...
[12/07/2025-21:29:40] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process starts ...
[12/07/2025-21:29:40] [TRT-LLM] [RANK 1] [I] Running autotuner warmup...
[12/07/2025-21:29:40] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process starts ...
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2, block_ids: [256]
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][INFO] Detecting local TP group for rank 1
[TensorRT-LLM][INFO] Detecting local TP group for rank 0
[TensorRT-LLM][INFO] TP group is intra-node for rank 1
[TensorRT-LLM][INFO] TP group is intra-node for rank 0
[TensorRT-LLM][INFO] Detecting local TP group for rank 1
[TensorRT-LLM][INFO] Detecting local TP group for rank 0
[TensorRT-LLM][INFO] Detecting local TP group for rank 3
[TensorRT-LLM][INFO] Detecting local TP group for rank 2
[TensorRT-LLM][INFO] TP group is intra-node for rank 1
[TensorRT-LLM][INFO] TP group is intra-node for rank 0
[TensorRT-LLM][INFO] TP group is intra-node for rank 2
[TensorRT-LLM][INFO] TP group is intra-node for rank 3
[12/07/2025-21:29:48] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process ends
[12/07/2025-21:29:48] [TRT-LLM] [RANK 1] [I] [Autotuner] Cache size after warmup is 28
[12/07/2025-21:29:48] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process ends
[12/07/2025-21:29:48] [TRT-LLM] [RANK 0] [I] [Autotuner] Cache size after warmup is 28
[12/07/2025-21:29:48] [TRT-LLM] [RANK 0] [I] global_steady_clock_offset at each rank: [0.0, 6.000016583129764e-06]
[12/07/2025-21:29:48] [TRT-LLM] [RANK 1] [I] Setting global_steady_clock_offset: 6.000016583129764e-06 seconds for rank 1
[12/07/2025-21:29:48] [TRT-LLM] [RANK 0] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/07/2025-21:29:48] [TRT-LLM] [RANK 0] [I] Memory used after loading model weights (inside torch) in memory usage profiling: 26.30 GiB
[12/07/2025-21:29:48] [TRT-LLM] [RANK 0] [I] Memory used after loading model weights (outside torch) in memory usage profiling: 5.77 GiB
[12/07/2025-21:29:48] [TRT-LLM] [RANK 1] [I] Memory used after loading model weights (inside torch) in memory usage profiling: 26.30 GiB
[12/07/2025-21:29:48] [TRT-LLM] [RANK 1] [I] Memory used after loading model weights (outside torch) in memory usage profiling: 4.41 GiB
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([ 63270, 102515, 117336,  ...,  33972,  34776,  39067], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2049, block_ids: [255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2050, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([ 63270, 102515, 117336,  ...,  33972,  34776,  39067], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2049, block_ids: [255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129, 128]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2050, block_ids: [256]
[12/07/2025-21:29:48] [TRT-LLM] [RANK 2] [I] [Autotuner] Autotuning process ends
[12/07/2025-21:29:48] [TRT-LLM] [RANK 2] [I] [Autotuner] Cache size after warmup is 28
[12/07/2025-21:29:48] [TRT-LLM] [RANK 4] [I] [Autotuner] Autotuning process ends
[12/07/2025-21:29:48] [TRT-LLM] [RANK 4] [I] [Autotuner] Cache size after warmup is 28
[12/07/2025-21:29:48] [TRT-LLM] [RANK 5] [I] [Autotuner] Autotuning process ends
[12/07/2025-21:29:48] [TRT-LLM] [RANK 3] [I] [Autotuner] Autotuning process ends
[12/07/2025-21:29:48] [TRT-LLM] [RANK 3] [I] [Autotuner] Cache size after warmup is 28
[12/07/2025-21:29:48] [TRT-LLM] [RANK 5] [I] [Autotuner] Cache size after warmup is 28
[12/07/2025-21:29:48] [TRT-LLM] [RANK 4] [I] Setting global_steady_clock_offset: 1.00000761449337e-06 seconds for rank 2
[12/07/2025-21:29:48] [TRT-LLM] [RANK 5] [I] Setting global_steady_clock_offset: -1.00000761449337e-06 seconds for rank 3
[12/07/2025-21:29:48] [TRT-LLM] [RANK 2] [I] global_steady_clock_offset at each rank: [0.0, 1.00000761449337e-06, 1.00000761449337e-06, -1.00000761449337e-06]
[12/07/2025-21:29:48] [TRT-LLM] [RANK 2] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/07/2025-21:29:48] [TRT-LLM] [RANK 3] [I] Setting global_steady_clock_offset: 1.00000761449337e-06 seconds for rank 1
[12/07/2025-21:29:48] [TRT-LLM] [RANK 3] [I] Setting PyTorch memory fraction to 0.7459883738826193 (199.69281005859375 GiB)
[12/07/2025-21:29:48] [TRT-LLM] [RANK 2] [I] Setting PyTorch memory fraction to 0.7461051138727922 (199.72406005859375 GiB)
[12/07/2025-21:29:48] [TRT-LLM] [RANK 4] [I] Setting PyTorch memory fraction to 0.7459883738826193 (199.69281005859375 GiB)
[12/07/2025-21:29:48] [TRT-LLM] [RANK 5] [I] Setting PyTorch memory fraction to 0.7465720738334837 (199.84906005859375 GiB)
[12/07/2025-21:29:48] [TRT-LLM] [RANK 4] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=2 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/07/2025-21:29:48] [TRT-LLM] [RANK 5] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=2 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/07/2025-21:29:48] [TRT-LLM] [RANK 3] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=2 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/07/2025-21:29:48] [TRT-LLM] [RANK 2] [I] LLM Args:
model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=2 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/07/2025-21:29:48] [TRT-LLM] [RANK 2] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=2 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={'cp_type': <CpType.HELIX: 3>, 'tokens_per_block': 32} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=None, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=0, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/07/2025-21:29:48] [TRT-LLM] [I] get signal from executor worker
[32mINFO[0m:     Started server process [[36m8100[0m]
[32mINFO[0m:     Waiting for application startup.
[32mINFO[0m:     Application startup complete.
[32mINFO[0m:     Uvicorn running on [1mhttp://localhost:8002[0m (Press CTRL+C to quit)
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2048
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2048
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2049
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2050
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2049
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 2050
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] Memory dynamically allocated during inference (inside torch) in memory usage profiling: 0.41 GiB
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] Memory used outside torch (e.g., NCCL and CUDA graphs) in memory usage profiling: 4.53 GiB
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] Memory dynamically allocated during inference (inside torch) in memory usage profiling: 0.41 GiB
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] Memory used outside torch (e.g., NCCL and CUDA graphs) in memory usage profiling: 5.89 GiB
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] Peak memory during memory usage profiling (torch + non-torch): 31.24 GiB, available KV cache memory when calculating max tokens: 59.18 GiB, fraction is set 0.25, kv size is 34560. device total memory 267.69 GiB, , tmp kv_mem 0.26 GiB
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] Estimated max memory in KV cache : 59.18 GiB
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [W] Both free_gpu_memory_fraction and max_tokens are set (to 0.25 and 1838622 with free memory 237.17523193359375GiB of total memory 267.68890380859375GiB, respectively). The smaller value will be used.
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] Peak memory during memory usage profiling (torch + non-torch): 32.60 GiB, available KV cache memory when calculating max tokens: 58.84 GiB, fraction is set 0.25, kv size is 34560. device total memory 267.69 GiB, , tmp kv_mem 0.26 GiB
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] Estimated max memory in KV cache : 58.84 GiB
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [W] Both free_gpu_memory_fraction and max_tokens are set (to 0.25 and 1828062 with free memory 235.81573486328125GiB of total memory 267.68890380859375GiB, respectively). The smaller value will be used.
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] Adjusted attention window size to 4096 in blocks_per_window
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] Adjusted attention window size to 4096 in blocks_per_window
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=57126, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 58.84 GiB for max tokens in paged KV cache (1828032).
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 128 [window size=4096], tokens per block=32, primary blocks=57126, secondary blocks=0, max sequence length=4096
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 58.84 GiB for max tokens in paged KV cache (1828032).
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] max_seq_len=4096, max_num_requests=2048, max_num_tokens=8192, max_batch_size=2048
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] Using UCX kv-cache transceiver. If your devices are not in the same domain, please consider setting UCX_CUDA_IPC_ENABLE_MNNVL=n, UCX_RNDV_SCHEME=put_zcopy and/or unset UCX_NET_DEVICES upon server hangs or lower-than-expected performance.
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO] CacheTransBufferManager: mMaxNumTokens:0, mRecvBufferCount:1, mSendBufferCount:1,mTransferBufferSize:536870912, mPreAllocBufferSize:1073741824,mOnlyUseDynamicBuffer:0 mUseFabricMemory:0 mDataType:7
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager localIp: 10.34.4.83
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager localIp: 10.34.4.83
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.34.4.83:43207
[TensorRT-LLM][INFO][1] UcxConnectionManager::UcxConnectionManager ip: 10.34.4.83, port: 43207
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager mZmqRepEndpoint: tcp://10.34.4.83:34637
[TensorRT-LLM][INFO][0] UcxConnectionManager::UcxConnectionManager ip: 10.34.4.83, port: 34637
[TensorRT-LLM][INFO] UCX Connection Manager created
[TensorRT-LLM][INFO] UCX Connection Manager created
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] Running autotuner warmup...
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process starts ...
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] Running autotuner warmup...
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process starts ...
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2, block_ids: [256]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[   0,    1,    2,  ..., 4094,    0,    1]], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0, 0, 0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([4095, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 0, block_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 1, block_ids: [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2, block_ids: [256]
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 184902656 bytes
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] [Autotuner] Autotuning process ends
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] [Autotuner] Cache size after warmup is 28
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] [Autotuner] Autotuning process ends
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] [Autotuner] Cache size after warmup is 28
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] global_steady_clock_offset at each rank: [0.0, 3.00002284348011e-06]
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] Setting global_steady_clock_offset: 3.00002284348011e-06 seconds for rank 1
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] Setting PyTorch memory fraction to 0.766439760911028 (205.16741943359375 GiB)
[12/07/2025-21:29:49] [TRT-LLM] [RANK 1] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=1838622, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=63542786048, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] Setting PyTorch memory fraction to 0.7613611153229217 (203.80792236328125 GiB)
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] LLM Args:
model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=1828062, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=63177848832, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/07/2025-21:29:49] [TRT-LLM] [RANK 0] [I] model='DeepSeek-V3-Lite/bf16' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=2 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=8 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=False, max_tokens=1828062, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.25, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=False, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=63177848832, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=CacheTransceiverConfig(backend='UCX', max_tokens_in_buffer=None, kv_transfer_timeout_ms=None, kv_transfer_sender_future_timeout_ms=1000) sparse_attention_config=None speculative_config=None max_batch_size=2048 max_input_len=1024 max_seq_len=None max_beam_width=1 max_num_tokens=8192 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='DeepSeek-V3-Lite/bf16' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None env_overrides=None garbage_collection_gen0_threshold=20000 cuda_graph_config=None attention_dp_config=None disable_overlap_scheduler=True moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False disable_flashinfer_sampling=False
[12/07/2025-21:29:49] [TRT-LLM] [I] get signal from executor worker
[32mINFO[0m:     Started server process [[36m8099[0m]
[32mINFO[0m:     Waiting for application startup.
[32mINFO[0m:     Application startup complete.
[32mINFO[0m:     Uvicorn running on [1mhttp://localhost:8001[0m (Press CTRL+C to quit)
[32mINFO[0m:     127.0.0.1:37746 - "[1mGET /health HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     127.0.0.1:56260 - "[1mGET /health HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     127.0.0.1:37754 - "[1mGET /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[32mINFO[0m:     127.0.0.1:56270 - "[1mGET /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[12/07/2025-21:29:50] [TRT-LLM] [I] The steady clock offset between local and disagg server: -0.00033549999352544546 second
[32mINFO[0m:     127.0.0.1:37766 - "[1mPOST /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[12/07/2025-21:29:50] [TRT-LLM] [I] The steady clock offset between local and disagg server: -0.00013250000483822078 second
[32mINFO[0m:     127.0.0.1:56280 - "[1mPOST /steady_clock_offset HTTP/1.1[0m" [32m200 OK[0m
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([28803, 27194,   344,   270,  1606,  2112,  9059,   295,  6677,  5169,
         7677,   513, 30603, 26251,   538,  2883,  4577,    14, 21779, 26498,
        32329,    14,   305, 80124,    16,   983, 10401,   304, 35317, 10639,
           14, 15852, 41366,    14,   305, 15075,  9670,   396, 52498, 29810,
           14, 25676,    14,   305,  1482,    16, 12795, 12327,  5217,  4271,
          344,   223], device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([28803, 27194,   344,   270,  1606,  2112,  9059,   295,  6677,  5169,
         7677,   513, 30603, 26251,   538,  2883,  4577,    14, 21779, 26498,
        32329,    14,   305, 80124,    16,   983, 10401,   304, 35317, 10639,
           14, 15852, 41366,    14,   305, 15075,  9670,   396, 52498, 29810,
           14, 25676,    14,   305,  1482,    16, 12795, 12327,  5217,  4271,
          344,   223], device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]],
       device='cuda:1', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  52, 4095,    2,  ...,    0,    0,    0], device='cuda:1',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]],
       device='cuda:0', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: None
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [0]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  52, 4095,    2,  ...,    0,    0,    0], device='cuda:0',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257, 258]
[32mINFO[0m:     127.0.0.1:37746 - "[1mPOST /v1/completions HTTP/1.1[0m" [32m200 OK[0m
[ExecutorRequestQueue::_merge_helix_requests][rank 0][cp_rank 0]: input_ids_this_rank: [28803, 27194, 344, 270, 1606, 2112, 9059, 295, 6677, 5169, 7677, 513, 30603, 26251, 538, 2883, 4577, 14, 21779, 26498, 32329, 14, 305, 80124, 16, 983, 10401, 304, 35317, 10639, 14, 15852]
[ExecutorRequestQueue::_merge_helix_requests][rank 0][cp_rank 0]: position_ids_this_rank: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[ExecutorRequestQueue::_merge_helix_requests][rank 3][cp_rank 1]: input_ids_this_rank: [41366, 14, 305, 15075, 9670, 396, 52498, 29810, 14, 25676, 14, 305, 1482, 16, 12795, 12327, 5217, 4271, 344, 223]
[ExecutorRequestQueue::_merge_helix_requests][rank 3][cp_rank 1]: position_ids_this_rank: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
[TensorRT-LLM][INFO] Set logger level to INFO
[ExecutorRequestQueue::_merge_helix_requests][rank 2][cp_rank 1]: input_ids_this_rank: [41366, 14, 305, 15075, 9670, 396, 52498, 29810, 14, 25676, 14, 305, 1482, 16, 12795, 12327, 5217, 4271, 344, 223]
[ExecutorRequestQueue::_merge_helix_requests][rank 2][cp_rank 1]: position_ids_this_rank: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
[TensorRT-LLM][INFO] Set logger level to INFO
[ExecutorRequestQueue::_merge_helix_requests][rank 1][cp_rank 0]: input_ids_this_rank: [28803, 27194, 344, 270, 1606, 2112, 9059, 295, 6677, 5169, 7677, 513, 30603, 26251, 538, 2883, 4577, 14, 21779, 26498, 32329, 14, 305, 80124, 16, 983, 10401, 304, 35317, 10639, 14, 15852]
[ExecutorRequestQueue::_merge_helix_requests][rank 1][cp_rank 0]: position_ids_this_rank: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[TensorRT-LLM][INFO] Set logger level to INFO
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 0][cp_rank 0]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([56840], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[52]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 1][cp_rank 0]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([56840], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[52]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 2][cp_rank 1]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[PyExecutor::_prepare_disagg_gen_transmission_complete][rank 3][cp_rank 1]: TRANSMISSION COMPLETE for request ID: 2048
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([56840], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[52]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [20]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  21, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([56840], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[52]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [20]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  21, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:3', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:5', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:4', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: BEFORE INPUT LAYERNORM hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.1309, -0.0894,  0.0942,  ...,  0.0012, -0.0210,  0.1191]],
       device='cuda:2', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank1_cp0_tp1_before_input_layernorm.pt
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:3', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank3_cp0_tp3_before_input_layernorm.pt
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:5', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank2_cp0_tp2_before_input_layernorm.pt
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank0_cp0_tp0_before_input_layernorm.pt
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: BEFORE ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3105, -0.2197,  0.2148,  ...,  0.0023, -0.0447,  0.2363]],
       device='cuda:2', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank1_cp0_tp1_before_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank3_cp0_tp3_before_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank2_cp0_tp2_before_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank0_cp0_tp0_before_attn.pt
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 184902656 bytes to 380108800 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 184902656 bytes to 380108800 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 184902656 bytes to 380108800 bytes
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 184902656 bytes to 380108800 bytes
[MLA::forward][rank 2][cp_rank 1]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[-3.5524e-05, -3.3417e-03,  8.1787e-03,  ...,  1.8311e-03,
          6.0654e-04,  3.9291e-04]], device='cuda:4', dtype=torch.bfloat16)
[MLA::forward][rank 0][cp_rank 0]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[-0.0023,  0.0042,  0.0032,  ..., -0.0003,  0.0005, -0.0003]],
       device='cuda:2', dtype=torch.bfloat16)
[MLA::forward][rank 1][cp_rank 0]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[ 4.4861e-03, -2.3651e-03, -5.0068e-05,  ..., -3.4332e-04,
          3.0823e-03, -2.2736e-03]], device='cuda:3', dtype=torch.bfloat16)
[MLA::forward][rank 3][cp_rank 1]: BEFORE O_PROJ attn_output: torch.Size([1, 1024]) 
 tensor([[-0.0018, -0.0044, -0.0079,  ..., -0.0009, -0.0009,  0.0006]],
       device='cuda:5', dtype=torch.bfloat16)
[MLA::forward][rank 0][cp_rank 0]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:2', dtype=torch.bfloat16)
[MLA::forward][rank 1][cp_rank 0]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:3', dtype=torch.bfloat16)
[MLA::forward][rank 2][cp_rank 1]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:4', dtype=torch.bfloat16)
[MLA::forward][rank 3][cp_rank 1]: AFTER O_PROJ attn_output: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:5', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:2', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:3', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:5', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: AFTER ATTN hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.0005, -0.0172, -0.0164,  ..., -0.0105, -0.1250, -0.0006]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank0_cp0_tp0_after_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank1_cp0_tp1_after_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank3_cp0_tp3_after_attn.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank2_cp0_tp2_after_attn.pt
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3477, -0.2656,  0.0327,  ..., -0.1367, -0.4453,  0.3652]],
       device='cuda:2', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3477, -0.2656,  0.0327,  ..., -0.1367, -0.4453,  0.3652]],
       device='cuda:3', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3477, -0.2656,  0.0327,  ..., -0.1367, -0.4453,  0.3652]],
       device='cuda:5', dtype=torch.bfloat16)
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: AFTER MLP hidden_states: torch.Size([1, 2560]) 
 tensor([[-0.3477, -0.2656,  0.0327,  ..., -0.1367, -0.4453,  0.3652]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank0_cp0_tp0_after_mlp.pt
[DeepseekV3DecoderLayer::forward][rank 0][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1465, -0.1152,  0.0112,  ..., -0.0562, -0.1895,  0.1465]],
       device='cuda:2', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank3_cp0_tp3_after_mlp.pt
[DeepseekV3DecoderLayer::forward][rank 3][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1465, -0.1152,  0.0112,  ..., -0.0562, -0.1895,  0.1465]],
       device='cuda:5', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank1_cp0_tp1_after_mlp.pt
[DeepseekV3DecoderLayer::forward][rank 1][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1465, -0.1152,  0.0112,  ..., -0.0562, -0.1895,  0.1465]],
       device='cuda:3', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank2_cp0_tp2_after_mlp.pt
[DeepseekV3DecoderLayer::forward][rank 2][cp_rank 0]: residual: torch.Size([1, 2560]) 
 tensor([[-0.1465, -0.1152,  0.0112,  ..., -0.0562, -0.1895,  0.1465]],
       device='cuda:4', dtype=torch.bfloat16)
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank0_cp0_tp0_residual.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank3_cp0_tp3_residual.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank1_cp0_tp1_residual.pt
Tensor saved to: /home/bbuddharaju/scratch/TensorRT-LLM/mixedTP2CP2/rank2_cp0_tp2_residual.pt
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([8158], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[53]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([8158], device='cuda:2', dtype=torch.int32)
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[53]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([8158], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[53]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [21]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([8158], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[53]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [21]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  22, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  22, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([850], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[54]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [22]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([850], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[54]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [22]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([850], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[54]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  23, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  23, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([850], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[54]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([271], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[55]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([271], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[55]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([271], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[55]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [23]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  24, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([271], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[55]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [23]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  24, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([28803], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[56]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [24]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([28803], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[56]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([28803], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[56]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [24]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  25, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  25, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([28803], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[56]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([27194], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[57]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [25]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([27194], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[57]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([27194], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[57]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([27194], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[57]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [25]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  26, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  26, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[58]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[58]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[58]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [26]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[58]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [26]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  27, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  27, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[59]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [27]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[59]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  28, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[59]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [27]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[59]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  28, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4138], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[60]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [28]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  29, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4138], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4138], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[60]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4138], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[60]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[60]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [28]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  29, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([295], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[61]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([295], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([295], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[61]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [29]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[61]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([295], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[61]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [29]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  30, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  30, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[62]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [30]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[62]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[62]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[62]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [30]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  31, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  31, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[63]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [31]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[63]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[63]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[63]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [31]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[64]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[64]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[64]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  33, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[64]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  33, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[65]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[65]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[65]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [33]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  34, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[65]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [33]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  34, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[66]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [34]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[66]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[66]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[66]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [34]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  35, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  35, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[67]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[67]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [35]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  36, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[67]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [35]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[67]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  36, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([305], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[68]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([305], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[68]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [36]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([305], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[68]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([305], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[68]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [36]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  37, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  37, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[69]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [37]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[69]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[69]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  38, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[69]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [37]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  38, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([3525], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[70]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([3525], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([3525], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[70]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [38]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[70]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([3525], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[70]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [38]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  39, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  39, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([3554], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[71]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([3554], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[71]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [39]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  40, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([3554], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[71]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [39]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([3554], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  40, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[71]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[72]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [40]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[72]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[72]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[72]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [40]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  41, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  41, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[73]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [41]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[73]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  42, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[73]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [41]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  42, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[73]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[74]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[74]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [42]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[74]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[74]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [42]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  43, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  43, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[75]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [43]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[75]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[75]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [43]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  44, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[75]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  44, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([16], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[76]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [44]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([16], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[76]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([16], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[76]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([16], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[76]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [44]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  45, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  45, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([455], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[77]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [45]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([455], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[77]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [45]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([455], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[77]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([455], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  46, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  46, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[77]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[78]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[78]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [46]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[78]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  47, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[78]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [46]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  47, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[79]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [47]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[79]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[79]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [47]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  48, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  48, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[79]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[80]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[80]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[80]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [48]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  49, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[80]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [48]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  49, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[81]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[81]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[81]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [49]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[81]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [49]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  50, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  50, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[82]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [50]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[82]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[82]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  51, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[82]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [50]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  51, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5004], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[83]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5004], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[83]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5004], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[83]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [51]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5004], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  52, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[83]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [51]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  52, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[84]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[84]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [52]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  53, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[84]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [52]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[84]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  53, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[85]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [53]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[85]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[85]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[85]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [53]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  54, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  54, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[86]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [54]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  55, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[86]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [54]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  55, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[86]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[86]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[87]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [55]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[87]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[87]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [55]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  56, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[87]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  56, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[88]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[88]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [56]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  57, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[88]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [56]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[88]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  57, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[89]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [57]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[89]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [57]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[89]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[89]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  58, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  58, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([16], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[90]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [58]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([16], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[90]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([16], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[90]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([16], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[90]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [58]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  59, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  59, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([455], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[91]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [59]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([455], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[91]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([455], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[91]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([455], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[91]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [59]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  60, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  60, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[92]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [60]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[92]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[92]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [60]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  61, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  61, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[92]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[93]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [61]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[93]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[93]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[93]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [61]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  62, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  62, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[94]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [62]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[94]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [62]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[94]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[94]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  63, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  63, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[95]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [63]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[95]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[95]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[95]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [63]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  64, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  64, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[96]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [64]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[96]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[96]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[96]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [64]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  65, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  65, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5004], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[97]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5004], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[97]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [65]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5004], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[97]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5004], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  66, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[97]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [65]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  66, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[98]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[98]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [66]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[98]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[98]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [66]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  67, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  67, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[99]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[99]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [67]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[99]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  68, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[99]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [67]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  68, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[100]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [68]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[100]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[100]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[100]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [68]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  69, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  69, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[101]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [69]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[101]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[101]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[101]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [69]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  70, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  70, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[102]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[102]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[102]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [70]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[102]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [70]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  71, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  71, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[103]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [71]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[103]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[103]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [71]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  72, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  72, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[103]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([16], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[104]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([16], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[104]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [72]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  73, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([16], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[104]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [72]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  73, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([16], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[104]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([455], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[105]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([455], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[105]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([455], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[105]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [73]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([455], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  74, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[105]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [73]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  74, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[106]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[106]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[106]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [74]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  75, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[106]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [74]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  75, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[107]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [75]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[107]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[107]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [75]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  76, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[107]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  76, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[108]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[108]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [76]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[108]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  77, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[108]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [76]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  77, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[109]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[109]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [77]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[109]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [77]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  78, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[109]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  78, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[110]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[110]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [78]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[110]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[110]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [78]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  79, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  79, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5004], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5004], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[111]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[111]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5004], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[111]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [79]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5004], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[111]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [79]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  80, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  80, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[112]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[112]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[112]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [80]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  81, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[112]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [80]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  81, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[113]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[113]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[113]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [81]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[113]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [81]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  82, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  82, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[114]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[114]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[114]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [82]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  83, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[114]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [82]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  83, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[115]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[115]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[115]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [83]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[115]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [83]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  84, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  84, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[116]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [84]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[116]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[116]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[116]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [84]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  85, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  85, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[117]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [85]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[117]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[117]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[117]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [85]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  86, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  86, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([16], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([16], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([16], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[118]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [86]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[118]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[118]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([16], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[118]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [86]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  87, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  87, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([455], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[119]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([455], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[119]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [87]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([455], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[119]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [87]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  88, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([455], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[119]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  88, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[120]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[120]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [88]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[120]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[120]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [88]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  89, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  89, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[121]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[121]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[121]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [89]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[121]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [89]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  90, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  90, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[122]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[122]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [90]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[122]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[122]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [90]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  91, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  91, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[123]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[123]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[123]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [91]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  92, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[123]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [91]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  92, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[124]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[124]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[124]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [92]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[124]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [92]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  93, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  93, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5004], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5004], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[125]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[125]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5004], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[125]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [93]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5004], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  94, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[125]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [93]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  94, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([5169], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[126]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([5169], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[126]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([5169], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[126]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [94]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([5169], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[126]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [94]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  95, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  95, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([294], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[127]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [95]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([294], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([294], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[127]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([294], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[127]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[127]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [95]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  96, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  96, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[128]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [96]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[128]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [96]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[128]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[128]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  97, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  97, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[129]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [97]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[129]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[129]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[129]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [97]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  98, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  98, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[130]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [98]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[130]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[130]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[130]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [98]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([  99, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([  99, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[131]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [99]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[131]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[131]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [99]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 100, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[131]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 100, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[132]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [100]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[132]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[132]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [100]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 101, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 101, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[132]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[133]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [101]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[133]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[133]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[133]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [101]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 102, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 102, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([16], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[134]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [102]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([16], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[134]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([16], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[134]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([16], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[134]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [102]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 103, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 103, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([455], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[135]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [103]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([455], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[135]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([455], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[135]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([455], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[135]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [103]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 104, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 104, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[136]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[136]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[136]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [104]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[136]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [104]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 105, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 105, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[137]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [105]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[137]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[137]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 106, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[137]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [105]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 106, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[138]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[138]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [106]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[138]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [106]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 107, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 107, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[138]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[139]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [107]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[139]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[139]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [107]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[139]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 108, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 108, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[140]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [108]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[140]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[140]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 109, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[140]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [108]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 109, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([16], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[141]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [109]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([16], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[141]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([16], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[141]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([16], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[141]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [109]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 110, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 110, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([455], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[142]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([455], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[142]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [110]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([455], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[142]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [110]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 111, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 111, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([455], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[142]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[143]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[143]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [111]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[143]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[143]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [111]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 112, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 112, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([734], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[144]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [112]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([734], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[144]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([734], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[144]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([734], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 113, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[144]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [112]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 113, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([4433], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[145]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([4433], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[145]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [113]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 114, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([4433], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[145]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [113]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([4433], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[145]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 114, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([344], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[146]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [114]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([344], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[146]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([344], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[146]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 115, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([344], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[146]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [114]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 115, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([270], device='cuda:4', dtype=torch.int32)
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([270], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[147]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[147]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [115]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([270], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 116, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[147]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [115]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 116, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([270], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[147]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([16], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([16], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[148]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[148]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([16], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[148]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [116]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([16], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 117, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[148]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [116]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 117, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([455], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[149]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [117]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([455], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[149]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([455], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[149]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 118, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([455], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[149]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [117]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 118, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[ResourceManager::prepare_resources][rank 2][cp_rank 1] Adding KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 0][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 1][cp_rank 0] Skipping KV allocation for request 2048.
[ResourceManager::prepare_resources][rank 3][cp_rank 1] Adding KV allocation for request 2048.
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] input_ids: tensor([6677], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] position_ids: tensor([[150]], device='cuda:4', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [118]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] input_ids: tensor([6677], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] position_ids: tensor([[150]], device='cuda:3', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] input_ids: tensor([6677], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] position_ids: tensor([[150]], device='cuda:2', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] helix_is_inactive_rank: [True]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_cache_params.num_cached_tokens_per_seq: [32]
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:3',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 1][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] kv_lens_cuda: tensor([  32, 4095,    2,  ...,    0,    0,    0], device='cuda:2',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 0][cp_rank 0] request_id: 2048, block_ids: [257]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] input_ids: tensor([6677], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] position_ids: tensor([[150]], device='cuda:5', dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] helix_is_inactive_rank: [False]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_cache_params.num_cached_tokens_per_seq: [118]
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] kv_lens_cuda: tensor([ 119, 4095,    2,  ...,    0,    0,    0], device='cuda:4',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 2][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] kv_lens_cuda: tensor([ 119, 4095,    2,  ...,    0,    0,    0], device='cuda:5',
       dtype=torch.int32)
[DeepseekV3ForCausalLM::forward][rank 3][cp_rank 1] request_id: 2048, block_ids: [257, 258, 259, 260]
[32mINFO[0m:     127.0.0.1:58854 - "[1mPOST /v1/completions HTTP/1.1[0m" [32m200 OK[0m
